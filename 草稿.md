











---

工作总结[2025-03-31]

1、改进和梳理了一下CAD绘图Agent工作流的输入模块讨论的材料与问题

2、对接吴晶晶那边讨论清楚了第一版输入流的框架，基于已有的材料和讨论的结果来重新整理一下输入流开发文档

工作总结[2025-04-01]

1、整理完成的模板输入对接方案发送给了开发团队和吴晶晶那边。然后参加AI周例会讨论开发规划

2、下午深入的去调研并设计了一下输入流的网站开发的最小可行技术栈有哪些以及开发框架初步设计

工作总结[2025-04-02]

1、基于之前讨论的初版输入CSV模板来设计用户的交互系统，结合学习使用streamlit框架来搭建这个交互的界面。这部分目前遇到的难点是利用streamlit库与sqlite数据库进行交互，还得补充学习一下基础的数据库操作

工作总结[2025-04-03]

1、写了一下今年第一季度的工作总结，对过去内容所有的日志和周总结做了一个回顾，发现一季度所有内容调研与开发都是相辅相成的，无论是熟悉crew框架还是开发重计量agent都是方便后续开发优秀的生图智能体，这块得一步步来深度求索

2、继续调试了一下streamlit开发的交互模块，目前可以在线编辑部分模板信息，但是多通讯柜情况自定义情况还要进一步设计调试；其次是完成第二次AI培训的考试测试

【2025.03.31-04.03 周工作总结】

- 与吴晶晶团队对齐了核心字段定义，完成了CAD绘图Agent输入流框架V1.0开发文档；
- 已经基于Streamlit搭建了初步的web交互系统原型，并实现在线模板编辑基础功能；但数据动态加载这部分还有问题，多通讯柜自定义交互及Streamlit-SQLite数据联动问题待研究突破。
- 完成一季度技术复盘（框架调研/Agent开发深度耦合验证）。

【本周计划】

1. 推进Streamlit交互模块多模板场景的调试，解决数据库动态加载的技术难点，以及对输入流接口特殊或者异常处理机制完善
2. 把初步具备交互能力的web开放测试，检查是否有输入逻辑错误，其次联系吴晶晶那边确认web设计是否合适

---





工作总结[2025-04-07]

1、review了一下上周智能生图web开发代码状态和问题，最后发现在线编辑模板表格数据会出现重置的情况是因为streamlit的数据更新节点的问题

2、我重新完全重写构造了一下与数据库交互的数据更新逻辑函数，初步解决了数据更新重置的问题。虽然现在在线编辑不重置了，但是我发现现在编辑任一表格都会导致最后一个表格sheet被编辑指令覆盖。这个现在还需要调试一下原因

工作总结[2025-04-08]

1、解决了在线表格编辑时候最后一个tab页会被修改的问题，主要是data_editor内部函数的闭包问题导致数据更新内容错误，我利用AI重写了一下data_editor的回调函数解决了这个问题。现在在线编辑基本没有错误了

2、参与AI组周例会

3、深入讨论了一下交互模块的设计思路，后面需要简化交互逻辑，减少项目信息录入的步骤，调整数据验证板块到数据预览板块上，方便用户修改迭代

工作总结[2025-04-09]

1、把交互web模块做了精简化，不需要项目信息填写与存储，只保留数据预览与在线编辑，目前交互的第一页也就是功能重点就在上传预览与编辑上；其次即是生成预览的结果页面

2、模板选择与示例数据功能仅作为必要下载模板时的补充选项，不再影响用户第一时间即为预览的需求；校验与预览合在一个页面板块，但是这里目前遇到些问题，校验提示都有，但是标红方式出不来，得研究下streamlit对css嵌入方法的应用

工作总结[2025-04-10]

1、重新用CSS与JS来编写了一下校验错误提示的标识信息，解决了标识更新不及时等问题；并分析补充了端口负载、柜号唯一性、类型验证等校验方法实现，初步完善校验体系

2、和曦哥开会讨论了一下从CSV模板到json数据的对接情况，讨论明确了我需要挑选并转化哪些数据结构，以及曦哥需要补充修改哪些JSON内容；然后初步尝试搭建了一下ELK接口与CAD生成的接口api

工作总结[2025-04-11]

1、参加RAG第二期培训，这次从最底层的向量角度看清楚了检索与嵌入存储的细节，感觉能优化研究的内容很多

工作总结[2025-04-12]

[勤奋时间][09:00][17:09]

今天是AI培训的第二期RAG专题学习第二天，今天主要对文本切分、嵌入、向量数据库有了全面的了解，然后深入实践学习了一下检索和提问优化的技巧与知识

【2025.04.07-04.12 周工作总结】

- 基本完成数据模板输入与检验模块。主要攻克了交互模块修复数据更新重置与表格覆盖问题，基于CSS/JS实现基于自定义规则动态校验提示；
- 精简了Web交互流程。合并校验/预览页面；并完善端口负载/柜号唯一性等10项校验规则；
- 跟曦哥确认并推进CSV-JSON数据结构对齐；参与RAG二期培训掌握文本切分/向量检索核心技术链路。

【本周计划】

1. 完成JSON数据结构转化模块开发，实现与曦哥那边ELK接口联调
2. 学习并调研基于ezdxf库的DXF编写方法
3. 实践RAG培训成果，完成大作业





---

工作总结[2025-04-14]

1、分析并重新整理了CSV-JSON数据结构转换的设计与改进；以及图框分割逻辑的初步设计与可能的问题分析汇总。

2、通过《关于csv-json数据结构转换对接及图框拆分设计方案讨论》内部会议确定了，数据转换调整与规范统一的标准，其次确定了分割图框的逻辑，后端处理生成判断再由前端处理管理机原子单元的JSON拆分合并。具体细节与更改内容我也通过会议纪要方式传达了。

工作总结[2025-04-15]

1、探索了一下章阁人才公寓电能管理系统结构图_t3.dwg的错误问题，主要是因为图纸的新比例设置导致的问题，这部分需要CAD数据爬取的时候带上比例缩放的数据；然后参加AI周例会

2、初步调了一下重计量CAD代码对坐标量纲动态适应，在章阁人才公寓电能管理系统结构图上没问题了，但后面还是需要比例缩放的数据

3、同步了最新的JSON模板和数据，然后初步尝试了解析CSV并进行JSON模板的转换，这部分还要进一步映射数据层级的关系并调试

工作总结[2025-04-16]

1、初步解决了主控层与通讯层的JSON转化与连接表示，然后设备层的区分直连与管理机连的转换配置还有点bug需要进一步联合通讯层的屏柜号调试

2、初步实践了一下第三次大作业，做了一点简单尝试用langchain写text2sql，并对生成的sql进行评价与建议，这需要对数据库有一定的学习了解

工作总结[2025-04-17]

1、主要是完成AI培训的第三次大作业，利用langchain框架搭建了一个text2sql的工作流，核心是优化好langchain的sql生成/sql优化prompt模板，让LLM能够理解用户意图生成sql并提供sql校验与优化建议。在此之外配合AI学习并深入理解sqlite和数据库表结构在AI中的的用法

2、重新思考并设计了一下JSON模板的图框范围拆分问题，明天需要开会确定下JSON结构的单位拆分对象以及精确的拆分点

工作总结[2025-04-18]

1、再次评审确定下JSON结构的单位拆分对象以及精确的拆分点，主要是达成团队的拆分逻辑共识；然后完成总结会议纪要

2、按照分图框的逻辑共识来设计相应的代码设计规范，重新从表格数据梳理通信柜分组，然后结合组网结构来设计新的JSON分块模板的逻辑



【2025.04.14-04.18 周工作总结】

- 完成了开发整个CSV到完整JSON数据结构转化标准制定及代码转换逻辑实现，但需要进一步预处理划分CSV
- 协助解决了上周设计院同事反应的图纸比例量纲适配问题，新增实现重计量CAD动态坐标量纲解析；
- 完成AI培训第三次大作业，搭建Text2SQL工作流并优化LangChain提示模板。
- 确定了新的多图框JSON划分方案，通过会议共识明确通信柜分组规则与JSON分块模板；

【本周计划】

1. 推进JSON新图框拆分模块代码开发，完成3组模拟通信柜联调测试
2. 对接并调试开发ELK布局对单图框JSON输入的接口





---

工作总结[2025-04-21]

1、初步完成代码逻辑调试开发，根据通信柜作为图框基础分组，然后对组内数据进行预处理生成单机单网的小的JSON模块。同步的也改进了一下CSV输入示例模拟数据，模拟端口通道数超出12上限的情况

2、对于双机双网的情况比较复杂，因为涉及到交换机与设备层的冗余，索引与匹配方式需要再进一步研究如何处理

工作总结[2025-04-22]

1、研究并初步调试处理了在非单机单网情况下，交换机在各个图框JSON块中的冗余表达，但是在分割多个图框的时候对象添加还需要理一下。目前在环网和双网的处理上可能还需要讨论对齐一下主控层在首图框或者所有图框中的表达方式

2、此外初步调试了一下双网情况下去除管理机部分的非必要的冗余，需要研究一下或者讨论新的切分图框中管理机的数据结构

工作总结[2025-04-23]

1、参加部门AI第三次关于Agent的培训，除了学习听讲师的Agent介绍和经验分享，自己也动手用豆包和coze的平台简单实验了写作的agent工作流，实现起来确实很快但是约束也多，没法进一步的精细调优

2、继续调试CAD分图的代码，现在变量有点多，后面尝试把程序的流程在细化整理一下

工作总结[2025-04-24]

1、进一步梳理并讨论了关于cad分图过程中设备层的交换机与管理机的冗余处理问题，以及主控层的放置问题，并在会议中同团队达成共识

2、重新梳理了关于excel表格的解析代码实现方法，后面在原有数据的基础上重新构建改进数据结构的schema并优化json 的拼接方式，避免将Excel数据直接映射为嵌套字典，应通过明确的数据模型转换实现。方便未来的灵活扩展与维护

工作总结[2025-04-25]

1、上午去体检

2、下午深入的去理解schema数据模型，并根据实际的验证后的表格数据去完善和修改它。

3、完成了完整的表格schema结构搭建，并调试表格解析。同时初步研究并调试使用新的数据结构模型去处理分图数据结构

【2025.04.21-04.25 周工作总结】

- 完成了单机单网场景下通信柜分组与JSON分块生成模块开发，支持12+端口超限预警；
- 讨论并达成双网冗余场景主控层表达共识，优化Excel解析逻辑实现Schema数据模型的转换；
- 参与AI Agent培训并验证低代码平台局限性，明确深度定制开发必要性。
- 完善了新数据模型及布局结构的Schema扩展性，并初步完成分图算法在新数据模型上的迁移；

【本周计划】

1. 推进双机双网交换机冗余逻辑及图框分割对象动态管理实现落地，构建3组复杂组网示例数据完成测试
2. 开展调试开发循环调用布局的接口程序及接口RESTful方案设计



---

工作总结[2025-04-27]

1、总结并整理完成CADDrawerAgent的交互与调度模块、生图模块的阶段规划与里程碑的详细文档设计

2、团队会议讨论后续开发的项目记录与管理安排

3、整理了一下专注于交互与调度模块的已完成与待规划项信息，并补充了相关的材料与汇报和备注

工作总结[2025-04-28]

1、调试并解决了分图算法中直连设备重复计算通道数的测试情况，这部分现调整为按直连设备类型属性来分组计算是否满足10个一列

2、参加AI小组例会；回顾梳理之前的前端和后端代码

工作总结[2025-04-29]

1、JSON分图方法测试初步通过了单机单网、双机双网、单/双光纤环网这些测试数据的情况，同时调整了单网络架构也同样输出commConnections结构

2、设计了初版的ELK模块的接口api与对应参数文档，并做了初步的参数调试

工作总结[2025-04-30]

1、调试并解决了CsvInfo对象一直识别错误的问题，主要不同的导入方式在python中被识别为不同的CsvInfo类型对象

2、测试通过了web前后端的ELK模拟接口，可实现从下载输入excel到分图JSON的列表，再到ELK接口循环输入。



【2025.04.27-04.30 周工作总结】

- 完成了绘图Agent交互模块的链路开发：
  - 解决了CsvInfo对象类型识别问题
  - 跑通Excel输入-分图JSON列表生成-ELK接口循环调用的测试验证路径；
- 通过对环网场景的excel调试优化了分图算法在通道分组上的逻辑；
- 设计了ELK模块接口API及参数文档，目前已测试通过一些比较简单的单/双机双网及环网的模拟录入数据用例

【本周计划】

1. 推进ELK接口对接并最终完成模拟联调，衔接交互输入模块到ELK布局模块
2. 初步调研CAD图纸绘制的技术内容与难点





---

工作总结[2025-05-06]

1、完善对在线录入的excel情况的进行分图JSON列表的稳定性和规则校验，主要包括图框索引规则、柜设备完整性、分图唯一性、控制层展示规则这些，确认分图算法的稳定性

2、除了输入的示例excel情况，还补充了设定阈值的边界测试情况，同样包括以上校验规则。初步测试无问题

工作总结[2025-05-07]

1、梳理并优化整个界面与绘图调度模块，调整项目代码结构方便演示展示

2、展示讨论了交互部分的进展，后面与曦哥对接联调ELK布局的单机单网api换掉现有的模拟占位api

工作总结[2025-05-08]

1、处理frameindex与controllayer和cont_commConnection的显示关系，保留每个框的主控信息。并同步更新主控层分图的校验规则

2、重构了API的请求处理逻辑，调整之前的模拟API成真实ELK模块API的返回参数结构一致，增强处理API返回错误时的处理和保存功能

3、讨论确定分图后各通信柜的主控确认关系，这部分主控与交换机连接不再显式配置，由ELK模块确定连接

工作总结[2025-05-09]

1、重构了项目代码中录入excel数据模型，中关于主控层的连接属性信息，不再自定义配置连接交换机，让ELK代码生成链接

2、更新测试ELK的api展示功能到web界面交互与调度端，目前可以把分图后返回的布局svg部分能展示到预览框中，实时观测录入数据与布局的变化对应关系，但svg显示会有截断的情况，svg容器大小需确定以统一预览框尺寸



【2025.05.06-05.09 周工作总结】

- ELK模块集成取得关键进展： 完成了与ELK布局API的对接联调，替换了原有模拟接口，并适配了其数据结构与处理逻辑。
- 数据处理与校验优化： 完善了Excel在线录入至分图JSON的算法及校验规则，并根据ELK的需求重构了数据模型，提升了稳定性。
- 前端功能初步实现： 实现了ELK返回的SVG布局在Web端的实时预览，并优化了前端代码结构。

【本周计划】

1. 解决SVG预览显示问题： 优化SVG在预览框中的显示，确保完整性和统一尺寸。
2. 协同曦哥深化ELK联调与测试： 扩展到环网与双网的测试场景，配合解决现有预览中布局异常的问题。
3. 调研绘图模块的技术路线、和相应的技术实验，周四15号出一个技术调研文档方便后续进一步讨论方案与分工安排

---



工作总结[2025-05-12]

1、上午重新调试调整了一下svg展示的容器显示，并梳理了当前项目情况与进展

2、对新的绘图模块进行初步的了解，确认有哪些mvp研发是必须的信息以及可能的难点，方便后续的完成dxf生成的技术实验

工作总结[2025-05-13]

1、调研梳理了ezdxf在绘制CAD图纸内容上技术文档与方法

2、结合之前输入给elk的json来尝试制造一个简单的模拟组态绘制的json输入数据示例

工作总结[2025-05-14]

1、主要是尝试解决了本地AutoCAD打开dxf无法显示图元的问题，这块目前实验出来的的原因大概是测试JSON数据属性缺失导致的AutoCAD无法识别元素

2、结合今天从曦哥那拿到的简单的布局坐标和导出的template.dxf的模型空间的信息来重新构建了含关键信息的数据，目前可以显示从输入JSON数据到图形的显示了，但还是有部分元素的缺失需要进一步改进测试数据

工作总结[2025-05-15]

1、跑通从构造的JSON测试用例数据中分别绘制block、line、text、frame这些图元属性，并正常显示生成的dxf

2、初步结合交互模块的metadata与elk布局简单的单机单网JSON数据构建新的真实测试用例，还需要调整结合到新的绘图输入JSON结构中

工作总结[2025-05-16]

1、构建了新的交互分图、ELK布局、图元库对象三个输入数据的schema数据模型，完成了从输入schema到绘图JSON的固定工作流转化

2、尝试手动修正部分图元的坐标的偏移，但是图元与多段线之间还是有偏移对不上，需要分析一下处理方法



【2025.05.12-05.16 周工作总结】

- DXF绘图模块取得初步突破： 成功调研并实验了ezdxf库，实现了从构造的JSON数据（含图块、线条、文字等基本图元）生成DXF文件，并解决了初期图元无法在AutoCAD中正确显示的问题。
- 绘图数据流构建与整合： 定义了交互分图、ELK布局及图元库对象三方输入数据的Schema模型，并初步完成了将这些输入数据转化为绘图模块所需JSON格式的固定工作流。
- 前端显示优化： 调试并优化了SVG预览容器的显示效果，并同步梳理了当前项目整体情况与进展。

【本周计划】

1. 解决DXF图元坐标偏移问题： 重点分析并修复当前DXF生成中图元与多段线之间存在的坐标偏移及对不齐问题，确保绘图准确性。
2. 完善和扩展测试用例： 进一步完善和扩展测试用例，优化绘图输入JSON的结构与内容完整性。研究并实验更复杂图元（如特定设备符号、连接关系等）的绘制方法，为支持更丰富的图纸内容做准备。
3. 固化并优化数据转换工作流： 补充测试用例，并根据测试结果持续优化绘图逻辑，提升DXF文件绘制效果质量

---

工作总结[2025-05-19]

1、调试更新图元库的基点计算方法，使用block的边界框左下角作为基点，这样就可以统一标准统计特殊图元及文本的偏移点。其次虽然观察到部分图元其中构成元素以左上作为基点，但是实验了左上角作为基点会导致大部分block偏离比较大

2、基于block的边界框标准可以统计所有用到的图元块的大小尺寸，这样可以我这边先把统计的模板图元尺寸给到ELK布局生成准确的block区域以及端口位置，这样等ELK返回的坐标应该是更准确

工作总结[2025-05-20]

1、上午梳理讨论绘图技术实验进度与问题，以及参加AI小组周例会

2、下午调研并实验解决了dxf转dwg的技术问题。解决了传统pyautocad频繁出现 COM 错误，无法初始化acad应用问题；win32com库的client方法SAVEAS转换功能总是失效的问题，改进并融合两个传统库的功能实现了基于AutoCAD客户端的dxf转换。但是pyautocad 自 2015 年起未更新，在高版本（包括 2024）中存在严重的兼容性问题，有时候还是会初始化失败

3、因此调研了并部署跑通了第三方ODA的方法，该外部工具有人维护的，可以较快且稳定的调出并转换dxf，因此会作为首选的部署的转换方法

工作总结[2025-05-21]

1、更新了绘图程序中图框和设备块的类型筛选处理逻辑，从ELK结构中获取cadName属性匹配图元库来实现简化结构化的dxf绘制工作流。

2、实现了特殊图元，如交换机、PDU电源等组合符号的插入绘制功能

3、解决了坐标系和偏移的自动修正、实现了从表格输入与模板信息中获取并自动添加图框与图元属性的功能

4、基本跑通从Excel输入到dwg输出的工作流框架，并进行了简单的演示。

工作总结[2025-05-22]

1、解决了手动配置分图JSON列表数据以及ELK布局输出数据的问题。自动化配置路径以及入参结构为绘图模块所适用的JSON模板数据信息

2、把绘图模块的所有功能进行了梳理整合。统一绘图模块的自动化pipline，实现完整的绘图模块的端到端绘图流程：多入参数据加载 -> schema解析 -> 绘图JSON数据生成 -> 坐标修正 -> dxf绘图

工作总结[2025-05-23]

1、协同ELK布局模块解决了图元插入在dxf绘制的坐标修正以及图元CAD插入点的问题，基本调通了一个dxf大致准确布局的案例

2、将坐标转换与基点修正算法部分内容从原始项目中隔离了出来并交给ELK布局去处理

3、初步在项目代码里面跑通了，绘图模块批量处理来自前端模块与ELK模块的列表JSON数据，不再用手动修改文件测试，实现了Excel到批量生成dxf的数据流管线，下一步就可以集成到前端的API界面，实现跑通web端到端绘图文件生成



【2025.05.19-05.23 周工作总结】

- 核心绘图流程打通与格式转换突破： 成功实现了从Excel输入到DWG输出的端到端工作流（并借助第三方ODA工具稳定转换DXF为DWG）。具体完成了绘图模块内部从多源数据加载、Schema解析、绘图JSON生成至DXF绘制的自动化处理管线整合。
- 绘图精度提升： 通过统一图元基点计算、与ELK协同优化坐标修正（坐标转换与基点修正逻辑已移交ELK处理），成功调试出布局基本准确的DXF案例；同时增强了特殊图元（如交换机、PDU）的绘制及图框图元属性的自动填充能力。
- 批量处理与API集成准备就绪： 实现了绘图模块对来自前端及ELK模块的列表JSON数据的批量处理能力，不再依赖手动修改文件测试，为下一步API集成实现Web端到端文件生成奠定了坚实基础。

【本周计划】

1. 完成绘图模块与前端API的集成：实现Web端从Excel上传到DXF文件生成的完整在线闭环。
2. 优化完善自动化生成： 补充针对批量处理和真实业务场景的充分测试，持续优化模块自动化的管线及系统稳定性。
3. 探索dxf上的图框整合算法： 研究整合算法的控制对象，探索控距算法的实现



---

工作总结[2025-05-26]

1、调研了一下ezdxf中整合多个dxf 的msp方法，主要是就是copy所有entity的信息到新的dxf的msp中，但是现在还没能解决的问题，可以整合多个dxf 的信息，但是新的block的属性值不能与block进行绑定，导致排列整合效果可以出现，但是属性值都会堆积在第一个框中。现在的难点就是如何控制属性值的排列的偏移

2、尝试了获取原始block的信息去重新赋值entity的属性值，但是这里面目前还有问题。获取的块确实已经导入到 master_doc.blocks 但 add_entity 仍然返回 None，感觉copy方法获取的结果指示该块定义本身在 master_doc 中是无效的

工作总结[2025-05-27]

1、上午讨论了绘图Agent的进展与后续计划，然后参与AI小组周例会

2、下午主要是完成了AI培训的第六七八次项目作业的源代码学习与总结，并将个人学习是实践的结果记录到作业的文档中完成作业上传；另外是第九次作业使用langchain生成测试用例的workflow也初步利用AI辅助完成了一个简化版的根据需求文档生成测试用例的AI工作流，听从刘总的指导深入的配合协同AI快速完成AIasCoworker的练习实践

3、下午另外也初步尝试了使用所有JSON的列表数据来生成一个大的dxf，但是这个设计的文件数据信息比较多，开发还是碰到不少bug需要后面再快速去调试解决

工作总结[2025-05-28]

1、上午完成了作业九与作业十测试智能助手的langchain框架workflow的搭建，进一步锻炼了利用AI的能力快速完成简单项目搭建、AI协同测试与调试的能力

2、下午重构了用于解析多绘图模块与ELK模块以及dxf模板信息JSON的函数，然后从数据模型上重新构建对多图框列表JSON的解析与排列坐标的搭建，接受排列间距与图框尺寸等等配置生成多图框绘图的输入JSON文件。

3、初步手动调试跑通输入多个图框测试JSON地址生成一个完成的大dxf绘图

工作总结[2025-05-29]

1、调试并整合了绘图模块新的全局数据生成、大dxf绘制的新启动脚本，并完成将ODA转换脚本接入到模块工作流中，手动测试通过从多个绘图输入JSON到全局转换的大DrawingData列表对象生成dxf，最后到DWG最终的输出

2、完成将新的全局DWG生成工作流替换原始旧的多个单图框dxf压缩包的前端API接口，完整实现了在前端web即可通过Excel生成单一大DWG文件

工作总结[2025-05-30]

1、调试修改drawing_data_generator类，解决了特殊块如PDU接入ELK坐标跟随图框生成

2、重新整理了所有配置项包括分图验证、布局参数、ELK结构配置和文件路径等参数到统一的全局文件中，解决了不同脚本配置修改不一致的问题避免了ELK输出的画幅不一致

3、加入了原始的场站安装位置信息到schema和cad_schema结构中并生成相应的全局布局偏移坐标给到ELK模块，目前已经取消了本地实验的全局布局偏移并将全局偏移实现方法移交给了ELK计算。可以稳定生成全局布局的多图框DWG



【2025.05.26-05.30 周工作总结】

- 多图框合并输出单一大DWG功能web上线： 成功实验攻克并实现了将多个独立图框数据的整合。生成单一、大型DWG文件（并集成了ODA进行格式转换）。目前此功能已上线并替换原有前端占位API，用户现可通过Web界面从Excel直接生成统一的DWG图纸（172.17.6.100:5000）。
- 全局布局与配置管理优化： 通过与ELK模块协同（给出具体偏移量到ELK），实现了多图框DWG的稳定全局定位（各行图框位置对齐）。同时，统一了项目各项配置管理在一个脚本中调控，提升了系统稳定性。并优化了特殊图元（如PDU）在ELK坐标系下的定位逻辑。
- AI技能深化与应用： 完成了公司安排的AI培训作业五六七八九十，特别是在LangChain框架构建智能应用（如测试用例生成、智能助手）的实操写代码大作业，锻炼了利用框架搭建AI辅助开发与测试的经验。

【本周计划】

1. 完善单一大DWG文件内容：添加场站信息到绘图的cad_schema，根据对齐的每行图框添加左侧相对的场站说明
2. 研究counterPro与绘图模块的数据对接：调整当前测试Excel数据结构，补充真实测试数据转换需要的信息。最终建立外部的转换工作流管线
3. 补充针对批量处理和真实业务场景数据的充分测试，持续优化模块自动化的管线及系统稳定性。

---

工作总结[2025-06-09]

1、调试解决了Excel中设备类型与设备数量互换后校验模块异常的问题，修正了decode_form中对新schema的解析匹配方法，包括设备属性结构、通讯柜映射关系及配套校验规则的修改与验证

2、解决了之前测试阶段参数预设与属性硬编码的问题，如新增了对通讯/通信柜或箱等字符的兼容支持，避免了莫名其妙的整个图框的设备识别不到的问题

3、构造测试了陕西榆林数据需要分图的情况，发现能够稳定支持分图后对页数与页次的动态排序，暂时没发现bug

工作总结[2025-06-10]

1、测试构建完整的陕西榆林需要分图情况的测试数据，验证通过了存在分图下的图框排序与行索引的计算规则

2、配合ELK排除解决了管理机图元的channelIndex不从1开始的情况，重新修改完整的测试数据，解决了部分端口跑到管理机上方的问题

工作总结[2025-06-11]

1、梳理并调整CAD生图的项目代码与材料，完整演示当前在单机单网上的生成效果，介绍当前项目的特点与未来的展望

2、修复了web 上先预览才能生成DWG的工作流流程

3、初步整理了一部分的深圳平湖智创园项目的双机双网的情况的真实测试数据

工作总结[2025-06-12]

1、补充完第一个构造的深圳平湖智创园项目的双机双网情况的真实测试数据

2、整理CAD生图材料，完成主持对设计院同事的第一次会议项目演示与交流，总结会议内容与后续改进

3、配合布局解决端口通道合并的配置问题

工作总结[2025-06-13]

1、更新智能绘图初步效果演示评审会议的会议纪要为模板格式，进一步整理设计分院的同事的所有会议现场问题与我们的回复

2、修正构建的双机双网数据中双网的结构化连接，构造了一版双机单网连接的简化测试数据。

3、单独测试多组图框JSON发现12通道作为合并阈值比较极限，后续会采取设置合并配置为11通道；同时测试发现直连设备多情况下会有多余的伸出线，已配合ELK解决。另外反复调试了双交换机情况下丢失直连通道的情况，发现主要是ELK目前暂时不支持第二交换机的结构

【2025.06.09-06.13 周工作总结】

- 数据结构重构完成并优化校验模块： 完成了模板数据结构的重构及相应校验的修正，提升了系统对真实数据（如"通讯/通信柜"等）的兼容性与鲁棒性，优化通道合并阈值（11通道）；
- 测试完善单机单网绘图代码并顺利演示：陕西榆林16图框项目100%通过分图场景测试，实现页次动态排序零异常；并成功面向设计院同事完成了项目首次演示与交流，收集了关键反馈，为后续优化指明了方向。
- 启动双机双网复杂场景适配工作： 以“深圳平湖智创园”项目为蓝本，初步完成了双机双网真实测试数据的构建，并基于测试对直连设备划分、连接线等细节进行了优化，正式开启对更复杂组网场景的适配研发。

【本周计划】

1. 解决双机双网支持问题：与曦哥对接，共同设计并推动解决“第二交换机”复杂布局结构的支持，并基于后面初步测试反馈，继续完善和修正双机双网测试Excel数据，以及改进对新复杂数据模型的解析与构建等上层逻辑。

2. 跟进上周演示会议反馈的绘图优化建议：整理并分析设计院同事提出的问题与建议，尝试新增统计数据表格或标注、安装位置与端口标注等。另外上线单机单网部分的功能到钉钉给吴晶晶团队试用


---

工作总结[2025-06-16]

1、整理优化了单机单网的处理工作流代码，移除了后面双机双网的相关数据schema结构以及代码影响

2、修改了录入模板，收集了目前测试图纸中常见的设备类型名称并加入到template_info.json中进行绘图匹配，目前可以支持下拉框选项限定模板里面设备层和通信层设备类型选型，方便约束输入设备不兼容的情况

3、找出了下载模板录入报错的问题，主要是下拉框Excel与agGrid组件不兼容，增加了在输入前进行Excel校验处理解决了预览的前端bug问题

工作总结[2025-06-17]

1、调试搬到43服务器上的绘图项目代码，现在服务器上的软件配置以及代码环境基本调好了，再解决一下agGrid表格显示又出现不兼容的情况基本就没问题了

2、参与AI小组例会

工作总结[2025-06-18]

1、解决了服务器上的agGrid的css与js远程CDN样式远程访问造成延时高的问题，服务器防火墙禁用端口的问题，解决了服务器的中文编码乱码问题

2、单机单网绘图功能已稳定部署到43服务器上，解决了环境的问题，同时也完成上线了钉钉工作台CADDrawerAgent，解决了钉钉开发平台的配置问题，现在可快速访问并生成CAD图纸

工作总结[2025-06-19]

1、更新了Excel模板中通信柜的设备层中设备图元的可选项，支持后续用户自定义和扩展设备块的库的图元类型

2、改写了通讯层交换机的JSON结构，现已支持第二交换机的直连设备数据结构。

3、新增了对上线43服务器和钉钉的CADDrawerAgent应用的使用说明文档，包括界面使用和详细的表格填写说明与注意

4、参与项目发货Agent的细化讨论会议

工作总结[2025-06-20]

1、优化了一下43服务器上绘图和交互两个模块的工作流代码，去除了多余的测试代码和实验性的调试代码，确保方便代码的易读和维护性

2、收集并处理设计院的朱珂那边反馈的试用疑问和报错，同时跟朱珂详细讲解说明了Excel的使用与填写细节。排查出他们试用时候图元丢失的情况，主要是目前ELK模块的数据库中对于管理机的类型支持比较少，ELK的布局输出会丢失如ismart8这种新类型的管理机及设备图元；



【2025-06-16-06-20 周工作总结】

- 单机单网功能成功部署上线：解决了环境配置、编码及防火墙等多项问题，将单机单网绘图功能稳定部署至43服务器，并同步上线钉钉工作台应用“CADDrawerAgent”。联系了吴晶晶团队进行初步试用反馈。
- 用户体验优化： 支持Excel录入模板通过下拉框选型及前端校验减少用户输入错误，解决了后端type主动限制的问题；同时编写了详细的用户操作说明文档和项目介绍。
- 推进双网功能实现： 在数据结构层面，完成了对“第二交换机”的支持改造，为双网功能开发奠定了基础；同时，通过用户反馈排查出ELK模块因设备库不全（如不支持ismart8管理机）导致图元丢失的新问题，明确了下一步需协同解决的依赖项。

【本周计划】

1. 协同解决设备库不全问题：将朱珂那边反馈的“ismart8”等新型管理机图元丢失问题进行完善，推动更新设备数据库，以保障真实项目数据的完整性。
2. 推进双机双网功能联调：基于已完成的“第二交换机”数据结构改造，与ELK推进双网功能的联合调试，实现在布局算法中第二交换机的支持。
3. 持续收集用户反馈并迭代优化：主动跟进设计院及其他用户的试用情况，收集更多使用场景下的问题与建议，并快速进行产品迭代与功能优化。
4. 启动意图识别项目的模块研究：识别客户的问题意图，并实现提取关键字作为下游流程的输入

---

工作总结[2025-06-23]

1、更新了完整的通讯层设备的属性库，优先解决了ismart8新类型管理机设备type无法识别导致设备丢失的问题。

2、协助曦哥那边排查并解决了更新数据库后33M坐标偏移的问题

3、解决了cetsoft-svr1账号登录不上的问题，同时排查并解决了代码远程推送过程中因代理导致错误的问题；并更新了推送上去的README文档

工作总结[2025-06-24]

1、详细梳理学习了当前myRoberta的代码库，理解其在项目信息中的处理工作流，以及当前多任务（意图分类、子域分类、实体识别、关系抽取）训练的逻辑

2、差不多理清了王工的Roberta代码整个框架和JSON数据的结构，以及各个分类的配置方法，目前调试遇到点环境问题，需要解决项目中termios与Windows不兼容的问题

工作总结[2025-06-25]

1、解决了ssh连接服务器的问题，初步熟悉了通过Linux完成王工的Roberta预测的项目操作流程

2、调试环境跑通了116服务器上Roberta项目的预测输出，但是发现实体识别的效果似乎还比较差，特别是项目编号经常被拆的很散（Chinese-roberta-wwm-ext和conversation-roberta两个版本模型似乎都这样），然后RobertaV2的导入还有点问题需要修正

3、请教了王工那边，理解清楚了当前最新项目模型RobertaV2的训练情况，以及数据分类的标注含义，后续可以逐步进行发货领域的数据扩充的训练

工作总结[2025-06-26]

1、解决了启动label-studio更新label定义时候出现的django.db.utils.OperationalError报错问题

2、梳理了略哥整理的项目发货问题决策树的问题分类思维导图和之前的项目大类问题汇总，初步根据分类的方向在王工的问答数据集Excel上新增差异性的部分问题集

3、目前根据初步整理的50条通用种子问题集构建了一个GPT扩充样本的prompt模板，后面可以批量生产并进行筛选微调，但是对于打标的实体标签、relation标签这两个难定的，需要后面请教下王工如何扩展且能与之前的规范保持一致

工作总结[2025-06-27]

1、初步整理了一下上半年的工作成果汇总与个人总结

2、整理了问题的分类领域并对种子问题进行了调整和补充。然后同王工确认了扩充方式没啥问题，但是对于问题标签的定义抽象不充分，还需要更新补充一版发货相关的实体与动作的抽象标签，区分实体、属性、动作与意图

3、请教理解了王工的实体建模的人-事-物出发点逻辑，基于此我更新了一些发货领域的实体划分标签。目前三个抽象实体的细部划分比较多，会遇到一些没考虑到的细节实体或属性，同时这也需要更合理的relation抽象设计来表达意图内实体关系，这部分生成数据的时候再去改进。



【2025-06-23-06-27 周工作总结】

- CAD绘图项目支持与收尾： 解决了用户反馈的“ismart8”新类型管理机设备丢失的关键问题，并协助处理了数据库更新后的坐标偏移及服务器部署问题，保障了已上线功能的稳定运行。
- 快速上手新NLP项目（myRoberta）： 深入学习并掌握了myRoberta代码库的整体框架、多任务训练逻辑及数据流，成功在服务器部署和跑通了模型预测流程，完成了项目切换后的技术熟悉与环境上手。
- 启动“项目发货”领域数据扩充与建模： 启动了面向“项目发货”新领域的NLP数据准备工作，包括分析业务问题、构建种子问题集（目前25条种子问题，165条扩充问题），并与王工对齐，初步设计了符合“人-事-物”逻辑的实体标签体系，但是标签的定义我还需要进一步细化和补充。

【本周计划】

1. 核心任务：完成“项目发货”领域首批数据扩充与标注：基于上周确定的实体关系建模逻辑，最终确定实体与关系标签体系，并利用GPT辅助生成扩充问答对，在`label-studio`中完成首批数据的清洗与标注工作。
2. 进行模型增量训练与初步评测：使用标注好的新数据集，对`myRoberta`模型进行增量训练或微调，并初步评测新模型在发货领域的意图分类和实体识别效果。
3. 持续跟进CAD绘图项目用户反馈：保持对CADDrawerAgent用户的支持，及时响应和处理可能出现的新问题。

---

工作总结[2025-06-30]

1、梳理了王工设计的基础属性本体标签与人-事-物的属性值标签的关系，并对目前遇到一些没考虑到的细节实体和属性对其进行了相应发货领域的补充完善

2、通过label-studio对扩展的问题语料进行实体与意图的标记，目前大致75条，边标注边更新label定义

工作总结[2025-07-01]

1、通过label-studio对扩展的问题语料进行实体与意图的标记，目前已标发货领域意图问题语料130条。标记问题的同时更新了关于信息系统等实体标签体系、问题动作判断等动作标签体系

2、目前用已标的75条意图语料以及concat王工之前的预训练语料进行了初步训练，发现对于类似的自创测试问题基本识别还行，但是实体泛化性还是差点，可能目前试训练的实体类型比较集中导致的，这个还需要更多数据来调试训练

工作总结[2025-07-02]

1、目前已标记完新增发货领域的165条扩充数据样本，再加上之前的基础信息共722条训练数据，包括实体、关系和问题意图等标签。

2、新加量版本数据表现在可以实现识别更多实体，包括发货和基础领域。但是对于多实体标签之间的关系却表现的过拟合了，所有实体都强行预测了关系，另外对于意图分类依旧欠拟合，识别准确性不是很高

工作总结[2025-07-03]

1、实验调整了Roberta多分类器中grammar和relation的训练loss权重，另外也尝试了采样降低query意图问题的比例，但是似乎都没啥影响，后面尝试按王工的第二种方法增加数据和数据中对relation的负例标签来训练

2、尝试除了正向关系的标签还给relation增加了none标签，明确告诉Roberta模型"什么是没有关系的实体对"，提供了正负样本的按比例分配的学习方法，缓解"所有实体对都有正向关系"的错误学习模式。但是grammar预测效果依旧需要改进

工作总结[2025-07-04]

1、尝试了使用多阶段训练的方法，针对不同难度的多分类问题分别进行训练，一阶段专注于基础任务（实体边界 + 语法分类）。阶段2再引入实体类型分类，识别文本词条所属实体标签定义。阶段3最终再全面学习关系抽取，正负样例关系都训练学习。但是目前实验效果并不明显，需要调整权重超参数或者整理数据质量和数量

2、调整了自动绘图的模板图元中文字展示定位异常的问题；另外配合陈燕燕那边完成外网版CET智译上线钉钉。

3、对标注的relation数据做了清洗，修正所有unknown的relation标注，统一为none的负例标注。进一步进行训练调试实验



【2025-06-30-07-04 周工作总结】

- 完成发货领域首轮数据标注与模型训练： 完成了“项目发货”领域首批165条扩展语料的实体、关系及意图的语料标注，并结合存量数据（共722条）进行了多轮模型训练与AB测试评估，并记录了对比测试结果
- 定位到模型关键问题（过拟合/欠拟合）： 通过训练定位了“关系抽取”存在过拟合、“意图分类”存在欠拟合的核心问题。通过引入“无关系”负例样本的核心训练方法，对关系过拟合问题有了初步缓解。但是对于高难度的意图分类依旧不准确，目前已实验了调整损失权重、多阶段训练等方法策略
- 解决跨项目支持问题： 标注数据训练模型的同时，继续为CAD自动绘图项目提供技术支持（修正图元文字定位）并回应朱珂那边的绘图问题，以及协助完成了“CET智译”的外网版上线工作。

【本周计划】

1. 核心任务：系统性提升意图分类准确率：针对当前意图分类欠拟合问题，计划通过扩充意图多样性的训练样本、构建单独针对意图的分类器模型结构、调整损失函数等方法优化训练实验。
2. 精细化数据与调优实验：利用大模型来清洗和优化现有标注数据，确保标签一致性；并系统性地调整超参数（如学习率、权重），对上周尝试的多阶段训练、负例采样等方法进行更深入的调试。另外对训练损失进行可视化的实现。
3. 持续扩充高质量训练数据：根据评测结果，分析模型薄弱环节，有针对性地设计和生产新的高质量训练数据，尤其是当前覆盖不足的实体和意图类型。



----

工作总结[2025-07-07]

1、回应设计院朱珂那边绘图的问题与需求，如缺少柜号导致接线不显示和图框选择等问题，以及要把项目属性值自动填充到图框中等需求

2、实现了基于王工Roberta代码的训练loss趋势的可视化，目前针对实体边界、实体类型分类、关系与语法抽取分别进行了训练效果的分析，初步得出Roberta改造的多分类器在不同任务上的表现

3、在使用RoBERTa进行多类别分类时，无论采用何种训练方法和参数调整，“语法”类别的分类性能都很差。按王工建议我尝试尝试在不改变Roberta的多分类器输出层的情况下，调整分类器训练权重，只保留grammar的训练梯度，使用的concat的700+的标注和学习率等配置。但是效果不理想，其他三个类无法识别是在意料之中，但是grammar训练了依旧无法识别就一定程度说明多分类器结构对这个难度的问题不是很实用

工作总结[2025-07-08]

1、梳理理清了Roberta训练可视化图中关于loss训练竖线的问题，主要是step在训练中与参数更新频率不一致的原因。但是loss图的整体趋势还是说明了entity-type的过拟合。

2、解决了朱珂反馈的图框属性值（如项目、内容、图号等）硬编码的问题，调整原始cad_schema信息以正确应用总览中的信息，实现了按要求自动完成填充。

3、排查解决了绘图中设备层图元显示不出设备名称的问题并告知了朱珂那边。

工作总结[2025-07-09]

1、梳理了王工之前设计的Roberta多分类器网络的架构，同王工讨论了关于如何修改数据结构label与模型的输出层的问题，确定先通过简单数据来训练Roberta实验其对grammar的拟合效果

2、结合原始创建一个专门用于语法分类的二分类或单任务分类器。并整理建新的数据集文件，用于特定于grammar的训练。目前已经改了一版Roberta只针对grammar的网络训练结构，但是tensor对齐还有点问题需要调整

工作总结[2025-07-10]

1、搭建并完整实验了只针对grammar语义识别问题的单独输出层Roberta的训练模型，实验了调整不同超参数与label权重比例的情况，但是结果初步都基本显示无法泛化学习到真实grammar语义。同王工讨论了后续从数据简化上去改进，先实验一下简化为3类易区分label的grammar进行训练

2、完成中国电子系统的工程系列职称申报和系统填写

3、修改了grammar的标签类数同时调试修改了训练的输出神经元个数与类数对应，但是调通后发现预测标签经常出现预测错乱的情况，这里需要实验排查看一下底下设计的编码是否准确对上了

工作总结[2025-07-11]

1、目前发现当前测试效果不好的原因不全出在数据集质量上，模型计算的F1分数好但训练集测试还不好这个是矛盾的。和王工讨论后解决思路主要分两个去实验排查，精选50个容易分辨的grammar的优质数据集；另外尝试把grammar多分类问题移到subdomain分类器上，这个subdomains单分类器是更简单、易处理

2、清洗训练数据集，去除历史数据中存在的少量错标以及可能有歧义的标注数据。同时继续调试排查多分类编码错误的原因

3、排查出主要是编码对齐的问题，目前已实验解决了在grammar单分类的Roberta上训练准确性的问题



【2025-07-07-07-11 周工作总结】

- NLP语法识别难题攻关取得突破性进展： 针对“语法”类别识别率低的问题，通过构建独立的单任务分类模型、简化标签体系等一系列实验，最终定位并修复了“标签编码错位”的底层BUG，成功解决了grammar语法模型的训练准确性问题。但是目前混合多分类器的协调训练的稳定性还需要实验提升
- CAD绘图项目功能迭代与用户支持： 响应设计院朱珂反馈，完成了图框项目属性值的自动填充功能开发，并修复了因缺少柜号导致接线不显示、设备名称不显示等多项问题，提升了应用的实用性。
- 模型训练分析工具与方法优化： 实现了训练过程损失（loss）的可视化，为分析模型在多任务上的过拟合等表现提供了直观依据；尝试多任务的多轮训练策略（调整权重、单任务输出、简化label等），但效果目前仍有限。并通过与王工的深入探讨，明确了后续模型优化的多种策略。

【本周计划】

1. 核心任务：探索语法分类任务的最优集成方案：研究将已验证有效的语法单任务分类器，重新整合回主多任务模型中的可行方案；或作为备选，尝试将语法分类任务迁移至结构更简单的`subdomain`分类器上进行实验，对比效果。
2. 聚焦多任务分类优化：继续验证多任务分类器在不同数据分布与标签设置下的效果，探索label简化与数据增强的组合策略。
3. 提升整体模型综合性能：在语法问题取得进展后，重新审视并优化实体（entity-type）过拟合等其他遗留问题，通过数据增强、调整模型结构等方法，提升模型的综合表现。

---

工作总结[2025-07-14]

1、已实验解决了更新grammar模型后，多任务分类中entity识别异常的问题，主要还是编码对齐和之前调试的权重没更新的问题

2、参加入职一周年职业导师线下会议的谈话

3、测试来看目前多任务分类grammar和entity问题不大，主要是subdomains和relation预测不准，他们的数据都比较差、另外relation分类预测任务相较另外几个任务难一点

工作总结[2025-07-15]

1、实现优化当前训练代码流程中模型保存和模型命名的功能，避免每次调试训练模型被覆盖的问题

2、调试解决了subdomains分类任务中forward编码没有进行转换的问题，修改preprocess的编码减一来对齐任务的loss计算过程。目前subdomains和grammar的识别提取基本都能准确

工作总结[2025-07-16]

1、上午完善之前的职称申报的材料；另外补充绩效评估的材料

2、目前Roberta主要是在entity和relation的识别与提取上准确率不够高，所以针对训练过程，下午研究了一下如何结合贝叶斯优化来调整当前的分类器权重与训练配置的超参数组合

3、另外针对训练的数据集，下午把构造的测试数据集中预测效果有问题的做了梳理汇总，针对这些badcase继续专门的数据增强，目前现扩充一批这些badcase数据集

工作总结[2025-07-17]

1、针对之前测试收集整理的预测badcase样本数据，将其作为种子问题进一步扩充了这部分训练语料的比例，目前新增55条badcase上相近的数据样本

2、跑通了利用随机搜索和网格搜索的超参数优化的方法，为了小步实验验证，针对仅训练的training_arg进行15组参数组合搜索训练，结果效果挺好，在之前的大部分badcase上也能够准确预测出来，badcase上仅少部分entity预测还是缺失，后续可以进一步使用贝叶斯优化算法，并结合接下来计划补充进来的基于badcase扩充数据来增强训练，大概率会有好的效果

工作总结[2025-07-18]

1、进一步实验通了随机搜索算法针对Roberta模型的训练配置+多任务权重的组合优化，新超参数组合在原有badcase数据集有进一步的改进，原始预测会丢失的entity能够准确召回

2、实验普查了代码在训练上是否有错误的地方。排查后发现的异常信息是，验证集的entity与relation的F1值很低，怀疑可能是数据稳定性方面的错误或者数据比例失衡导致的。目前初步实验研究了一下metric计算上的设计，但还没找出问题原因

3、另外同步针对之前测试收集的badcase进行扩充数据的标注，这部分又增标了30+的语料样本



【上周工作总结 2025.07.14-07.18】

- 修复关键BUG: 解决了`subdomains`分类器的编码对齐、训练结果保存与可视化等问题，模型四大任务中的（grammar、subdomains）任务得分能到90+，基本可用；但entity和relation 识别仍是薄弱点，已开始聚焦优化。
- 引入自动调优: 应用随机/网格搜索找到更优超参数组合，之前预测失败的“badcase”目前大部分已能被模型准确识别。
- 精准增强数据: 针对模型弱点，定向扩充了80+条与“badcase”相似的高价值语料。

【本周计划】

1. 解决F1值异常: 彻查验证集entity/relation F1值过低原因，分析数据分布与评测代码，必须确保评估体系可信。
2. 应用新数据与贝叶斯优化: 完成80+条标注数据并投入训练，并引入更高效的贝叶斯优化算法，继续攻克疑难样本。
3. 实验解决“关系与实体抽取”: 集中精力解决（entity、relation识别任务），尝试针对性数据增强与Lora方式训练模型。

---

工作总结[2025-07-21]

1、遇到并解决了模型加载错误的问题，排查后发现是因为修改扩充了label范围导致原有模型输出size对应不上

2、实验调试entity与relation的验证集F1值极低的问题，打印输出了很多之间loss已经混淆矩阵，但这块目前还没有明确的发现，预测根本原因还是任务难度不均衡导致这两个任务学习失败

工作总结[2025-07-22]

1、排查解决随机搜索算法更新的训练配置参数设置前后不一致的代码问题，解决无法还原之前模型训练效果的问题

2、梳理当前模型多个任务的输出头（线性层）的设计，初步调研研究多不同难度任务的分离训练方法

工作总结[2025-07-23]

1、排查并解决了116的容器上网络默认代理对api服务的影响，避免了容器劫持测试开发的api请求的问题

2、实现把之前训练的当前中间效果最佳的模型做成api的形式，方便后续快速验证效果并迭代。目前在修复导入复用 `my_roberta_conversation` 类方法中出现的bug问题

工作总结[2025-07-24]

1、基本解决并实现了当前已训练的多任务Roberta模型在服务器容器中api接口服务，后续可以对接项目助理的决策树的实体名词来进行对齐和预测测试

2、实验跑通了冻结参数的分离式Roberta训练策略，解决了期间碰到的很多如batched张量不匹配、实例方法调用不正确等等问题，目前我主要是分冻结entity和relation参数的阶段一与冻结grammar和subdomain参数的阶段二来尝试进行优化训练。这条路线的预测效果还需要进一步实验测试

工作总结[2025-07-25]

1、实验了训练超参数优化的目标从combined_f1更换为evalLOSS，以减轻entity和relation任务极低F1值对优化配置训练的影响，实验下来发现确实优化的超参数组合确实能够出更完美的loss曲线，但是entity与relation测试效果却降低了，预测是新优化目标让模型学会了避重就轻，偏向训grammar和subdomains的简单evalLOSS

2、对接雷鹏那边了解了问答领域的数据情况，他这块目前基本都是脏数据LLM都不好分辨，还需要等他那边问题决策树的节点名词出来再对齐Roberta的实体标签。关于早期阶段的Roberta模型的API接口我已经开放给雷鹏去试用了

3、下午把最近标注好的badcase的补充数据集50个样本补充进去，实验发现针对badcase的问题是可以通过不断补充垂域相关的泛化数据进行有效改善的



【2025.07.21-07.25 周工作总结】

- 初始模型已封装为API服务： 成功将之前的阶段训练最优Roberta模型封装为API接口，已开放给R组用于对接项目助理的决策树，并明确后续需基于“决策树节点”进一步优化实体标签体系与效果验证。
- 模型训练与优化调整
  - 解决因 label 扩充导致模型加载异常的问题；修复了训练配置不一致导致随机搜索效果复现失败的问题。
  - 实验了冻结参数的分离式训练策略（entity/relation 阶段一，grammar/subdomain 阶段二），成功跑通训练实验流程。目前观察发现loss曲线很好，但是意外的是entity部分样本会丢失。说明后续需单独去训练entity/relation
- 验证数据策略有效性： 
  - 实验证明，新增 50 条 badcase 补充语料，实验证实对薄弱预测样本具有提升效果。同时，也验证了单纯优化loss会导致模型“避重就轻”，忽略`entity/relation`任务。

【本周计划】

1. 解决entity/relation任务F1低的问题： 实验entity/relation单独训练模型的策略，评估是否能平衡难度差异。通过调整训练顺序与超参数，找到提升`entity/relation`任务性能的最优组合。
2. 持续精准增强数据： 根据分阶段训练和API批量测试中发现的新badcase，继续定向扩充增强训练语料，特别是针对`entity`抽取任务的复杂样本，构建“问题预测-数据失败回流-模型增强”的闭环优化流程。

---

工作总结[2025-07-28]

1、基于之前实验的冻结神经元训练效果不佳的基础上，尝试另外一条路线-实现拆解4任务训练器为entity和relation的双任务训练器

2、目前已经解决了针对双任务的网络架构、数据处理以及labelMAP对齐的等等诸多bug问题，第一次实验下来发现，相较于之前冻结四任务部分输出神经元权重的效果模糊的实验，单独抽离出两任务的Roberta模型是确定的可以提升模型在抽取entity上的稳定性，减少entity_positions之间交叉的错误问题出现

工作总结[2025-07-29]

1、排查解决了entity/relation的loss计算与labelmap双任务对齐问题，基本跑通了单独针对双任务模式下的Roberta训练与推理，手动测试下来，分离后的双任务效果是要好于四任务的输出的对应位置

2、尝试同步的把grammar/subdomains双任务也抽离出来，这里还有点bug，grammar/subdomains是基于cls语义向量计算metric的，需要适应l不同的oss调试方法

工作总结[2025-07-30]

1、基本跑通实现了两个双任务的Roberta模型的训练与推理过程，解决了grammar/subdomains基于cls语义向量的loss配置问题。

2、同时我实现了把两个双任务的预测输出进行拼接，实测下来拼接结果效果是有改进的，entity命中概率有较大提升。所以基于此我已经把意图识别小模型的API端口更新到8000/api/v2/predict的两双任务的推理输出版本

3、调研学习当前语义识别任务上的强化学习技术路线

工作总结[2025-07-31]

1、排查解决了导致`span_f1`和`relation_f1`分数极低的根本原因，数据预处理与评估不一致。在预处理阶段`span_entity_labels`被保存为列表格式，但在`compute_metrics`中，代码期望的是张量格式来提取第三列（标签ID），导致，代码始终创建了全零的虚拟标签。目前已经将预处理的列表与metric的张量统一分开处理。

2、实验并学习PPO方法的路线，目前实践是初步简化奖励模型为原始两个加权的F1、以及新加position重叠负奖励。但是目前看来RL框架还是比较复杂难理解，还需要和AI进一步协作来试验搭建

工作总结[2025-08-01]

1、排查解决了库的版本环境的报错问题，解决了更新强化学习库同步改变了之前的sft的包，导致一些推理时候forward函数不兼容的报错

2、试验并理解DPO与PPO在文本处理上的实验示例代码。自己在初步实验DPO的偏好数据的微调方法，但是依旧回到了数据处理上，目前准备通过基于实体位置+F1分数对比来筛选badcase用于构造新的偏好数据，但是还是有模型预测结果莫名其妙的问题以及一些实现上的bug需要解决



【2025.07.28–08.01 周工作总结】

- 多任务模型结构重构优化: 成功将原四任务模型拆分为两个独立的双任务模型（entity/relation 和 grammar/subdomain），发现entity识别稳定性显著提升，同时更新了/v2 API接口。另外分离任务的模型也是为方便针对性DPO强化学习实验
- 指标异常定位与修复: 成功定位 span_f1 和 relation_f1 长期极低的根因，修复了预处理与 metric 不一致导致的全零标签问题。方便后续将F1作为奖励的一部分。
- 强化学习路线探索: 已启动基于强化学习（PPO/DPO）的优化路线探索，并初步完成了对DPO偏好数据构造方法（F1加权+位置重叠惩罚）的实验。当前主要问题集中在偏好数据构造逻辑与部分 bug 调试。

【本周计划】

1. 继续完善 RL 微调流程：从技术探索转向具体实施，重点构建用于DPO的偏好数据集（筛选好/坏预测对），并进行首次模型微调实验。
2. 研究PPO/GRPO的奖励模型搭建与训练的实现方法，尝试进一步利用构造的偏好对来训练专门的Reward Model

---

工作总结[2025-08-04]

1、排查出构造偏好对样本时基线模型预测输出质量极差的原因是因为库的函数功能不一致，在huggingface的transform库中模型配置加载的` from_pretrained`方法与BertConfig.from_pretrained在加载基线模型权重上不同，这就导致预测输出老是与预想的不一致，目前已排查并解决

2、另外在DPO强化学习的偏好三元组数据集构建中还遇到基于规则与F1分数来筛选原始数据集比例异常的问题，我之前实测是存在个别的实体预测不准或丢失的badcase，但是目前筛选逻辑却统计显示大部分都是高F1预测，这也与预期的比例对不上，目前问题还在进一步微调修复中

工作总结[2025-08-05]

1、实验通了利用固定规则与样本F1分数作为简单奖励模型的方法，但是目前测试出来的输出不符合预期。基于F1分数与整体平均分数对比的方法会把绝大部分样本判为高质量样本（异常猜测是由于单样本实体数较少的原因），而基于规则（实体重复、实体位置重叠、空结果）的方法的负奖励计算涉及的样本比较少。后续应该先试着优化F1值的判定

2、参加AI小组周例会；完成总结2025上半年绩效评估责任书

工作总结[2025-08-06]

1、基本跑通了基于多规则筛选与F1分数评估的DPO三元组数据集生成，能够把所有badcase返回给我并构造多样性的chosen-rejected pair样本数据集。这是在已有数据集下的意图识别模型迭代优化的数据闭环基础

2、梳理当前迭代的记录内容，方便与王工讨论强化学习的研究现状，并整理之前意图识别模型API接口文档以及数据集和lable抽象化解释的文档，初步给周晖说明当前意图识别的结构与现状

工作总结[2025-08-07]

1、排查并解决了构造生成的偏好对数据集JSON中relation的结果缺失以及idx重复的问题

2、整理最近跑通的SFT训练和构建的RL偏好数据集的阶段记录内容，同王工讨论现状和具体效果以及后续规划。重点设计评估的侧重点与边界情况，然和协同AI来填充具体的评估方法，暂不考虑扩大识别的范围

工作总结[2025-08-08]

1、调研整理意图识别的评估方案，协同AI来设计具体的评估指标，设计评估数据集的构造，目前最缺的还是数据集

2、整理之前的训练数据集频率情况，以及问题的类型分布，目前初步整理了测试评估数据集的生成方案，以及一部分待标注数据



【2025.08.04–08.08 周工作总结】

- 偏好数据生成闭环: 成功实现基于多规则（实体重复、位置重叠、空结果）与F1分数的badcase自动筛选，现可稳定生成DPO所需的偏好对（chosen/rejected）的新数据集。
- 关键BUG修复: 解决了因模型加载权重方式不一致导致的基线预测错误，以及生成偏好数据JSON中relation 缺失、样本 idx 重复等数据一致性问题，为DPO流程以及模型评估的设计扫清了障碍。
- 评估方法与测试集设计: 已完成意图识别模型的评估方案设计，并完成了初步评估数据集构造的思路与部分待标注的测试样本。

【本周计划】

1. 构建并标注评估数据集： 完成首批标准评估数据集的标注工作，为量化模型性能提供可靠基准。
2. 量化DPO微调效果： 在上周生成的偏好数据集中进行首轮DPO训练，并在新评估集上，对比RL微调前后的模型性能，用数据验证DPO策略的实际提升。

---

工作总结[2025-08-11]

1、参考之前的变种问题筛选构建了33个评估样本的数据集，并在label studio上完成新的评估样本的完整标注

2、构建了初步的基于F1分数的基础评估脚本，进行分数计算，发现grammar、subdomains的f1都非常接近1，然后entityf1在0.7这样。

3、我从数据集上详细对比发现根本问题主要是在数据集的质量上和评估方法设计上，首先label体系边界划分还是不够完善，然后我前后标注的时候也会存在少量不一致情况；其次，目前entity测试效果主观看还行，但是F1作为评估指标是字符级别的，计算分数比较苛刻，所以分数虚低。还需要进一步研究合适的处理办法

工作总结[2025-08-12]

1、实验在评估数据集的一致性标注中引入AI打标，实验了结合钉钉多维表格的AI功能，写好包含label框架的初步提示词来批量对表格数据进行多任务拆解的打标。初步结果发现grammar、subdomains确实可以很快很稳定的生成标注，但是到entity上相较于我自己标注AI很容易缺省部分实体，相当于AI的精确率高但是召回率低

2、梳理并优化了评估结果对比的可视化方法与脚本，目前能够更直观的找出评估测试结果中badcase到底是预测的问题还是标注本身的问题，基于此就可以进行数据的迭代优化

工作总结[2025-08-13]

1、排查评估结果relation分数低的问题。从可视化发现除了entity中的一些label标注歧义问题导致relation不匹配外，还有relation的评估集中缺少负关系，这部分写好修复函数后f1效果有少量提升

2、继续深入数据集层，通过评估对比结果发掘标注中不合理的以及错误的label体系，修复比如panel和box容易混淆标错的标签等等。然后梳理略哥那边的多个表结构的实体，目前正在把这些正在用的新实体都融入到label体系中

工作总结[2025-08-14]

1、完整的把略哥那边的外部多个表结构中的所有实体全部融合到当前entity实体框架体系中，同时也完整重构了旧版的标签JSON，从 "my_roberta_entity_labels.json" （82个实体）修改为 "entity_labels_v2_final.json"（184个实体），实体范围更全面且分级分离的更清晰，彻底解决之前标注体系存在少量混乱和歧义的问题

2、目前已经部分实现针对重构的entity标签标注数据转换到新新框架下的标注数据，比如1to1和Nto1标注目前已经通过函数的方式转换新版本标注，还需要针对1toN的样本进行单独处理（原始仅7类一级抽象实体） 转换为新entity框架（18类一级实体）。后续新的label必定更具兼容性能，同时也减少标注的混淆性。

工作总结[2025-08-15]

1、针对昨天重构的新的entity标签框架进行原始标注的映射优化修改，处理脚本无法自动匹配的1toN标注分配的转换问题，目前已经完整的把原始标注转换为新184个实体框架下的数据集

2、针对标签数据集完成了映射的校验和人工检查，最大程度确保数据集中不再有歧义的标注情况，目前可以用新数据集训练Roberta的意图识别模型，实验了一下，可以在略哥的外部表结构实体数据上构建问题进行一部分的意图分类（因为原有语料包含部分表结构中的新实体数据）

【2025.08.11–08.15 周工作总结】

- 重构并扩展核心实体标签体系： 完成实体标签体系重构，解决评测中发现的标注混淆，同时全面融入外部表结构中的业务实体，实体数量从82个扩展至186个，大幅提升体系的清晰度与覆盖度
- 完成配套数据迁移与测试验证： 设计方法将全部旧标注数据自动映射至186的新标签体系下，并完成了人工校验。初步训练验证，新训练模型也能识别大部分新业务实体，可以提供接口接入试用
- 评估体系构建并引入AI辅助： 构建并标注了 33 个新评估样本，实现基础 F1 评估和可视化脚本；发现 entity F1 ≈0.7，grammar/subdomains F1≈1。并实验了引入AI来辅助标注，但是效果一般。

【本周计划】

1. 按需扩充新实体相关语料： 分析新模型在新实体上的识别短板，定向补充相关问答语料，快速提升模型对新业务的覆盖度。

2. 优化评估指标与模型续训练： 使用迁移和校验后的186实体新数据集，训练新版意图识别模型，并在评估集上进行评测，同时优化评估方法以避免字符级F1带来的分数虚低问题。

---



工作总结[2025-08-18]

1、实验修改偏好数据集为transformer的TRL框架下的数据格式，但是目前遇到dpo训练的难点，使用trl框架需要添加包装器将多任务输出的模型头转换为生成模型的CausalLM格式

2、尝试简化之前构建的新的entity标签框架，设计并优化其中部分过于细化的实体名称。另外同步的修改部分标注数据的映射逻辑

3、整理了意图识别的模型评估、框架拓展的应用、dpo训练实验的现状以及问题，和王工确认了以标注数据优化和新实体语料补充的核心研究优化路径，后续会进一步筛查原始数据和标注中问题样本和测试的badcase，用来进一步迭代优化

工作总结[2025-08-19]

1、整理原始标注JSON数据集的时候发现之前扩展的标签新框架有点冗余，手动处理1toN的原始映射转换的时候不是很好打标，所以我重新简化了一版本标签文件到entity_labels_v3_final.json的第三版，从之前的v2版本186个实体简化提炼到v3版本146个实体，既保证领域实体覆盖，也确保打标减少标签重叠的歧义

2、重新调整Roberta模型的标签和新框架训练后的模型配置，同时也将API更新到最新兼容的新框架模型，方便模型接口能同步的支持略哥那边的新表结构实体数据，实现后续数据及模型的双迭代优化

工作总结[2025-08-20]

1、整理完成意图识别小模型的接口集成代码，通过装饰器的方式成功嵌入A组的问答工作流中了，目前可同时返回Roberta和LLM的意图识别结果，方便后续数据和模型迭代

2、调研了R组的问答工作流情况，R组是先对用户问题进行三分类（chat闲聊、download文档查看、query信息咨询），再考虑回答，且会拼接约多条历史记录辅助识别意图。所有可以先仅尝试支持对query的小模型训练以替代这部分的大模型回答。而R组的槽位比较复杂不固定，依赖大模型根据大致信息做决策，所以这部分先不做替代的模型训练

工作总结[2025-08-21]

1、为了解决实体匹配的评估中采用集合等价即“完全一致才算TP”，否则要么是FP（多报）要么是FN（漏报）的局限性问题，新增了Partial-IoU（实体重叠率大于0.5都算tp=1） 和 Semantic（实体有包含关系算tp=0.8，全等算1） 包含两个补充评估指标，可以避免因为严格匹配的f1而压低指标

2、对A组以及R组的测试数据进行整理并结合GPT生成部分变体语料。目前标注了一小部分样本，同时实验利用AI表格的方式也实验了自动预标注一部分减轻标注工作量

工作总结[2025-08-22]

1、排查并修复了新的IOU宽松评估方法中，会把所有评估TP的统计都视为子串识别的问题进行处理了，目前可以正常实现对A组提供的测试集问题进行合理的近语义级别的宽松匹配评估，F1值从之前压低的0.69提升到更加合理的0.76，说明绝大部分样本都能很好的进行意图识别与槽位提取

2、继续进行一部分A组训练样本的label studio的手动标注，同时结合多维表格的AI功能结合新label框架进行微调提示词逐步实现更加精准的自动标注，避免当前AI标注容易出现错误的entity_idx位置值。目前自动标注以可以部分拿来作为人工标注的补充和参考

【2025.08.18–08.22 周工作总结】

- 模型集成上线： 意图识别模型已通过API成功嵌入A组的问答工作流，现与LLM并行运行，为后续对比优化和数据迭代提供了实战环境。
- 评估体系升级： 针对字符级F1评估过于严苛导致分数虚低的问题，新增了Partial-IoU等更合理的近义匹配评估指标，使A组测试集F1分数从0.69提升至更真实的0.76。
- 数据框架最终定版： 为提升标注效率与准确性，将实体标签体系从186个精简至146个（v3版），并完成了模型与API的同步更新，已能支持部分新业务实体的预测。

【本周计划】

1. 数据生成与标注：扩充 A 组样本标注，结合多维表格进行预标注，针对R组工作流，生成并标注首批仅“query”分类的小型数据集；
2. 应用新评估体系定位badcase： 利用上周升级的评估脚本，定位在新业务实体与复杂意图上的识别短板，进行样本的补充与增强

---

工作总结[2025-08-25]

1、排查并解决了新JSON数据验证的函数报错问题，通过函数修正AI部分不稳定的异常标注，如Contract_Baseinfo -> Contract_Info、Action_Changed -> Action_Updated等等

2、完整的A组新框架的495个样本数据我都加入训练了，应该是确保了小模型上线使用的准确性和稳定性。（这块主要是已经实验通了自动化标注 通过多维表格AI和脚本修正与验证的方式完成）

工作总结[2025-08-26]

1、为Roberta小模型添加功能实现了非极大值抑制(NMS)机制，用于解决badcase中发现的实体边界重叠的问题。在实体预测循环中添加置信度计算，添加IoU计算函数，添加NMS函数，在推理流程中应用NMS，例如如果IoU超过阈值(0.5)，则抑制(移除)那些重叠的实体

2、通过LLM生成的方法，为测试数据添加了更多覆盖各种实体类型的代表性测试语句，确保测试集能够说明模型对大部分实体类型都有预测的泛化能力

3、调研整理王工规划的基于A组工作流过程的评估方案，从输入合规、意图识别槽位提取、sql生成、答案生成 这四个完整节点来分别构造整理测试评估方法

工作总结[2025-08-27]

1、调研整理sqlAgent评估的方案，并在AI小组例会上确定后续的评估方向

2、初步设计了一版MongoDB Agent 质量评估框架，针对比较清楚的输入语料层构建了比较清楚的语料扩充的结构方案、以及第二层语句抽取和意图识别基于我之前完善的意图识别方法也设计了更全面的评估指标。然后是sql生成和最终答案生成这两个不好确定黄金标准的评估层，主要从准确这个目标的侧面来设计规则性的自动化审查，在结合部分的人工主观筛选，来完成初步完整的流程评估。

工作总结[2025-08-28]

1、继续调研并完善针对MQLAgent的评估方案框架，特别是L3、L4两部分设计更加清楚的规则性检测评估以替换需要黄金标准的评估

2、初步详细整理了一下在L1层的评估数据构造需要的类别和意图子类的划分，当前独立的分为典型问题验证Agent能否正确处理定义明确的、可直接回答的各类查询（比如“查询项目负责人”、“统计项目数量”、“查询发货状态”等），包括能返回数据和返回“空/边缘”结果的情况。以及其他苛刻条件下Agent的稳定性测试评估问题，实体密集型问题、歧义问题、领域外问题和无效/恶意问题的类别分类

工作总结[2025-08-29]

1、利用初步整理的评估样本数据进行项目助理Agent的测试实验，目前发现Agent对于无效和恶意问题的抵抗拒答效果是蛮好的，但是对设计的歧义问题需要的澄清效果略微差点。

2、设计整理了一下针对L1三个量化指标的计算方法，通过对分类评估样本的预测效果的比例来说明输入层的合规性与鲁棒性

3、初步修改了评估方案的文档和王工讨论了一下，当前大体框架基本就是这样了，然后内部评估的分指标的细节还需要与A组对齐讨论

【2025.08.25–08.29 周工作总结】

- 调研评估框架设计： 设计了一套覆盖“语料输入-意图识别-MQL生成-答案生成”四层级的完整评估框架，并已开展初步测试，验证了Agent在拒答无效问题上的有效性。
- 模型训练与优化： 基于自动化标注，完成了A组全部495个样本的训练；并引入非极大值抑制（NMS）机制，解决了实体边界重叠的badcase。
- 评估数据构建： 详细设计了L1层评估数据的分类体系，覆盖典型查询、实体密集、歧义、领域外及无效/恶意问题五大类，为后续量化评估奠定基础。

【本周计划】

1. 研究检索A组各环节数据： 研究从laminar框架数据库建立提取所需任务数据的管道，根据数据情况同步并敲定评估框架的各项指标与细节，确保评估维度符合项目组需求。
2. 尝试优先跑通前两层评估实验： 利用新数据集对助理Agent进行语料输入、意图识别两个环节任务的初步评估，量化其在各任务子维度下的性能，明确后续优化方向。

----

工作总结[2025-09-01]

1、研究整理了一下laminar框架的使用方法，数据获取都在clickhouse库，目前先本地提取数据构建评估计算方法，后续就可将评估函数转移到laminar原生的自动化评估框架中

2、初步实践了一下结合dbeaver提取的数据进行单一追踪对话trace的探索性分析，针对每个trace分析当前划分的四层任务目标数据的字段情况，理清需要的目标字段如何抽取，目前这块还在进一步调试梳理

工作总结[2025-09-02]

1、进一步梳理针对单一追踪对话trace的探索性分析，从中筛选我们在L1层语料合规性评估所需要的文本内容，这里需要设计一些关键字段的匹配进行计算是否与黄金标准一致

2、初步设计了一些提取的脚本并尝试计算匹配的比例，但是目前在评估对象的提取精准定位上还遇到些难点，以及如何加入我之前分类好的语料的黄金标准也需要考虑一下嵌入的方法

工作总结[2025-09-03]

1、初步实验了问题语料的黄金标准与助理Agent匹配计算评估的方法，这里是修复了之前AI自己生成的基于关键词的语料分类评估的方法，之前评估对象有误。现在是基于我自己标注的语料分类来进行匹配计算f1值。但是目前有点问题就是对A组历史已输入数据的标注需要利用标注处理一下

2、L1层评估的管线初步理清并搭建了一版，但是目前还有部分处理上的bug需要针对性调试一下

工作总结[2025-09-04]

1、排查并修复了从laminar的数据库中抓取同一trace下对话数据错误以及抓取数据过时的问题，目前已经完整实现了从数据库中提取历史问题进行打标得到gold_category，这里结合了钉钉AI多维表格的方式快速自动化打标

2、也跑通实现了从助理Agent的中间环节回答中提取subAgent的最终决策。也就是当有明确接受或拒绝"valid"字段以字段为准，没有明确结果的subAgent则以"抱歉", "无法处理", "不能回答", "请提供更多信息",等等类似文本去匹配获取Agent对于输入语料的L1层决策结果，这部分细节后续再决定继续深入优化

工作总结[2025-09-05]

1、解决了之前从laminar数据库中匹配trace总是出错的问题，主要是API是没有trace_Id的，通过时间戳检索laminar的clickhouse数据库会出现UTC的时差错配问题

2、完整解决了在线评估的问题。原始项目助理Agent的API只能返回最终回答结果，现在实现了通过在线API接口参数匹配clickhouse中间环节数据库的方法获取中间数据的特定spans进行有针对性的完整评估

3、目前可以通过标注数据集批量在线请求完后进行完整的F1指标的计算。实现跑通了在线评估管线，包含批量API异步请求、trace等待、匹配查找，以及将评估数据集黄金标准与agent_behavior进行匹配评估计算。目前还剩部分评估细节待优化

【2025.09.01–09.05 周工作总结】

- 在线评估管线跑通： 成功打通了从批量API请求、匹配Clickhouse数据库trace、提取subAgent决策 到计算F1指标的L1层的在线评估全流程，解决了API缺少trace_id、UTC时差错配等关键数据处理难题。
- 评估数据与方法建立： 利用钉钉AI表格完成对历史输入数据的自动化标注然后人工抽查的方法实现了评估数据生成管线。并建立了基于“标注黄金标准”与Agent实际决策进行匹配计算的评估方法。目前20构造样本的L1层初步评估中TP 9、TN 1、FP 9，准确率: 52.6%、召回率: 100.0%（仅作参考）

【本周计划】

- 优化L1层完整评估： 目前初步评估管线（标注数据 - 批量预测 - 匹配评估）基本完整了，接下来调优语料分类以及匹配算法代码调试，确保黄金标准引入更精准，量化当前Agent的鲁棒性。
- 设计L2层黄金标准数据并进行评估： 在L1数据生成与管理的基础上，开始设计L2（LLM意图识别）的评估方法与评测集，不过这里要重点研究LLM实体系统的黄金标准label的构建。

---

工作总结[2025-09-08]

1、目前整个L1层评估的计算管线跑通了，但是计算设计的代码还有点bug（subAgent的计算优先级存在混淆），这部分还需要进一步调试

2、梳理了L1层评估的现状，目前实现clickhouse的成功解析，匹配对应trace_id的subAgent评估对象，也就是目前有了y_true，也能批量预测并获取到y_pred。但是对于指代subAgent的评估效果一般，这部分和王工以及略哥讨论后决定当前作为非重点指标评估，所以构造评估集数据会微调一下大致分布，以及自动标注的设置

工作总结[2025-09-09]

1、解决L1层评估中subAgent结果的优先级解析错误的问题。排查并调试之前代码中对于指代消解子Agent的输出结果总是被语义校验子Agent的结果覆盖的代码设计问题，同时也把这两个subAgent的评估指标进行区分，避免混淆

2、优化多subAgent评估结果报告生成的输出结果，调试日志的代码，这部分目前实现了通过表格的方式进行对比输出展示，这样同时也能导入到之前自动标注的多维表格进行对比观察

工作总结[2025-09-10]

1、优化代码。对L1层评估的报告生成代码做了进一步优化，现在可以直接查看问题是具体到哪个badcase，然后对应的subAgent是怎么样的输出，这样可以更加方便进行后续的提示词或者Agent迭代

2、深入调研审查了之前对于L3层评估的方案设计。在与GPT多次讨论和分析过后，将其原始的层次二和层次三的关键结果数据点召回率、基于黄金标准的逻辑形式准确率(LFA)替换掉，采用更加合理且容易的Cypher关键组件验证通过率、Cypher语法驱动通过率作为替代方案，这样可以显著降低对于评估集构造的难度，同时也修正了查询可生成性判断准确率的偏差，这个应该采用图数据库Cypher查询生成专家subAgent的complete字段作为匹配对象。

工作总结[2025-09-11]

1、结合昨天初步改进完整的L3层评估的方案设计进行相应的 评估数据集的构造，这里难点在于构建cypher的关键组件，避免生成完整的gold cypher。然后继续利用钉钉的AI多维表格来实现语料的自动批量打标

2、实践尝试搭建L3评估的管线，目前评估数据端数据处理与数据分析的代码调试没问题了，接下来要把计算指标的方法融入到评估引擎脚本中，这部分还在进一步调试

工作总结[2025-09-12]

1、排查解决了黄金标准加载器函数找不到测试用例的bug，以及评估管线经常出现对导入的clickhouse中间数据未找到Cypher查询的问题

2、当前用代码跑通了L3层评估的几个关键组件脚本，针对subAgent图数据库Cypher查询生成专家设计了三个易评估的指标，具体是围绕查询可生成性判断准确率、Cypher语法通过率、Cypher关键组件验证通过率来设计的评估引擎，目前实现的是一个“初版可运行”版本，但是多个指标的计算结果都有问题，主要难点在于找到这个灵活的“关键”匹配，所以引擎多指标的计算函数的代码调试上还需要进一步优化

------

【2025.09.08–09.12 周工作总结】

- L1评估优化： 修复了多subAgent评估优先级结果混淆的BUG，并优化了报告以精确定位badcase，完成了L1评估管线的收尾工作。
- L3评估方案细化与实现： 重新优化了L3（Cypher生成）评估方案，用（生成性、语法、组件）3个更易于自动化的指标替代了原有的“逻辑形式准确率(LFA)”；并基于新方案搭建并跑通了“初版可运行”的L3评估管线基础设施。

【本周计划】

- 调试并优化L3评估引擎： 集中解决L3评估管线中“关键组件”选取不准、导致多项指标计算结果错误的问题，确保评估引擎的准确性。
- 产出L3评估基线报告： 在评估引擎调试完成后，利用已构造的数据集进行首轮完整的L3层评估，量化当前Agent在Cypher生成上的性能，并生成Baseline报告。

---

工作总结[2025-09-15]

1、解决了“查询可生成性判断”指标一直无法匹配subAgent的complete字段的问题，排查根因是之前提取“裁剪过度”导致 complete 字段在早期筛选阶段丢失。

2、整理L3评估管线的代码框架及现状分析，同王工确认后续开发侧重点，重点围绕核心不变的组件展开评估指标设计

工作总结[2025-09-16]

1、梳理当前L3层评估的指标设计，并同A组讨论确认清楚了当前具体结合了表结构的查询配置subAgent对象为智能工具选择器，所以L3层的评估后续会增加指标，对结合了表结构的智能工具选择器生成的语句进行关键组件匹配评分

2、调整补充L3的评估方案的设计指标，增加并深化对实际结合了项目表结构信息的查询的subagent的效果评估设计

工作总结[2025-09-17]

1、对之前的cypher查询的关键组件匹配的脚本进行重构。结合之前编辑的从clickhouse中获取subAgent的输出的引擎以及spans结构解析等等函数的基础设施来构建L3层新指标“工具选择器”subAgent的输出内容的获取，这部分新增的指标的数据匹配也已经跑通

2、修复在新指标聚合查询准确率下无效问题语料也会被计算查询组件的得分率的bug问题

工作总结[2025-09-18]

1、成功重构并跑通了对智能工具选择器subAgent的评估的指标计算，解决了在这个subAgent下生成的MQL结构存在find与aggregate的两种查询工具的结构的代码不适应的情况

2、所以当前L3层的评估修改后聚焦在查询可生成性判断准确率、MCP工具选择正确性、集合（pms表范围）选择正确性、过滤条件组件匹配度，目前对于如果任务规划subAgent安排了多轮的查询这里还需要单独处理调试

工作总结[2025-09-19]

1、今天也修复了针对智能工具选择器subagent出现多次即多轮查询的情况遗漏的问题，通过处理查询工具、合集、查询条件这些关键组件的聚合策略，实现了即使多轮subagent的复合查询也能进行MQL子智能体的效果评估

2、排查并修复了针对无效问题的输入也会参与得分计算的问题，调试修复了工具和集合匹配逻辑，支持列表形式的多个选项，即通过假设简化认为多轮查询至少有一次查询是能够进入查到目标数据库的表就认为查询路径正确

------

【2025.09.15–09.19 周工作总结】

- L3评估重构与跑通：对L3（MQL生成）评估引擎和语料进行了重构。指标覆盖查询可生成性、工具选择、集合（表范围）选择、过滤条件匹配度4个核心维度。目前19构造样本的L3层初步评估中工具准确率: 52.6%、表召回率: 93.0%，平均查询条件得分: 0.64（仅作参考）
- 多轮复合查询评估实现： 解决了多轮查询 subAgent 匹配缺失问题，新引擎可以非常完整的查出MQL查询到底是在MCP工具、表选取、还是筛选条件上的出现的错误
- L4评估方案更新：重新调整了L4（答案生成）评估方案，将评估逻辑与L3解耦，从 answer 关键词覆盖率出发，而非依赖 L3 的 JSON 返回。将原有的“答案忠实度”更新为更易于实现的“关键词精准匹配”评估

【本周计划】

- 构造L4评估集：开始构建L4评估所需的高质量“问答对”黄金标准数据集，并利用AI多维表格等工具完成标注
- 跑通L4评估管线： 基于新的“关键词精准匹配”方案，同步开发相应的自动化评估脚本。最后输出整个端到端的评估基线（Baseline）报告



---

工作总结[2025-09-22]

1、跟GPT讨论并重新修改和设计了L4，改写了一下L4的评估逻辑为端到端的评分，L4答案层的效果评估与L3的查询解耦，不在关注或断言这些信息是否存在于L3返回的查询结果(JSON)中。将基于L3的答案忠实度改为answer关键词精准匹配，将从数据实际值或者关键字出发，构造精准的问答查询对，判断答案是否覆盖了用户问题中的所有查询点。

2、实践利用PMS的数据库表格数据构建黄金的查询问答对，目前初步构建了二十个涉及不同关键人员、组织和关联信息与边界情况的，并结合AI表格来管理数据。另外也初步尝试了复用L3的查询框架脚本来搭建L4的评估管线

工作总结[2025-09-23]

1、梳理L3评估的当前现状并整理L4更新的指标与实现管线的方案思路，然后与王工确认最终开发路线；以及参与AI周例会

2、重新优化之前使用Gemini帮忙生成的基于PMS数据库的L4黄金数据，但是测试运行的时候发现大量失败案例，主要是数据必须限制在当前项目号下，存在跨项目的问题时候项目助理agent就会很容易回答错误，所以这里又重新结合AI去调整新的黄金语料与标注

工作总结[2025-09-24]

1、整理并优化新的L4数据集的标注，加载使用新的黄金标准来测试L4的新评估指标

2、解决了评估引擎脚本匹配答案关键词遗漏的问题。目前基本调试通过和跑通了L4层的评估指标计算并生成报告

工作总结[2025-09-25]

1、进一步调试解决了L4层评估中对于否定类型的边界问题存在多种回答的匹配遗漏情况，增加了匹配的词汇表和近似关键词计算的方法

2、另外也优化了对于数值统计类型的计算问题匹配方法，之前部分评估会把项目号或者合同号中的数值也计入，这里做了正则的隔离预处理，解决了评估数值错位的问题

3、调研整理了一下如何搭建多AI基础模型的横向评测平台，初步设计了一下横向对比的方案，主要在于需要spans中新增并追踪各个评估层L1、L3和L4的模型attributes字段。这里还需要再整理调研一下目标开源LLM，确定最终的对比对象再与A组那边对接具体实现

工作总结[2025-09-26]

1、调研整理确定了6个合适的开源LLM作为横向对比评估的对象，然后与略哥那边初步对接确认了先新建其中4个LLM的Agent实例。目前相当于已经初步建立起横向评估的平台，但是需要进行全面的多模型横向测试

2、调试修复，因为评估数据文件的变更导致的L1出现报错的问题，现在优化了针对L1、L3、L4各层评估数据的导入模板，以及优化了运行Agent的实例接口URL的统一配置

3、因为L4层的评估管线已经优化通过，所以相当于整个Agent评估的框架都是基本完整的。然后目前初步进行了一步的横向评估实验，调试评估运行的一些报错问题，整个横向评估的报告结果还得进一步实验、调整和总结

工作总结[2025-09-28]

1、在多LLM的横向评估中添加不同模型对各层评测的耗时统计，补充了横向评估中最重要的效率指标的一环。在 ParsedTrace 类中增加聊天时间和持续秒数字段，实现聊天时间提取和解析功能。同时也修改报告模板，增加时间表现部分和各条目的耗时显示

2、排查并发现了新的Qwen-NEXT80B评估效果较差的问题，主要是外部的LLM调用测试评估被限制了并发数和每分钟token数，导致当前Agent工作流经常会有部分subagent会调用失败，从而导致完整的Agent工作流成功率降低。

3、测试发现阿里的魔搭平台免费调用最大卡点主要在每日可调用次数上，硅基流动平台付费调用评估的最大卡点在每分钟token限制上，因为当前针对每层的评估最少都是20个样例以上，所以为了确保评估的所有分布下的样本都能被测试到，最终还是以设置为硅基平台为准，约束评估的批次以解决外部LLM横向评估失真的问题

------

【2025.09.22–09.28 周工作总结】

- L4评估管线跑通： 构建了新的端到端评估数据集并跑通了指标评估的管线，实现基于answer关键词覆盖率的精准评分，解决了关键词匹配、数值统计错位、否定回答匹配遗漏等多个问题。目前15个端到端QA样本的L4层初步评估中答案关键词准确率: 80.00%（Qwen3-32B）

- 横向评测平台搭建： 确定 6 个对比开源 LLM，已初步接入并新建 4 个 Agent 实例Qwen3-32B（本地baseline）、Qwen3-Next-80B-A3B-Instruct（同系列升级）、GLM-4.5-Air（旗舰级对标）、Qwen3-Coder-30B-A3B-Instruct（性价比/差异化）。并新增了模型耗时这一关键效率指标的统计。

- 调用接口横评的问题： 在初步横向评测中，发现硅基的LLM会因API每分钟并发和每分钟token限制，或者接口不稳定导致偶发的工作流某个subagent失败，进而导致外接的整体工作流成功率降低。造成评测结果部分失真的问题。

  - L1层横向待调试目前仅L3和L4的部分评估分析如下（数据比较粗糙，数值仅供对比参考）
  - ![](https://img2024.cnblogs.com/blog/2045416/202509/2045416-20250930084922054-1099628456.png)

  

【本周计划】

- 输出多LLM横向评测报告： 优化调用批次与 token 限制策略，保证 ≥95% 样本能成功跑完；解决中间环节的部分代码问题后，对已确定的多个LLM进行一次更加稳健的、且覆盖L1-L4的横向评估，并输出首份（准确率 + 耗时 + 成功率）综合评测报告。
- 基于评测报告进行模型选型与优化： 根据横向评测结果，分析不同LLM在各评估层级上的优劣势，为项目后续的模型选型提供数据支持，并针对当前Agent工作流的薄弱环节提出优化建议。



---

【上两周工作总结 2025.9.28-9.30/10.9-10.11】

- **多模型横评平台搭建与跑通：**成功搭建并初步跑通了覆盖7+国内外主流与开源LLM的横向评测平台，并解决了外部API连接限制和稳定性的问题。初步评测表明，Qwen系列与当前Agent框架和prompt的适配性优于GLM系列。
-  **评估管线重构与升级：** 对L1/L3/L4评估管线进行了全面改进，L4评估解耦为端到端“关键词匹配”；L1/L3的评估逻辑也完成修正，可分别衡量Agent在**业务相关性判断**和**工具选择**上的能力。
- **评估数据集优化：** 针对前期第一次横向评估效果不明显的问题，重新设计并构建了更具区分度的评估数据集，分层设计了用于鲁棒性测试（L1）和精准度测试（L3/L4）的专用语料。

目前单纯从LLM计算的结果数据上得出的总结是比较符合评估的效果的，从模型尺寸大小、特化方向、架构差异、不同基模这几个方面分析简述如下：

- 单纯的Qwen模型尺寸和架构升级 (Qwen3-Next-80B)，并没有带来效果提升
- 更换不同的架构或者厂商的基模（GLM系列）并没造成明显得分差距，甚至新架构对当前框架和prompt适应性不好
- 从Qwen的QwQ文本理解任务系列升级到coder编程任务系列基本能够带来10%的当前整体效果提升
- 顶级旗舰模型GPT5确实文本理解和Agent能力各个指标基本都是最高水平，但是模型闭源而且中转国外接口偶尔不稳定，另外的国外Gemini模型对当前架构表现出极度不适应

具体的数值和案例如下（横向评估维度、纵向7个不同模型）：

![img](https://img2024.cnblogs.com/blog/2045416/202510/2045416-20251014090459435-1677670171.png)

#### **案例一：基础查询能力 (CaseID C001) - 能力基线**

* **问题**: “查一下项目I-23112109的名称。”
* **挑战**: 简单、直接的单实体单属性查询。
* **综合表现**: **所有模型均能轻松完成**。这表明对于基础的、模式化的查询任务，当前Agent的底层能力是稳定可靠的

#### **案例二：多条件组合查询 (CaseID C013) - 区分点 1**

* **问题**: “由销售工程师李杰负责的马来西亚MY06-PH2-BMS&CCTV改造项目，属于哪个一级销售团队？”
* **挑战**: 从长句中准确抽取出两个不同维度的筛选条件 (`项目名称` + `销售工程师`)。

![img](https://img2024.cnblogs.com/blog/2045416/202510/2045416-20251014090527646-163861778.png)

**案例结论**: 此案例极具价值，它揭示了**所有模型在处理看似简单的多条件组合查询时的普遍性困难**。没有任何一个模型能够完美解决此问题，大部分模型只能抓住其中一个条件，而`GPT-5`和`Gemini`甚至出现了灾难性的失败。**这表明Prompt的适配性鲁棒性是当前系统的巨大短板**。

#### **案例三：复杂嵌套查询 (CaseID C011) - 区分点 2**

* **问题**: “熊瀛担任一级项目经理的I-23112109项目，它的交付团队是哪个？”
* **挑战**: 理解“熊瀛”和“一级项目经理”之间的**归属关系**，并生成针对JSON数组成员的嵌套查询。

![img](https://img2024.cnblogs.com/blog/2045416/202510/2045416-20251014090559238-1185226244.png)

**案例结论**: 这个案例清晰地划分出了模型的**逻辑推理能力等级**。`qwen3-coder-plus` 在此展现了**无可争议的领先优势**，其对复杂逻辑的拆解能力远超其他模型。这强有力地证明了，在需要深度理解数据结构和逻辑关系的场景下，**为代码/逻辑优化的专业模型是不可或缺的**。

初步总结：模型的卡点主要体现在SQL查询层。一个高智商的Agent必须能攻克多条件和嵌套查询这两个堡垒。当前Qwen-coder30B综合比较上都比现在的QwQ32要强，甚至有些地方比GPT5这种通用旗舰模型还好

**【本周计划】**

- **优化并完善评估集数据：** 利用构造的精选高区分度样本，优化 L1/L3 的多轮任务检测逻辑，提高复杂样例评估指标的识别率
- **深度分析结果并提炼优化方向：** 深入分析横评报告，定位当前Agent框架下综合表现最佳的基座模型，并总结各类模型的通用弱点与特定优势，为后续的模型选型、微调或Prompt优化提供明确的数据指导。



---

工作总结[2025-10-13]

1、继续测试新增的旗舰模型GPT5等与Qwen的coder系列模型，基于之前优化了外部模型调用测试的稳定性，目前的横向评估基本都能保证因为连接失败的样本都2个以内，即LLMs的横向多样本的对比评估是足够公正的

2、下午完全输出和整理了所有优化语料下的7个对比模型的各层评估的输出，挑选了有对比意义的输出数据与指标作为比较分析的参考，初步的结论是单纯从LLM计算的结果数据上得出的总结是比较符合评估的效果的，Qwen的coder系列综合比较上都比现在的QwQ32要强，通用旗舰模型效果均衡但是耗时久而且API接口经常不稳定

工作总结[2025-10-14]

1、上午整理评估管线与横向对比的数据与最终的总结文档，挑选合适的典型案例方便AI例会进行讲解与讨论；参加AI例会确定评估效果与后续规划

2、下午归档评估的项目管线，然后调研在当前项目助理Agent中然后进行进一步的强化学习的后续研究

工作总结[2025-10-15]

1、上午把基于项目助理Agent工作流的强化学习研究路线初步整理出一个方案路线文档，主要包含RL的主要目标，引导意图模糊的用户，在一个预定义的“项目问题决策树”上进行推理，最终明确其真实需求。以及RL的状态和动作空间建模的设计方案，然后发给王工审核以确定最终LLM组的RL强化学习路线

2、下午与王工讨论了当前“项目问题决策树”上进行推理的RL方案可行性，以及后续LLM组需要收集更多的可能从模型、框架、输入等层面用RL能提升的地方。另外也与略哥那边确认了当前项目助理Agent优化升级的方向有哪些？以及痛点是什么等等。后续需要综合这些方案去制定LLM组更全面且完善的RL研究方向

工作总结[2025-10-16]

1、综合项目助理Agent的当前完整工作流初步调研整理了所有可能且有意义的用强化学习进行升级优化环节。总共整理从五个工作流节点中整理出八个能应用RL去改进LLM节点的探索能力

2、针对调研梳理的所有RL可应用点进行强化学习的可行性分析，分析不同工作流节点组件进行RL时需要的工作准备以及可能的难点是那些。最后将调研梳理内容初步汇总到强化学习战略分析报告的文档中

工作总结[2025-10-17]

1、针对A组当前工作流的MQL查询不准的痛点，结合调研的八个能应用RL去改进LLM节点的进行了对比并跟GPT进行讨论取舍，最后确定第一个RL方案基于强化学习的“智能查询路由”作为深入的方向，基于此我初步整理出了一个实验方案的文档，包含RL的动作空间、状态空间以及对应的奖励函数与SFT数据的获取的方法

2、整理的方案也跟略哥那边对齐了一下，初步思路可行，但是需要先进行技术试验看看是否补充的路由action对MQL查询结果有提升效果

【2025.10.13–10.17 周工作总结】

- **RL应用可行性分析：** 完成项目助理Agent工作流的强化学习（RL）应用调研，从5个工作流节点中识别出8个潜在可用RL的优化点，如查询路径探索、MQL约束遵循强化训练、RL优化实体识别与问题拆解等等，并做了可行性与难点分析，整理出RL战略综合分析报告。
- **智能路由RL方案设计：** 针对A组MQL查询不准的痛点，与A组对齐了确定首个RL应用方向为“智能查询路由”。已完成实验方案初步设计，定义了动作空间、状态空间与奖励函数。

【本周计划】

- **智能路由技术验证：** 技术试验验证补充路由action到系统prompt中对MQL查询准确性的实际提升效果，并为后续动作空间RL数据构建提供依据。

---

工作总结[2025-10-20]

1、深入研究了当前节点3问题归一化的prompt设计，梳理可能的元素作为state也就是RL方案的输入内容，初步确定state粒度的设计，只保留归一化问题（来自节点3）和已知的已知实体槽位以及最小Schema关系图，通过slots和设计的edges限定合法路径从而保证RL的输入不会过于复杂

2、初步整理研究节点4的NL→MongoDB 生成器的system与user prompt的设计，但是这里还需要进一步划分action的粒度和界限，难点在于router的引导与prompt的规则约束做好边界对齐与处理情况协调，这里尝试了一些可能的组合，目前都发现难以构造合适的action组合，这里还需要尝试进一步技术试验

工作总结[2025-10-21]

1、整理当前MQL查询路由的方案研究切入点，同王工讨论后续技术实验应该关注的对比点，主要是需要找到有区分度的查询数据集，然后通过RL训练专有模型来对比原始LLM节点在生成的MQL查询路径上的优劣，这块还需要深入的进行技术研究

2、下午继续研究设计state与action的组件构造与所有可能的路径组合，这里在设计action上还有个难点在于粒度难界定，如果包含了查询表的主体集合、查询业务的口径分类、再包含几个路径集合之后组合就会非常多。所以这里还需要考虑一下如何简化action空间的设计。以及后续需要基于action去针对性的去构造有对比效果的查询问题集

工作总结[2025-10-22]

1、基于PMS的表结构信息，针对强化学习的action空间构建了初步的20条路径模板作为初期的试验输出选项。另外也完整确定下来了RL的输出format，含Module业务口径（帮助选择预设的业务逻辑），CollectionPath搜索路径

2、调研整理了一部分历史的查询错误数据，并分析总结了后续做RL训练效果对照实验的数据构造板块与方向，主要围绕未发货（contract_bom 业务口径、双lookup链路）yu合同变更（contract→change_log 范式A单跳）yu合同变更（contract→change_log 范式A单跳）这两个旧流程易出错部分的进行数据构造与处理

工作总结[2025-10-23]

1、目前先跑通了仅包含项目助理Agent的节点3、节点4以及mongodb查询的对照组baseline工作流，通过快速针对性的复现MQL生成与查询的LLM节点并微调prompt来生成state状态空间与action动作空间的对照组

2、利用昨天初步手动构造的20个容易出错的MQL查询样本语料进行验证，语料主要围绕未发货清单与合同变更的细节来展开，目前初步对baseline对照组进行了评测，发现20个问题有2个无法生成查询、2个查询路径错误。后续还可以测试实验组用相同20个样例测试Router + Node4，是否生成MQL更准更符合查询路径

3、这对照组的初步情况说明了85%的"MQL有结果"不等于85%的"查询结果正确"，但是目前对于路径的处理还有点问题，后面需要进一步优化对照组的路径提取设计，需要进一步分析路径错误是否导致，返回了错误数据(如S3-02返回合同本身而非变更记录)、返回空结果(如S3-03、S3-08)这些badcase样本的根因

工作总结[2025-10-24]

1、针对昨天实验的生成的结果数据进行了分析，初步可以确认原始baseline（原Agent工作流）确实会困惑于在判断"是否需要$lookup"和"lookup哪些中间表"时最容易失误。特别是针对合同变更记录的相关问题很容易让LLM产生幻觉

2、另外也调试了一下代码判断了确实不是代码的原因产生的偏差，当前从原始Agent工作流提取查询路径path是准确的，能够说明当前baseline的纯prompt结构存在查询推理的缺陷。其次我也初步实验了一下把标注的router作为模拟RL生成查询路径嵌入到当前baseline的prompt中，发现嵌入Router能显著改善LLM的表选择准确率与pass@1

【2025.10.20–10.24 周工作总结】

- **RL方案输入设计：** 在之前的RL调研的方案基础上，进一步确定了智能路由RL方案的最小化输入`State`（归一化问题、Slots、Schema），并基于PMS表结构构建了包含20个路径模板的`Action`空间及输出格式，跟王工对齐了技术实验对比点
- **Baseline对照组实验：** 搭建跑通了对照组最简工作流（节点3+4+MongoDB查询），使用20个MQL易错样本（合同变更、未发货）测试。结果：4/20失败（2个无法生成，2个路径错误）。根因：原始Prompt结构在判断"是否需要$lookup"和"lookup哪些中间表"时最容易失误和产生幻觉
- **RL(模拟)方案验证：** 初步实验将标注的“标注路由路径”作为模拟RL生成的router嵌入Baseline Prompt，结果显示LLM的表选择准确率与`pass@1`从(16/20)提升到(19/20)，空查询返回从3个降为1个，验证了方案可行性。

【本周计划】

- **RL路由对比挑错与消融实验：** 用Router+Node4跑实验组测试其他的问题领域（物料细化、发货申请等），继续挖掘Baseline的路径错误样例，标注并分类典型Badcase，构建并沉淀扩充RL训练集与A/B测试集
- **RL动作空间收敛:** 通过扩充的测试数据集，深入分析路径错误导致的空结果和错误数据的根因，然后测试并简化action粒度与边界，迭代RL的生成推理路径模板

![img](https://img2024.cnblogs.com/blog/2045416/202510/2045416-20251027102714133-1125270624.png)

---

工作总结[2025-10-27]

1、梳理上周实现的模拟RL查询路径进行测试的数据分析问题，总结出目前Baseline（MQL生成）核心问题在于，原始工作流很容易出现表选择错误与$lookup方向错误，例如S3场景(合同变更)涉及2个表关联，Baseline 无法准确判断起始表,随机性强，导致MQL查询返回总是为空

2、所以我通过模拟RL实验组注入的router到prompt中，分析A/B测试结果数据发现可通过`anchor`参数强制指定起始表,消除歧义、通过`target`参数明确跳转目标,确保$lookup方向正确、Node4 Prompt的【Router指导】板块强制执行约束规则。结果LLM的表选择准确率与`pass@1`从(16/20)提升到(19/20)，空查询返回从3个降为1个，这也说明验证了未来RL方案可行性。

工作总结[2025-10-28]

1、上午把之前智能路由的对照组baseline与模拟RL实验组的代码进行了一次模块化重构，目前已经验证模拟的RL生成MQL查询路径是有益的。所以可以先把基于项目助理Agent的RL准备工作先做到这，后面按刘总要求和指导，先去深入的理解，并真正的实验出一个RL模型与学习项目，摸清楚各个RL概念的意义与作用

2、基于今天AI例会的讨论与指导，下午就深入继续之前的RL课程学习RL中的策略学习基础与价值学习基础知识，理解清楚基本框架概念等

工作总结[2025-10-29]

1、今天深入学习了一下RL中价值学习的两种策略网络的更新方法，分别是actor-critic和reinforce两种不同计算TD误差的方式，后者算是前者的N步时序差分的特殊情况，ac除了需要单步奖励的表示还需要状态价值函数值，需要两个函数来作为critic，而reinforce则是需要跑完全局的transition计算全局的奖励，各有特点和应用场景

2、其次也在hugging face的平台上运行并实验训练一个简单的RL且有环境反馈的模型，用stable_baselines3库来实现一个LunarLander的实验，这部分通过调包实现会比较容易一些，但是无法理解ppo库训练了哪些策略模型，所以后续需要拆解调试库里面为什么动作空间的设计是这样以及ppo的策略是如何更新的

工作总结[2025-10-30]

1、继续RL的理论扫盲，今天把基于价值学习的三种主流的优化方法学习理清了一下，分别是多步时序差分、经验回放以及with baseline的降低方差的价值更新方法，总的来说有了这些理论的了解后面来看PPO的前身，也就是TRPO算法能明白为什么强化的策略要这样更新，其次也找了一个很简单的数字解密题来尝试理解贝尔曼的过程

2、理论补充的同时实操，也继续在hugging face的平台上复习第二个Huggy的小狗游戏的案例，虽然案例比较基础，但这次在环境处理上遇到一点点问题，这也从侧面说明RL的环境配置的要求还是比较高的，需要理解环境、策略模型、价值模型等等的RL组件之间的关系以及配置方法，目前这个还在继续实验调试

工作总结[2025-10-31]

1、继续补齐对Q-learning的训练和推理的流程理解，理解在训练Q函数算法实现上MC与TD的区别与应用场景，结合Q-learning的伪代码来尝试实现了编写简单的使用Q-table的强化学习训练过程

2、对于实操过程，调通了之前的Huggy的案例，但是那个动作和状态空间都是库预设的，所以理解不强，但是后面手动练习了Q-learning的基于Q-table的案例，理解了动作、状态与奖励之间的联系，后续就可以把Q-table换成神经网络进行训练练手学习，以扩展到更大的状态空间的案例与项目

【2025.10.27–10.31 周工作总结】

- **路由查询A/B测试验证：** 补完了之前Baseline与模拟RL（注入路由）的对比测试。Baseline在`$lookup`场景下准确率仅为(16/20)。模拟RL通过`anchor`/`target`参数约束，将准确率提升至(19/20)，空查询由3个降至1个，验证了RL路由方案的可行性。并且完成了代码模块化重构，沉淀后续可复用A/B评测骨架
- **RL理论与实操学习进展：** 遵照指导暂停Agent强化摸索，转向RL基础。完成价值学习（Q-learning、AC、REINFORCE）与策略学习理论扫盲；实操HuggingFace（LunarLander、Huggy）案例，并手动推了一遍Q-table算法，掌握状态-动作-奖励机制。

【本周计划】

- **NN-RL模型实践：** 将上周Q-table实践升级，使用神经网络（如DQN）替代Q值表格，在更大状态空间（如CartPole）的环境中进行训练与调优，掌握NN作为函数逼近器在RL中的应用（争取自建一个简单MLP不调包实现DQN）

![img](https://img2024.cnblogs.com/blog/2045416/202511/2045416-20251103104440805-1649283708.gif)

---

工作总结[2025-11-03]

1、学习和整理笔记记录深度强化学习的经典方法DQN，并进行相应的技术实验，尝试了利用DQN学习一个价值网络模型来解决状态空间复杂时的案例，如hugging face上新的space invader案例，其中的游戏元素状态空间就是无法穷举的，类似我们当前的项目助理Agent中用户输入的问题类型很难去穷举，所以需要通过价值网络模型去拟合q-table生成Q值

2、也尝试了一下自己搭建一个CNN来作为价值网络进行DQN训练，目前这块还是在RL的环境处理上有比较多的问题需要一步步实验并解决

工作总结[2025-11-04]

1、梳理最近的RL学习与实操进展，然后参与AI小组周例会讨论当前方向与后续的指导方向，探索一下之前弄的意图识别的模型是否可以用价值学习的方法进行RL技术实验

2、继续调试DQN网络的代码跑通SpaceInvader的案例，这块目前已经解决了该案例中图像的处理问题，因为RL需要简化图像游戏的环境，所以在调试环境的时候需要对状态输入进行图像的tensor的维度进行转换，遇到一些bug

3、前面实验的是基于价值的学习方法，然后继续实验基于策略学习的策略梯度方法PG，这里也是一步步手动的构造策略网络和迭代的管线，这块还在调试实验

工作总结[2025-11-05]

1、在前面理清策略学习与价值学习的基础上，寻找并更换来更容易判断自己搭建的神经网络的RL学习效果的案例cartpole作为技术实验对象

2、在选定的案例上分别实验了之前学习的基于价值的方法DQN、基于策略的方法PG（都使用相同的MLP作为模型），得出的效果发现DQN会比PG训练收敛快点，这也响应了当前案例的动作空间较为简单这一现状（PG在奖励采样上效率低、DQN在动作摸索上效率低），所以通过当前简单RL方法实验也验证了当前学习的经验（基于什么样的案例与环境，选择怎么样的RL方法）；后续还可以进一步的学习并实验更复杂的A3C强化学习方法，查看不同的RL的提升效果

工作总结[2025-11-06]

1、继续填补了actor-critic这部分的理论知识和训练代码的实现过程，同样了在之前的LunarLander案例上进行了代码实操，结合效果分析，A2C的方法对于LunarLander这种奖励比较稀疏的情况会收敛很慢，分析因为on-policy对于数据的使用效率比较低，之前DQN只需要500次就可以训练通过，目前的A2C在1000次后依旧奖励值很低，但是A2C的训练速度快

2、另外也补充学习更加复杂和工业化应用较多的PPO算法，这部分还待实操实验。目前除了更前沿的GRPO、ASPO等等没补充，基本RL框架与逻辑梳理差不多了，后续可以尝试挑选真实但是简单的案例进行环境、奖励搭建，并学习如何构造RL的数据，然后训练不同的RL模型

工作总结[2025-11-07]

1、基于昨天学完的actor-critic架构算法，如A2C和PPO算法来手动搭建价值网络模型和策略网络模型（都是使用简单的两层神经网络作为策略模型）跑lunarlander案例，通过这种基础的env包来对比不同算法的特性与适用场景，同时也通过控制变量的方法实现了奖励变化趋势和损失变化的对比功能以及gif效果的对比

2、另外也探索了一下模拟真实场景下的PPO训练方法，打算使用预设的业务逻辑路径作为状态输入，然后设计为一个`64维`的、带噪声的Embedding作为状态，以弥补当前没有训练数据的缺陷，通过这种模拟的方法实验PPO在实际的这种奖励稀疏场景（回合制奖励）下的实现的可行性

3、分析了一下对于之前的基于RoBERTa的实体关系抽取任务中使用RL的方法去强化训练的难度会高一点，因为其中的输出的action空间相比当前查询路径规划任务更大，所以推荐还是以当前基础的表查询路径生成为初步实验目标

【2025.11.03–11.07 周工作总结】

- **RL主流算法实验深化**: 完成DQN、PG、A2C及PPO算法的理论学习与代码实现。在CartPole环境上验证DQN（价值）收敛快于PG（策略）；在LunarLander环境验证A2C（1000次迭代）因On-policy导致数据效率低于DQN（500次迭代），但A2C训练速度确实快；RL主流框架基本摸清了

![img](https://img2024.cnblogs.com/blog/2045416/202511/2045416-20251110155234211-1143400303.png)

![img](https://img2024.cnblogs.com/blog/2045416/202511/2045416-20251110155254671-307036271.gif)

- **RL应用目标精炼：** 调研分析了实体关系抽取任务相比查询路径任务的这种树action空间复杂度更高，所以后续优先以表查询路径生成为实验主线
- **场景仿真探索**：设计PPO业务路径模拟的简化实验，采用64维噪声embedding替代真实状态输入，搭建稀疏奖励（回合制）RL环境进行了初步模拟的RL技术实验，也第一次遇到了reward hack问题

![img](https://img2024.cnblogs.com/blog/2045416/202511/2045416-20251110174325438-1255472005.png)

【本周计划】

- **PPO模拟环境深入实验：** 针对MQL查询路径的稀疏奖励场景，继续实验如何构造更好的PPO模拟训练环境，实验最佳的奖励模型建模方式，**验证PPO在回合制奖励下的可行性。**
- 补充更多前沿RL算法知识和案例的学习，了解各种RL的难点与解决方法

---

工作总结[2025-11-10]

1、梳理上周学习的A2C和PPO算法以及梳理借助模拟环境实验PPO进行路径生成的代码，整理了初步的在RL上探索的一点总结，如在CartPole环境上验证DQN（价值）收敛快于PG（策略）；在LunarLander环境验证A2C（1000次迭代）因On-policy导致数据效率低于DQN（500次迭代）；以及总结了三种利用PPO在我们当前业务中（这种回合制奖励模式下）可能的训练方式

2、选择了初步采用最简单的单回合交互方式，即输入问题语料作为state、输出完整路径作为action，让PPO最多重复五次得到episode奖励作为训练方式，目前已经调试通过了代码实现，不过模拟实验的结果还得进一步分析为什么模型的模拟没有收敛到一次命中

工作总结[2025-11-11]

1、在模拟输入的RL实验上目前状态向量 =路径行为向量 * 冻结线性层(W映射层) + 噪声，目前尝试了许多技术实验让RL训练收敛到一次预测成功。如尝试不改任务设定，改激励结构，让“越早命中”比“反复拿高分不结束”获得更大的回合回报；延长训练episode + 提高探索概率等，但是都无法使得RL的训练收敛。

2、也尝试了通过消融实验方法和过程监控的方式，尝试比对噪声、网络容量等对应RL训练收敛性的影响，发现训练的一次命中率还是太低了，RL的训练，后续考虑补充学习RL训练的一些踩坑和技巧等，也尝试找一下是否有接近我们业务任务的RL训练的开源案例和数据

工作总结[2025-11-12]

1、在模拟RL训练实验上初步的实现了查询的一次命中的收敛，通过调整冻结层+噪声的输入编码方式简化为onehot编码，并优化奖励机制对时间步的惩罚，目前模拟的100次近8成用例可以一次命中答案，这也一部分的说明了PPO在这种回合制的生成任务上有训练的可行性

2、继上次AI例会刘总的要求，尝试结合当前我们真实的业务如表查询路由问题进行最新的RL实现框架设计，目前跟GPT讨论多次，确定AI自动标注样例的方法与初步prompt、真实路由查询的RL框架等等初步的内容以及目前需要补充的准备工作有哪些。后续就按设计来补充数据和必要的工作准备；

3、同时后面也会继续看一下找找是否也有成熟的且业务类似的开源项目与RL数据可以练习的，通过成熟案例来了解RL训练学习的坑和经验有哪些

工作总结[2025-11-13]

1、通过调研找到了与当前业务相似的基于GRPO的text2sql强化训练开源项目sql-rl-gen，同时搜集了部分相关性高的SQL问答数据集。目前初步调试了一下开源RL查询项目的环境，后续尝试能否基于当前完整的项目了解使用强化学习框架训练小模型生成SQL的流程与可能所有的坑点

2、另外，也将之前跑通的MQL查询模拟的RL训练的简单框架添加到当前真实的项目助理Agent的查询业务中，这里是初步的把真实场景需要的设计方案，如完整的状态空间、动作空间以及奖励机制补充完善，另外也优化了对AI自动标注SQL的方法与prompt，所以当前基于真实业务场景的RL准备工作差不多了，后续就可以参考跑通的开源项目来逐步实验当前RL查询路由的数据集准备、环境调试、强化训练

工作总结[2025-11-14]

1、完整跑通开源项目并验证了PPO可以很好的训练单回合制的SQL生成任务。基于IBM开源的sql-rl-gen项目和主流的Spader数据库查询数据集，实现了在现有小模型的基础上，再用“执行结果是否正确”来做强化学习微调。

2、通过跑通强化训练的SQL生成的开源项目，理清了之前的误区，明白PPO算法可以仅在一个单步回合制的episode下完成对于奖励机制的学习收敛，所以之前在模拟路由实验环境中使用在一个样本上重复的查询的长链条方式其实不是必须的。彻底明白简化的RL在一次性生成任务上的实现机制

3、另外也深入的去分析了一下当前项目用到的强化训练的Spader数据集，以及当前项目在处理PPO算法所需结构化输入的数据预处理方法。并且也挑了几个典型的样本进行了实验调试，对比分析RL前后的输出效果对比。后续就可以基于当前所有实现与学习分析的基础上进行我们自己业务的问题的强化学习的数据集构建，以及PPO的训练环境的进一步改进完善，特别是对于奖励机制的进一步实验完善

【2025.11.10–11.14 周工作总结】

- **同类开源RL项目验证：** 找到IBM开源的sql-rl-gen项目+Spader数据集5693条SQL，完整跑通基于flan-t5的PPO强化训练SQL生成流程；理清关键误区——PPO可以在单步回合制episode下完成学习收敛，不需要之前强行模拟的在单样本上搞长链条重复查询；深入分析Spader数据集预处理方法和RL前后效果对比
- **PPO模拟训练收敛：** 也解决了之前模拟的PPO训练不收敛问题，通过简化输入(one-hot)与优化奖励(时间惩罚)，在100次模拟测试中实现~80%的单次命中率。
- **MQL-RL框架设计：** 完成MQL查询路由的真实RL框架设计，已基于之前的验证与模拟重新定义了状态/动作空间、奖励机制及AI自动标注路由的Prompt，并完成初步训练代码集成。

【本周计划】

- **自建业务RL数据集**: 参考sql-rl-gen经验，构建MQL查询路由的RL训练数据集

- 启动真实业务场景（查询路由）的强化训练实验，在过程中完善PPO训练环境特别是奖励机制设计

  

---

工作总结[2025-11-17]

1、基于上周找到并跑通的开源项目sql-rl-gen继续深入分析其项目构建的流程和原理，比如RL训练中的observation_list、data_list_to_pass、columns_names_mismatch的数据类型意义与起到作用等等。其中也发现了一些当前开源项目的缺陷，通过借助项目中的评估引擎脚本我构建了基模与rl训练后模型的A/B测试分析模块，发现当前项目的环境奖励机制实际上是有缺陷的。很多SQL结果的评判指标都没有用上，原项目似乎是要借助于更高级的Eureka机制去动态构建奖励标准

2、不过Eureka这种动态奖励的超前设计对于我们的项目业务来说不是必要的，所以在经过深入PPO训练数据利用的解析之后，我给项目重构了更合适的奖励机制，并重新小规模训练了基于spider的PPO策略模型（因为当前4090在这个RL项目上训练很慢，大约半个小时只能跑100个step）

3、完整地优化配置并修改了项目sql-rl-gen训练的代码，将所有的T5模型组件、输入prompt模板都放到本地的27服务器上，目前这个开源项目基本吃透，而且训练、评估和对比分析基本都我优化完善了。后续不管是表查询路由还是公司自己业务的SQL生成都可以直接复用和升级

工作总结[2025-11-18]

1、上午把之前跑通的sql-rl-gen训练的结果与我们之前的RL训练进行了对比分析，区别是训练对象表现形式不同（单回合无环境交互 VS 重复查询的长链条交互）、输出不同（完整SQL生成任务 VS 查询路径选择分类任务）、reward计算不同（对比SQL查询结果差异 VS 对比路由与标注的差异）

2、然后也跑了2000step周的sql-rl-gen强化学习训练模型与基模的对比分析，发现虽然目前训练是没有问题的，但是reward并没有明显上升的趋势（即退化为SFT的效果），怀疑可能三点原因，1、训练step2000少于了样本量5693的倍数；2、奖励机制还是有缺陷；3、没有预先SFT冷启动导致的PPO训练不稳定

3、下午就回归到MQL表查询路由RL训练的板块，基于之前理解了的text2SQL的强化学习训练的基础来尝试构建我们自己业务的数据集，这里重新调整利用AI帮忙标注路由数据的提示词，不过这里需要解决标注提示词过长的问题，也就是需要和周晖那边确定动态注入的表结构提示词的方法怎么在这里去使用

工作总结[2025-11-19]

1、实验使用昨天从周晖那边拿到的历史MQL查询的数据以及自己生成的一小部分数据（目前有96条）进行AI自动标注，先利用这部分“正确标注”数据来调试跑通我们自己本地业务的训练环境和代码

2、目前在实验试跑Roberta模型进行路由分类强化学习训练时候遇到的第一个问题是，输入的token窗口不够的问题，Roberta极限是512个token，但是当前输入（问题50左右+表结构481）就已经超出了最大的上下文窗口，这部分现在初步的方案是先优化table schema，后续可以参考上一个开源项目sql-rl-gen对T5模型（也是512窗口，输入为{system}+{tables}+{question}）的输入进行tokenizer的预处理

3、初步调试通了基于MQL查询路由的强化学习训练，因为当前任务是直接匹配路径是否学准了，奖励路径较短反馈快，所有目前在4090上训练Roberta（目前只有96个样本）跑20000个step大约只需要半个小时。不过测试训练的指标结果还不行，还是之前那个问题reward hacking，模型的强化学习一直钻奖励的漏子，没有完整的学到路径决策树的模式。这部分跑通了后续就可以不断实验优化迭代

工作总结[2025-11-20]

1、继续在当前小数据集样本上调试训练，对比分析各组件训练的曲线变化，发现当前PPO训练表查询路由的卡点在于，基于标准答案路径的组件匹配奖励机制难适配数据分布的真实情况，模型可能逮着一个“容易”得奖的组件使劲薅（例如单跳容易，模型就猛学单跳有关的组件奖励，而多跳的更多组件没照顾到），反而整体的准确率不高

2、后续奖励机制可能还是要实验换成简单而直接的“生成值得信赖的 SQL 语句”，而不仅是语法组件正确的SQL，因为大模型困难不在于生成表面SQL的技术，而是SQL的执行逻辑。只要这个路由决策树逻辑训练通了，后续任意带逻辑的生成任务都可以用这种业务逻辑私有模型+大模型生成的机制

工作总结[2025-11-21]

1、利用之前跑通的sql-rl-gen开源项目上实验了一下奖励机制目标明确但奖励值稀疏的RL训练，这里设置了10000个step跑了一天都没跑完（这说明这种基于最终查询结果的RL训练链路过长会导致训练速度极慢），当前仅一半的训练记录也展示了新的训练目标会出现批量成功、失败的情况（即明确但稀疏的奖励会导致训练极不稳定）

2、同时今天也在基于我们自己的表查询业务实现了一下新的强化学习训练机制，即直接从路由对比结果获取奖励到mongodb返回查询结果的反馈中获取奖励，借助使用之前实验过的模拟查询路由对比分析管线，将强化训练下的Roberta生成的查询路由嵌入到“mongodb查询生成子Agent”的系统提示词中以获得改进提示词后的完整流程的查询结果，目前这块还待调试代码，并实现完整的200step的RL训练

3、在我们自己业务RL项目和开源项目的多次实验的背景下，深入的梳理整理了一下当前的逐步摸索RL的训练框架，以及不断优化之后的RL项目架构，并整成markdown文档记录下来

【2025.11.17–11.21 周工作总结】

- **MQL路由训练：** 构建96条AI自动标注数据，调试通Roberta路由分类强化训练（20000步半小时）；发现两大问题
  - ①小模型token窗口512限制（问题50+表结构481已超限），优化table schema暂时解决；
  - ②reward hacking严重，模型专薅"单跳"等简单组件奖励，多跳组件没学到，整体准确率不行
- **sql-rl-gen项目技术实验：** 深度优化`sql-rl-gen`项目，利用T5模型实验基于“最终执行结果”的稀疏奖励（链路长，100step花半小时）
  - 在4090服务器上跑了32小时的PPO训练的结果，奖励收敛到-0.5到1之间，发现75000步后有一个aha moment（模型不再出现**查空表名、Syntax Error、危险指令**的负reward情况），后面只剩下1与-0.5（即会的已会，不会的还是不会）——猜测可能是复杂SQL嵌套逻辑过难、模型过小

![img](https://img2024.cnblogs.com/blog/2045416/202511/2045416-20251124094007993-1872243297.png)

- **奖励机制重构：** 转向新机制：将RL生成的路由嵌入MongoDB查询生成子Agent Prompt，直接以MongoDB查询反馈为奖励信号。让模型学SQL执行逻辑而非答案组件，目前代码待调试；已完成RL训练架构文档沉淀。

  ![img](https://img2024.cnblogs.com/blog/2045416/202511/2045416-20251124150724037-673972323.png)

【本周计划】

- **新环境奖励机制验证**: 调试完成基于mongodb查询结果反馈的RL训练，验证能否解决reward hacking问题及提升复杂多跳查询
  - 解决“最终执行结果”的好坏自动评估的问题
  - 构造查询问题与标注路由的数据集200条

---







---





这是在4090服务器上跑了32小时的RL训练的结果，基于简单直接的目标得分规则奖励机制下的PPO训练的T5模型的奖励收敛到-0.5到1之间（之前还是训练太少了，这次实验发现这种SQL生成任务上每个样本都至少跑15遍才能有aha moment和一部分收敛效果），说明通过RL模型完全学会并解决了语法错误、危险SQL的问题，但是在spider测试集上的复杂查询依旧没学会——结论过小的模型即使训练集再多也无法完全理解复杂SQL的生成任务

目前在表结构查询任务上的PPO跑通了训练，但是模拟实际环境反馈的奖励机制比较难设计，模型很难收敛到预期的目标结果，





---

- 上上次总结
  - 分析在之前的开源项目sql-rl-gen上跑完了基于“最终执行结果”的稀疏奖励的强化学习训练的结果，因为这个基于结果的链路比较长，在4090的服务器上10000个step花了32小时，根据最终的reward曲线变化可得出结论，明确的奖励目标确实可以避免模型reward hacking，但是训练极不稳定，其次发现T5模型还是过小了，无法通过RL训练使其掌握复杂逻辑的SQL生成任务
  - 调试直接以MongoDB查询反馈为奖励信号的训练代码；另外整理所有实验的阶段结果并记录沉淀到文档中

- 上次总结

  - 整理当前所有的技术训练实验的数据与代码，然后与王工讨论了接下来的阶段性强化学习训练方案，明确针对当前RL训练查询路由不收敛的问题，后续要重点围绕数据构建和Roberta模型初期的SFT冷启动训练开展，这样后续模型的RL训练就会有更好的数据和模型处理能力的基础了；以及参与AI周例会的演示讨论
  - 下午基于前面的整理和讨论，重构当前的MQL查询RL任务的框架设计，比如简化动作空间，去掉并不是最关键的module动作中间件，只保留路径的起始、中间、终点表这三元组作为路径动作空间。目前这部分代码都重新重构并调试差不多了，后续就围绕这个路径组件的配置来设计SFT的问题文本数据的比例分布和数据量
  
- 总结

  - 修复并调试完全移除module动作中间件的一些bug；
  
  - 开始系统性的构造表结构查询业务的RL训练冷启动数据。先用表结构构造路径（控制好分布）；再用路径构造问题变体（控制好多样性比例），目前再次梳理了当前PMS的mongodb数据库所有表结构，并建立对应的简单的graph关系，依据此来关系来构建所有可能的合法path
  
  - 目前已经精筛出来了42条查询路径，但是部分路径（比如有些直接关联的单跳与多跳之间有争议）可能在后续构造多样性问题语料的时候再来单独修改。下一步将按路径分类来配比批量的构造Question-Path 对，这将作为后期RL强化训练的奖励组件的一部分
  
    

---





目前基于我们业务的表查询路由的强化训练流程能够跑通了，但是也遇到了强化训练中的卡点。现在我差不多也摸索清楚了强化学习的最大卡点有哪些，主要是合理基模的预选（平衡性能与算力数据要求）、数据集的构建（适配真实的分布情况）、奖励函数/模型的建立（模拟真实的反馈环境）

其中构建PPO训练的奖励函数的卡点在于，基于标准答案路径组件的匹配奖励机制难适配数据分布的真实情况，模型可能逮着一个“容易”得奖的组件使劲薅（例如单跳容易，模型就猛学单跳有关的组件奖励，而多跳的更多组件没照顾到），整体的准确率提升不明显



目前我结合一篇ACL中的生成SQL的强化论文总结了当前RL能够改进的方向主要在以下几点：

（换RL框架算法）在奖励稀疏的环境下（即许多早期查询失败的情况下），GRPO可作为专为噪声环境和有限成功率而设计的强化学习技术。针对Arctic-Text2SQL-R1的训练策略开始于对优化算法的仔细评估，发现GRPO 在 Text2SQL 任务上始终优于 PPO。

（换基座模型）像 OmniSQL 这样经过监督式微调的LLM变体——它们本身在指令执行和准确率方面就已表现出色——提供了显著优势。

（精筛数据）需要利用模型或者方法来验证合成查询数据，并仅保留高质量、正确的示例

（优化奖励机制）换成简单而直接的“生成值得信赖的 SQL 语句”，而不仅是语法正确的SQL，即最终的目标是让模型生成的SQL能够返回与标准答案相同的结果，而不仅仅是语法相似的查询语句（系统对比的是**SQL查询返回的结果**，而不是SQL语句本身）。因为不同的SQL语句可能产生相同的查询结果

（修改输入提示词结构）高性能的文本到 SQL 并非源于单一的突破。它需要将优质数据、智能初始化、合适的强化学习策略和精心设计的提示信息整合到一个统一的系统中



因为大模型困难不在于生成表面SQL的技术（例如略哥那边30B模型已经有八成成功率），而是SQL的执行逻辑。只要这个路由决策树逻辑训练通了，后续任意带逻辑的生成任务都可以用这种业务逻辑私有模型+大模型生成的机制

后续奖励机制尝试要换成简单而直接的“生成值得信赖的 SQL 语句”，而不是仅语法正确的SQL，在之前跑通的sql-rl-gen开源项目上试了一下目标明确单奖励稀疏的RL训练，效果如下（会出现批量成功、失败的情况）

![img](https://img2024.cnblogs.com/blog/2045416/202511/2045416-20251121160550814-1817331866.png)

![img](https://img2024.cnblogs.com/blog/2045416/202511/2045416-20251121162943549-775385911.png)

所以关于这些问题想跟王工讨论一下，看看您这边对于RL训练有没有什么改进建议和指导，以及后续的规划安排



---



- CADAgent待办
- [x] 上线钉钉
- [x] 修复web 上先预览才能生成DWG的流程
- [ ] 更新下载按钮
- [x] 更新workflow_state.md
- [ ] **解决双机双网的第二个环网交换机的问题**
- [ ] 解决位置与端口标注（问一下吴晶晶要什么样的）
- [ ] 解决数量统计标注
- [x] 提供图元下拉选项
- [ ] 提供更多布局配置参数控制
- [ ] 调试字体显示效果
- [ ] 支持线型的调整
- [ ] **支持Excel中设备层的图元选型（目前仅提供33M）**
- [ ] 增加回路号是否为空的校验
- [ ] 增加厂房的序号校验
- [x] 监控设备的名称赋值问题（似乎被type错误替代了）
- [x] 修复图框属性值的硬编码填充问题



- 待办任务

- [x] 完成上半年工作总结（根据去年底的一对一绩效评估word）

- [x] 14号下午一年培训期谈话

- [x] 工程系列职称评审材料提交核对

  

- 意图识别待办

- [x] 分析王工在项目信息领域的意图分类、问题分类

- [x] 梳理发货领域的问题分类、每一个问题数据的意图分类

- [x] 发货问题打标，命名实体识别（NER）标注神器——Label Studio

- [x] 定义问题抽象板块。意图（query、static、judge、reason）-实体（）-属性-动作

- [x] 查看一下各个分类器的训练权重

- [x] 修改一下模型保存指令的保存地址

- [x] loss可视化

  - [x] 修复可视化加载log过时的问题

- [x] 针对grammar的优化训练

  - [x] 精选50个grammar的优质数据集（确定是容易分辨的，适合训练强化模型能力的）

  - [x] ~~（或者更简化-只保留query与reason）~~

  - [x] ~~把grammar多分类问题移到subdomain分类器上（这个subdomains单分类器是更简单、易处理的）~~

  - [x] 尝试删除unknown标签（本身数据中就不存在unknown情况）

    - [x] 为什么删除了grammar中unknown标签，inference还是能输出unknown呢？——可能原始的grammar多分类中就已经编码对歪了（因为之前会预测出未标注的unknown），需要排查一下

      - [x] 检查是否./dl_models/hfl/chinese-roberta-wwm-ext已经被污染了（现在原始的都一直预测为query），模型之前的保存方式是怎么样的？——正常
      - [x] 为什么训练验证集内部的数据也无法准确预测？（这个就是与f1分数相矛盾的——要检查代码）

  - [x] ~~加数据继续训练RobertaOnGrammar看看是否有改进 + 同时同步的挑选其他小模型看是否有更好的效果~~

- [x] 对于grammar问题：1、增加权重；2、增加数据量

  - [x] 先尝试从0.1到0.3再到1来修改权重影响——**没啥效果**
  - [x] ~~使用**带权重的交叉熵损失**——抑制模型总是预测多数类别~~
  - [x] 尝试只单独训练grammar分类器

- [x] 对于relation问题：1、对于无关系的relation增加负例标签/None标签；2、减小权重；

  - [x] **负采样**，使用脚本将没有标注实体之间relation的加上none的关系标签**作为负例的补充**，缓解"所有实体对都有关系"的错误学习模式

- [x] 对于数据历史标注准确性和分类占比

  - [x] **分阶段训练**。阶段1：专注于基础任务（实体边界 + 语法分类）
  - [x] 对于`size mismatch for entity_type_classifier.weight`的问题——原有训练模型无法适应新增entity的标注JSON（需要在新JSON上重新训练）

- [x] 优化训练的超参数组合（分类器权重+训练配置参数）

  - [x] 使用Optuna的贝叶斯优化库—与Huggingface集成比较好（需要根据之前的经验，设置合理的超参数搜索范围）

- [x] 扩充数据、标注数据

  - [x] 目前都是prj-id居多，还需要补充问题数据中的prj-name的实体问题（类似的还有其他常问实体的问题也要搜集补充）
  - [x] 使用Gemini等大模型进行清洗数据的标注准确性

- [ ] 目前发现训练的Roberta好像对于类似`发货流程卡在什么地方？`这种很“短”的问题上实体提取和关系抽取效果不是很好

- [x] 只处理简单问题（先上线意图识别）、加数据

- [x] 在 compute_metrics 中确保预测和真实标签使用相同的索引基准

- [x] 尝试不同的阈值，可能需要针对不同任务使用不同阈值

- [x] ~~实验label-studio的自动标注-基于spacy 的 zh_core_web_sm~~

  

- [x] 可以先不管relation的分类任务（把这部分的分类器注释掉）

- [x] 解决entity识别正确性的问题

  - [x] 后续可以考虑用lora方式先把grammar与subdomains冻结起来（不再使用staged training）
  - [x] 把多问题的样本剔除或者拆解分开
  - [x] 检查当前eval的F1计算是否合理，F1值确实太低了（不合理情况）
    - [x] 探索是否需要更换eval的评价方法
      - [x] 就是这两个新加的F1值计算问题很大span_type_losses与relation_losses（代码为什么是单独的（疑点）？能否与position loss一致保持简洁？）
        - span_f1 = f1_score(valid_true_span_ids, valid_pred_span_ids, zero_division=0, average='weighted')有问题
        - relation_f1 = f1_score(valid_true_relation_ids, valid_pred_relation_ids, zero_division=0, average='weighted')
    - [x] **或者更换随机搜索的优化目标（从综合F1换成验证集loss）**——评估指标好，但实测效果不好，基于evalLoss似乎会导致模型避重就轻，忽视难任务的loss训练

- [x] 研究label studio的基于预训练模型的半自动数据标注方法

- [x] 先上线当前多任务的最好的训练结果的模型的推理接口

- [x] 解决生成的偏好数据中relation的idx都为0的问题

- [x] 探索强化学习RL在entity/relation上训练的路线与效果

  - [x] 搭建DPO训练的偏好对数据集——基于规则与F1统计指标的筛选方法__F1指标还得和AI讨论一下，不然单样本正常F为1的情况很多
    - [x] F1为1情况很多，但是不影响，基于单样本F1值，**(TP)**：模型正确预测的槽位数量。**(FP)**：模型预测出但实际不存在的槽位数量（“幻觉”）。**(FN)**：真实存在但模型未能预测出的槽位数量（“失明”）。可以通过F1去综合精确率与召回率来筛选出与真实样本对比效果不好的数据（规则筛选中不好匹配比对真实样本的情况）
  - [x] 搭建generated_samples_for_sentence脚本（真实样本、基线预测样本、扰动样本、阈值探索样本、极端错误样本 ）——训练奖励模型所需的数据（为一个句子生成5-10条扩充训练数据）
  - [x] 生成偏好对的脚本中随机损坏策略（如生成实体）是有问题的

- [x] 整理RL目前的探索——与王工讨论

- [x] 探索当前意图识别模型在项目助理任务中评估方法

  - [x] 构建意图与槽位case的知识库——使用多维表格管理（方便未来RAG/SFT）
    - [x] 扩展测试数据——从“用户会怎么问”的常规思维，扩展到“用户可能会怎么问，甚至是怎么问错”的极限思维。

  - [x] 评估流程（可复用的标准步骤）数据读取 → 批量推理 → 结果对齐 → 指标计算 → 误差分析 → 报告生成。
    - [x] 评估方法优化。新增 Partial-IoU（实体重叠率大于0.5都算tp=1） 和 Semantic（实体有包含关系算tp=0.8，全等算1） 包含两个补充评估指标


- [ ] 清洗现有数据 (评估集优先)
    - [ ] 确保评估集是**100%准确的“黄金标准”**。
    - [ ] 结合评估的badcase一步步对照着去清洗训练集进行重新微调
    
- [x] 调研构建意图识别任务领域label体系的传统方法——无系统性技巧发现

- [x] 增加最新的表结构实体到label体系中去
    - [x] 手动测试自动标注转换工具是否准确
      - [x] datasets/my_roberta_v2_traindata_entity_relation_only.json
      - [x] datasets/my_roberta_v2_traindata_0724_ALL_negative_noStatistic.json
      - [x] 不需要deprecate，确保原始标注所有label都可以完成转换
    - [x] 之后再开始新增标签的标注+ aug trial去重（如物料清单、）审查新label体系是否存在交叉重复
      - [x] 接下来你需要仔细审查新label体系datasets/entity_labels_v2_final.json中是否存在交叉重复或者其他有可以优化的标签内容
    - [x] 另外一个vscode调试DPO训练流程
    
- [x] 意图识别api的落地
  - [x] 整理新框架下的意图识别api给到和加入略哥他们的工作流中
    - [x] 通过装饰器套件进行request通信 或者curl指令方式
  - [x] 了解雷鹏那边的意图分类的数据与输入输出流，看能不能也加入Roberta的接口
    - 意图：提到RAG系统是基于大模型分三类意图：chat、download、query。先对用户问题进行三分类，再考虑回答，且会拼接约多条历史记录辅助识别意图。
    - 槽位：槽位范围大，用户输入模糊，难以精准提取，**且相近型号易出错**。RAG组通过决策树设定节点，引导用户补充问题信息，大模型根据信息做决策，虽槽位识别非精准提取信息，但决策准确度较高。
      - RAG 检索痛点问题：
        - **检索误差**：RAG 本身是概率性检索，相近型号和版本号易导致检索太多，无法控制token，如 5100 和 5100G、不同版本号等情况。
        - **权重识别需求**：检索时需识别关键词权重，避免因分词权重相同导致相似度误判，但实现难度较大。
  
- [x] 意图识别是分为多级意图分类（如grammar+subdomain：查询-发货/基础信息）+ **槽位提取**（抽象的槽位类型 -一级类别-二级类别-具体对象）

- [x] 研究一下当前Roberta模型网络结构下span识别为什么会出现span重叠的情况

- [x] ~~调研一下laminar如何通过API的方式获取执行的之间过程结果~~

- [x] _extract_agent_behavior_from_spans有bug，似乎都被语义识别的回答拦截了——修复歧义问题的识别评估失效的问题

- [x] 构造L3的语料（含complete属性、关键组件）——并搭建多维表格自动生成提示词

- [x] 在README中进一步细化黄金语料的与spans中的信息匹配用法

- [x] ~~进一步优化AI表格的cypher生成与laminar中实际cypher匹配度，同时考虑是否缩小cypher关键组件匹配的范围（突出“关键”，同时减少匹配对象波动太大问题）~~

- [x] 先和王工讨论一下他的意见，说明一下当前难点

- [x] 看是不是先从**查询可生成性判断准确率**、**Cypher语法通过率**这两个容易的先去优化；**Cypher关键组件验证通过率**太灵活了（待王工意见）
  - 先介绍为什么挑选了这三个作为指标（考虑到什么原因）
  - 再介绍前两者好调试，难点是第三个指标的组件不好选定-展示生成组件与Agent的组件的区别截图
  - [x] 王工的意见是，需要确定助理Agent中不变的（cypher可能会换）元素进行评估
    - [x] 评估集还是得构造，可以不多但是要精（先让AI整3-5条黄金标准评估数据）
    - [x] 问一下A组的sql数据怎么获取
  
- [x] 重新构建针对多LLM的多层评估的语料集，L3与L4的评估语料可以重构且合并，L1语料可改变类型分布

- [x] 再挑选一些对比的LLM，要求控制对比的变量（系列、size、特化类型）

- [x] 重构L1层的评估，将cypher识别整合到L1中，并调整优化L1的新报告生成

  - [x] L1管线修复-分三个subAgent的三个独立评分指标
  - [x] 数据优化——问一下Gemini最佳的L1数据分布构建
  - [x] 平台选取及报销——openrutor、云雾（打折优惠的中转API平台）——云雾的得等到11月初才能开发票
  - [x] 看一下L3测试语料为什么还有无效问题
  
- [x] 尽可能的梳理当前A组可用RL升级替代当前Agent节点的环节有哪些，整理一版可能的多个研究方向（不用含太多技术细节）然后跟王工与略哥他们讨论对齐一下

  - [x] 从流程环节上有哪些RL方向
  - [x] 从问题类型上有哪些RL方向（简单查询问题、复杂查询问题、为什么的推理问题）
  
- [x] RL的增量研究路线可以，但是改进用的action空间需要重构，因为有些查询是复合动作（prompt中有fewshot示例）

    - [x] 可以单独抽出节点4的prompt进行模拟，然后使用mongodb的数据库地址自助利用python脚本验证查询结果
    - [x] 作为优化prompt用的action空间需要先进行**A/B测试**等方式验证对badcase确实有提升效果（使用badcase测试集评估——改进前原始prompt vs 改进后嵌入优化的prompt）
    
- [x] 用RL跑通一个游戏——掌握技术

- [x] 找一个与我们业务类似的开源RL训练案例（含数据）

    - [x] 差不多理清了当前sql-rl-gen的实现流程与原理——现在可以做一个流程图（汇报用）、复现效果（演示用）
    - [x] 优化流程图；优化开源项目演示的脚本


- [ ] 或者直接上真实场景，使用AI表格构造路径查询的数据进行初步的RL训练实验（换为真实数据、和Roberta基础模型）业务场景：Router智能决策层

  - [x] 优化自动标注路由的AI提示词

  - [ ] 并行内容有，构建**业务RL训练（Text-to-MQL策略）**的数据、基模、训练框架
    - [x] ~~数据——使用PMS的真实数据构造问题（由答案反向构造问题-LMArena，prompt需要表结构和）、使用多维表格对问题批量的构造查询路径~~
      ~~baseline是可以直接从工作流中提取出原始Agent使用查询路径~~
    - [ ] 数据——先用表结构构造路径（控制好分布）；再用路径构造问题变体（控制好多样性比例）-LMArena
      - [ ] 构建的合法path有问题（原始graph有问题），project表下并没有contract，而且大部分表还有对应的【change_log】表
    - [ ] 基模——难点在于SFT要作为前提
    - [ ] 训练框架不能使用sb3，得自己搭建


- [ ] 或者还是并行，模拟实验继续探索其他RL算法（反正框架也搭好了，只需要微调部分板块即可）
  可快速产出可视化结果<br>- 算法对比有图有故事（上级关心换算法能否解决当前汇报过的问题-如reward hacking和不收敛）<br>- 操作熟练度提升







![RL_research_项目助理Agent-第 2 页.drawio](E:\vscode\vs_democode\sql-rl-gen\sql-rl-gen\data_preprocess\data\RL_research_项目助理Agent-第 2 页.drawio.svg)

相同点：
输入类似（问题+schema+标注SQL VS 问题+schema+标注路由）
强化训练框架一致（都是PPO策略优化、基模也都是小模型）

不同点：
训练对象表现形式不同（单回合无环境交互 VS 重复查询的长链条交互）
输出不同（完整SQL生成任务 VS 查询路径选择分类任务）
reward计算不同（对比SQL查询结果差异 VS 对比路由与标注的差异）

困惑点：
当前跑通了sql-rl-gen的强化训练，但reward并没有明显上升的趋势（即退化为SFT的效果），怀疑可能三点原因，1、训练step2000少于了样本量5693的倍数；2、奖励机制还是有缺陷；3、没有预先SFT冷启动导致的PPO训练不稳定

```mermaid
graph TD
    Input[Input: Token IDs + Mask] --> RoBERTa[RoBERTa Model]
    RoBERTa -->|CLS Token Vector （768 dim）| Features[Shared Features]
    
    subgraph Actor [Actor Network （Policy）]
        Features --> PolicyNet[MLP Extractor]
        PolicyNet -->|Latent Pi （256 dim）| LatentPi
        LatentPi -->|Linear Head| Logits[Action Logits]
        Logits --> Dist[Multi-Categorical Distribution]
        Dist --> Action[Sampled Action: Mod, Anc, Tar, Via]
    end
    
    subgraph Critic [Critic Network （Value）]
        Features --> ValueNet[MLP Extractor]
        ValueNet -->|Latent Vf （256 dim）| LatentVf
        LatentVf -->|Linear Head| Value[State Value V（s）]
    end
```

### 数据维度上的“重复利用”

PPO 的核心是：

- 使用 old policy 采样得到的一批 [(s,a,r)](vscode-file://vscode-app/e:/vscode/Microsoft VS Code/Microsoft VS Code/resources/app/out/vs/code/electron-browser/workbench/workbench.html)，
- 用重要性比率 [π_new(a|s)/π_old(a|s)](vscode-file://vscode-app/e:/vscode/Microsoft VS Code/Microsoft VS Code/resources/app/out/vs/code/electron-browser/workbench/workbench.html) 和 clipping 来更新参数，
- 同一批数据可以在 **多个 epoch** 里反复使用（提高数据利用率）。

> 在sql-rl-gen这个项目里，这点依然成立：
>
> - 对于每个 (observation, generated SQL, reward) 样本：
>   - PPO 内部会在 [epochs](vscode-file://vscode-app/e:/vscode/Microsoft VS Code/Microsoft VS Code/resources/app/out/vs/code/electron-browser/workbench/workbench.html) 轮中多次遍历这批数据做梯度更新。
> - 这就是“同一条数据多次使用”的地方，而不是在环境里再跑一次 episode。

## 再用一个对比 LunaLander 的类比总结

你可以这样对比，帮助直觉：

- **LunaLander：**
  - 每一步 action：发动机推力（连续控制）。
  - 同一条轨迹里，会不断调整推力直到落地或坠毁。
  - reward 在时间上分布，episode reward 是多个 step 的和。
- **本项目：**
  - 每一步 action：一整条 SQL（宏动作）；
  - 没有在“同一个样本上反复试”这条长链条，
    但有**很多个样本**和**很多轮训练**，跨样本与跨轮次形成试错过程；
  - reward 只在后面一次性给出，但 PPO 通过多轮遍历同一批数据（重要性采样 + clipping）进行稳定更新。





---





---



### 1. 核心挑战：当前Agent路由的“业务逻辑”瓶颈

- **现状：** 我们当前的LLM Agent在处理“Text-to-MQL”任务时，**严重依赖Prompt，缺乏对复杂业务逻辑的“可控”理解。**
- **痛点：** 导致查询**路径不稳定**、成功率低，**且难以归因和优化**。
- **目标：** 我们需要一个“**可训练、可迭代”的策略引擎**来接管“路由决策”。


## 🧭 推荐策略：**主打方向① + 包装方向③ + 预告方向②**

汇报方案如下：

1. **核心成果展示 → 来自方向①**
   “模型自我策略优化验证”PPO/SAC实验，可视化好看、容易讲。
   汇报时包装成：“我们已在验证模型自主学习框架，为下一阶段业务任务做RL技术预热。”

>  **成果1：完成V3技术验证**
>
> 成功构建了一个“高保真”模拟环境（V3），它100%复刻了我们`[Module, Anchor, Target]`的`[5, 10, 10]`高维动作空间和“路径约束”。模拟RL训练发现**最佳模型(在第40万步)能达到92%成功率**,其中87%是首次尝试成功!
>
> **结论：** V3实验证明，PPO算法**可以**被训练来100%遵守我们的“业务逻辑”，并精准输出结构化的动作。（*这就是您V3实验的唯一价值！*）

1. **战略落地延伸 → 借方向②**
   可附一句报告中提到：

   > “我也在调研开源RL在NLP任务上的工程实践（例如sql-rl-gen这个项目）
   >
   >  **成果2：跑通并验证了PPO可以很好的训练单回合制的SQL生成任务**
   >
   > 基于IBM开源的sql-rl-gen项目和主流的Spader数据库查询数据集，实现了在现有小模型的基础上，再用“执行结果是否正确”来做强化学习微调
   >
   > ## 
   >
   > - 在简单问题上，模型已经可以稳定给出与标准 SQL 完全一致的结果；
   > - 在稍复杂（排序、多列）的场景下，模型也能正确生成；
   > - 当遇到语义模糊或字段定义细节问题时，模型会出现“似是而非”的错误；
   > - 强化学习通过“执行结果 + 奖励”的机制，自动放大奖励高的写法，压制错误写法，持续提高模型对业务查询的可靠性。

2. **延展想象展示 → 来自方向③**
   展示 Text-to-MQL Router 的结构图，配上“下一阶段RL应用构想图”
   不需实现，只需画逻辑图 + 写小部分伪代码（展现你已在准备，准备了完整的框架图，后续只需要构造合适的数据集进行实验）





---





---



## 📚 开源项目推荐（方向2的参考）

### **选项A：Text-to-SQL + RL（最相关）**

```
GitHub搜索关键词：
- "reinforcement learning text-to-sql"
- "RL query generation"
- "semantic routing RL"
```

推荐项目：

- **BIRD-SQL** 或 **Spider** 数据集的RL方法实现
- **SeqZero**（Seq2Seq + RL优化）

### **选项B：工具调用Agent（逻辑相似）**

```
关键词：
- "LLM tool selection RL"  
- "ReAct Agent reinforcement learning"
```

推荐：

- **Langchain-RL**（虽然简单，但逻辑类似）
- **Toolformer**相关实现

### **选项C：意图识别 + RL（技术层面）**

```
关键词：
- "intent classification reinforcement learning"
- "dialogue policy learning"
```



## 根本问题分析

我意识到这个任务的**设计存在根本缺陷**:

- **观测空间(64维)远大于动作空间(3维)的信息量**
- **固定线性编码器**从3维映射到64维,然后加噪声,agent需要反向学习这个映射
- **这本质上是一个过度参数化的记忆任务,而非强化学习擅长的决策任务**

### 关键发现

1. **最佳模型(40万步)达到92%成功率**,其中87%是首次尝试成功!
2. **简化观测空间是关键** - 从64维神经网络编码改为25维one-hot特征
3. **训练步数存在最优点** - 40万步最佳,50万步反而过拟合

### 🔑 **关键经验**

1. **环境设计比超参数更重要** - 合理的观测空间设计胜过大量调参
2. **one-hot特征 > 神经网络编码** - 对于离散动作空间的映射任务
3. **early stopping很重要** - EvalCallback自动保存最佳模型,避免过拟合
4. **噪声水平要匹配任务** - 0.03对于25维one-hot特征是合适的平衡点









### 📋 下一步建议

**阶段1: 验证问题严重性**

- 检查4个不匹配样例的MongoDB返回结果
- 分析路径错误是否导致:
  - 返回了错误数据(如S3-02返回合同本身而非变更记录)
  - 返回空结果(如S3-03、S3-08)

**阶段2: Router训练目标明确**

- 重点训练Router在S3切片识别"变更"语义→`contract_change_log`
- 教会Router区分Type A(直接起点)和Type B(需跳转)两种模式
- 期望Router将Via准确率从80%提升到95%+

**阶段3: 对比实验设计**

- 用相同20个样例测试Router + Node4
- 对比CollectionPath匹配率: Baseline 80% vs Router ?%
- 证明Router是否能修正Baseline的系统性路径错误







A组的全局目标是打通当前问题分支的60分（当前的记忆模块），以及扩充处理的决策树分支的问题类型

A组当前需要：设计出来的RL方案必须是可以集成到当前Agent框架的五个节点之一的（满足可用性）——即当前RL的研究方案必须围绕Agent五个节点去增量升级

A组当前痛点：mongodb节点生成不准（需要哪些升级？RL？对抗训练？转DSL？冗余LLM监管...）；prompt太多了（需要训练内化？或者其他集成形式？要语义层？）

A组决策树还没开展、上下文优化被刘总暂时否了

A组可用的参考数据只有laminar中的哪些记录



L组的需求是调研收集哪些模块是可以用RL去提升的（目前两个方向）

基于Agent的工作流，存在以下两个主要的、有价值的强化学习应用点：

1. **`A. L1/L2 - 对话策略优化 (Dialogue Policy Optimization)`**: 优化Agent在“输入处理与对话引导”阶段的决策能力。这是一个经典的**对话管理 (Dialogue Management)** 问题，核心在于学习一个最优策略，以最有效的方式引导对话，从而收集到足够且准确的信息。
2. **`B. L3 - MQL查询生成优化 (MQL Generation Optimization)`**: 优化Agent在“查询生成”阶段的能力。这可以被看作一个**结构化语言生成 (Structured Language Generation)** 问题，核心在于学习一个策略，将自然语言意图准确地映射为可执行的、正确的MQL查询语句。

- 引入RL优化MQL生成的查询路由决策（当前“NL→MongoDB 生成器”由大量Prompt指导生成JSON查询，需要RL来辅助LLM提升查询业务口径与查询范式的路由选择决策能力）
  - **与现有架构无缝集成**: 该RL Agent可以作为一个新的、轻量级的决策节点，插入到现有流程中。它接收“问题归一化器”的输出，其决策结果（选择的口径或范式）可以直接作为“NL→MongoDB 生成器”Prompt的一部分，指导其生成最终的MQL，完美嵌入当前架构。

问法-当前整理的针对输入的决策树推理RL是否有效
A组认为可能合适推荐的RL替换点有哪些（是否有完全的框架流程图）？-我再基于这些突破点去调研可行性

- [ ] 从流程环节上有哪些RL方向
- [ ] 从问题类型上有哪些RL方向（简单查询问题、复杂查询问题、为什么的推理问题）

在输入处理阶段，Agent的核心职责是通过多轮对话，引导意图模糊的用户，在一个预定义的“项目问题决策树”上进行推理，最终明确其真实需求。

RL黄金突破口：

1. **节点4**：结构化生成与候选重排（直接提升MQL正确率）
2. **节点5代行L1/L2**：澄清/引导策略（减少slot-error，缩短轮次）
3. **节点3**：RL优化实体识别与问题拆解（已有部分实体识别经验）

MQL查询生成RL优化的难点在于RL环境不好获取，A组是否可以提供合适的RL训练反馈的环境（当前工作流能改不？）

RL优化实体识别与问题拆解（已有部分实体识别经验），虽然不局限于工作流，但是难点在于之前旧的实体体系又得重新与A组同步




---










---



---

**奖励：查到正确答案+10，查错-1，每步-0.01（鼓励最短路径）** 非稀疏奖励

先做一个极简、可快速上线验证价值的 Router。仅围绕三大歧义 A+B+C（主体 Anchor、口径 Module、路径 JoinPath）做单步路由决策；输入来自节点3归一化问题+对话上下文槽位+固定的关系图；输出为精简的结构化JSON（Format 2），注入节点4 Prompt 的位置B（输出格式之后、总则之前）；节点4保留采纳/否决/纠错权；训练流程分三步（小样本SFT → 真实执行回传的RL微调）。

---

**RL策略**：

- **State**：“归一化问题 + schema_text + project_id（可选）”

  - 示例
    {
    "norm_q": "项目I-23112109的物料细化是否已完成？",
    "schema_text": "<节点4 user prompt中的表结构+关系+模块规则全文>",
    "project_id": "I-23112109"
    }

- **Action**：Action 只输出“Module + JoinPath”两部分，其余细节（数组策略/字段裁剪/排序）由节点4“总则”负责

  - Module：告诉节点4是否触发“业务口径模块”（节点4有专门段落）

    - 口径与入口集合关系（给定的四类 + 通用）
      - unshipped：未发货/发货比例/是否发完/还剩多少 → 典型 start=contract_bom, target=none
      - fees：服务费/采购费/交通费/检测与认证费 → 典型 start=contract_bom, target=none
      - systems：监控系统/软件系统/软件模块/驱动 → 典型 start=contract_bom, target=none
      - material_refinement：物料细化是否完成 → 典型 start=contract_bom, target=none
      - generic：其余非特殊口径 → start/target 由语义决定（变更/发货申请/发货执行等）

  - JoinPath：告诉节点4“从哪张表起步 + 按范式A怎么走到目标表”，直接对应节点4的“跨集合与多跳”段落与collection起点选择

  - ```python
    Action = {
        "Module": str,           # 业务口径
        "CollectionPath": {
            "anchor": str,       # 入口集合（节点4的collection字段）
            "target": str,       # 目标集合（最终要查的数据所在表）
            "via": List[str]     # 关联路径（为空表示单表查询）
        }
    }
    ```

    

- 注入采用结构化JSON（Format 2），插在位置B

- 奖励只看 Compass 执行成败 + 路由一致性，先不引入更多复杂信号

- Reward

  ```
  reward = {
      +10: MQL执行成功 且 返回正确结果
      +3:  选择了索引字段（高选择性）
      +2:  路径简洁（无冗余$lookup）
      -5:  执行失败
      -2:  路径冗余（多余关联）
  }
  ```

**关键价值**：在歧义场景选择**信息最丰富且唯一可达**的路径。

```markdown
用户输入 → 节点1(校验) → 节点2(消解) → 节点3(归一化)
                                              ↓
                                    🎯 Semantic Router (RL核心)
                                              ↓
                        【A.主体歧义】合同的项目经理 → 起点应查project还是contract?
                        【B.口径歧义】物料细化完成 → 用哪个业务逻辑判断?
                        【C.路径歧义】项目→合同→变更 → 单跳还是多跳?
                                              ↓
                            生成结构化指导JSON (Format 2)
                                              ↓
                            注入节点4 Prompt (位置B: 总则之前)
                                              ↓
                                    节点4(MQL生成) → MQL
                                           ↓
                                         [MongoDB真实执行] → 结果
                                           ↓
                                         [AI评估 + 人工抽检] → 奖励信号
                                           ↓
                                         更新Router模型
```

合法典型

- 变更：start=contract, via=[], target=contract_change_log
- 发货申请：start=contract, target=delivery_request
- 发货执行：start=contract, target=delivery_order
- 项目→合同：start=project, target=contract
- 四类模块：start=contract_bom, target=none

阶段2 监督学习（SFT）

- 输入：{norm_q, schema_text, project_id?}
- 输出：{module, join_path.start, join_path.via, join_path.target}
- 小模型即可（如6B以下），训练为分类+序列结构填充（via可用序列多标签或受限解码）

训练脚本
• SFT：多头分类/受限解码
• RL：基于奖励表的Bandit/REINFORCE loop





---

【Router Guidance（可选 - 若提供请严格遵守）】
- Router_output 格式 (JSON):
  {
    "Module": "<module name>",
    "CollectionPath": {
      "anchor": "<anchor_collection>",
      "target": "<target_collection>",
      "via": [ "<via_col1>", ... ]
    }
  }
- 注入说明:
  1. 若提供 Router Guidance，请将下列要点视为“必须遵守”的约束，先于你的生成逻辑应用：
     - 首先以 Router 指定的 `CollectionPath.anchor` 作为 pipeline 的 entry collection（collection 字段）。
     - 若 `CollectionPath.target != CollectionPath.anchor`，优先考虑使用 `$lookup` + `$unwind`（或范式A允许的子管道）来实现跳转，除非问题语义明确要求“count/size”类不展开统计。 
     - 当 Router 建议了 `via`，尽量保持通过 `via` 指定的中间集合（例如 `via=["contract"]`）实现单跳。
  2. 若 Router 与问题语义冲突，请返回 status="intent-error" 并在 reason 指明冲突。
  3. 针对数组处理优先使用“数组最小原则”：若意图是“List（元素级展开）”或需要排序/TopN，使用 `$unwind`；若意图是“Count/Exists/ElemCount”且没有要求元素级输出，请使用 `$size/$filter` 不展开。
  4. Router Guidance 仅影响“选择哪个表/是否跳转”的决策；字段名/过滤条件/高选择性 $match 等仍由 Node4 根据会话决定，但请优先保证 collection/start/target/via 的一致性。
- 示例（注入占位）:
  <ROUTER_GUIDANCE>
  {"Module":"unshipped", "CollectionPath":{"anchor":"contract_bom","target":"contract_bom","via":[]}}
  </ROUTER_GUIDANCE>





---

## 📊 CollectionPath一致性评估结果分析

### 🎯 核心发现

**1. 真实成功率揭示:**

- **表面成功率**: 85% (17/20样例返回了结果)
- **CollectionPath匹配率**: 80% (16/20样例路径完全正确)
- **关键差异**: 有1个样例虽然返回了结果,但CollectionPath不正确

**2. 不匹配模式分析:**

| 验证维度               | 说明                                      |
| ---------------------- | ----------------------------------------- |
| **Anchor提取**         | 直接使用collection,符合定义               |
| **Target推断(无跳转)** | 无$unwind→返回anchor,符合S1样例           |
| **Target推断(有跳转)** | $lookup+$unwind→返回from,符合S3样例       |
| **Via推断(无跳转)**    | target=anchor→via=[],符合S1标注           |
| **Via推断(单跳)**      | target≠anchor→via=[anchor],符合S3标注     |
| **边界情况**           | S3-02的$lookup无$unwind被正确识别为未跳转 |
| **置信度评分**         | 明确模式0.95-1.0,模糊模式0.7-0.8          |

所有4个CollectionPath不匹配样例都集中在**S3切片(合同变更)**,暴露了Baseline的系统性弱点:

| 样例  | 问题                    | 期望路径（RL的模拟）         | 实际路径                                | 得分 | 问题分析           |
| ----- | ----------------------- | ---------------------------- | --------------------------------------- | ---- | ------------------ |
| S3-02 | 合同变更有多少条        | contract→contract_change_log | contract→contract (无跳转)              | 0.30 | **未跳转到变更表** |
| S3-03 | 合同的最高变更金额      | contract→contract_change_log | contract_change_log (直接起点)          | 0.50 | **起点选择错误**   |
| S3-08 | 合同25-IX-040002S的变更 | contract_change_log (直接)   | contract→contract_change_log (多余跳转) | 0.50 | **过度关联**       |
| S3-10 | 合同变更次数            | contract→contract_change_log | contract→contract (无跳转)              | 0.30 | **未跳转到变更表** |

### 🔍 深层洞察

**S3切片的两类路径混淆:**——可以专注S3板块问题进行数据扩充

1. **Type A - 起点明确(已知变更ID)**: 应直接从`contract_change_log`开始
   - S3-08期望: `contract_change_log` (直接)
   - Baseline错误: `contract → contract_change_log` (多余$lookup)
2. **Type B - 起点模糊(只知合同)**: 应从`contract`跳转到`contract_change_log`
   - S3-02/S3-10期望: `contract → contract_change_log`
   - Baseline错误: `contract → contract` (未跳转)

**Baseline的策略缺陷:**

- 无法理解"变更"需要关联`contract_change_log`表
- 对"合同C-888的变更"这种明确指定变更ID的场景,仍然从contract起始做多余$lookup

### 📈 组件准确率对比

**Via准确率最低**说明:Baseline在判断"是否需要$lookup"和"lookup哪些中间表"时最容易失误。

### 🎓 评估设计验证

**CollectionPath一致性评估**成功揭示了:

1. ✅ **表面成功率具有欺骗性**: 85%的"有结果"不等于85%的"结果正确"
2. ✅ **Router的核心价值**: S3切片中40%的样例路径选择错误,Router可以通过准确预测CollectionPath来修正
3. ✅ **评估维度细化**: 通过anchor/target/via三个维度,定位到"Via准确率"是最大瓶颈

### 📋 下一步建议

**阶段1: 验证问题严重性**

- 检查4个不匹配样例的MongoDB返回结果
- 分析路径错误是否导致:
  - 返回了错误数据(如S3-02返回合同本身而非变更记录)
  - 返回空结果(如S3-03、S3-08)

**阶段2: Router训练目标明确**

- 重点训练Router在S3切片识别"变更"语义→`contract_change_log`
- 教会Router区分Type A(直接起点)和Type B(需跳转)两种模式
- 期望Router将Via准确率从80%提升到95%+

**阶段3: 对比实验设计**

- 用相同20个样例测试Router + Node4
- 对比CollectionPath匹配率: Baseline 80% vs Router ?%
- 证明Router是否能修正Baseline的系统性路径错误

---





---

**Router Oracle模式显著优于Baseline:**

| 指标                     | Baseline      | Router            | 提升     |
| ------------------------ | ------------- | ----------------- | -------- |
| **pass@1**               | 85.0% (17/20) | **95.0% (19/20)** | **+10%** |
| **CollectionPath一致率** | 80.0% (16/20) | **85.0% (17/20)** | **+5%**  |
| **empty_result错误**     | 3个           | **1个**           | **-67%** |

------

### ✨ 关键改进案例分析

#### **案例1: S3-03 - "项目I-24094056的合同的变更金额是多少?"**

- **Baseline**: ❌ 失败
  - 错误选择了`contract_change_log`表作为起点
  - MongoDB查询返回0条记录 (empty_result)
  - 原因: 直接从change_log查询,缺少$lookup关联
- **Router**: ✅ 成功
  - Router指导使用`anchor=contract, target=contract_change_log`
  - 正确从`contract`表出发,$lookup到`contract_change_log`
  - 返回1条聚合结果(变更金额总和)

#### **案例2: S3-08 - "项目I-24094056的合同C-888的变更有哪些?"**

- **Baseline**: ❌ 失败
  - 错误选择了`contract`表作为起点
  - 缺少对change_log的关联查询,返回0条记录
- **Router**: ✅ 成功
  - Router指导使用`anchor=contract_change_log, target=contract_change_log`
  - 直接从`contract_change_log`表查询,过滤contract_id
  - 返回6条变更记录

------

### 🔍 根因分析

**两个失败案例暴露的Baseline核心问题:**

1. **表选择错误** - S3场景(合同变更)涉及2个表关联:
   - 查询**变更金额**(聚合场景) → 应从`contract`出发
   - 查询**变更记录**(列表场景) → 应从`contract_change_log`出发
   - Baseline **无法准确判断起始表**,随机性强
2. **$lookup方向错误** - Baseline经常错误选择:
   - 应该`contract → lookup(change_log)`却反向查询
   - 导致无法匹配到正确的外键关联

**Router Oracle如何解决:**

- 通过`anchor`参数**强制指定起始表**,消除歧义
- 通过`target`参数**明确跳转目标**,确保$lookup方向正确
- Node4 Prompt的【Router指导】板块强制执行约束规则

#### ✅ 结论

1. **Router必要性已验证**: 10%的pass@1提升证明Router能显著改善表选择准确性
2. **Oracle模式有效**: expected_router_output能准确指导Node4生成正确MQL
3. **CollectionPath一致性提升**: 从80%→85%,证明Router指导确实被Node4采纳

#### 🚀 下一步建议

**【强烈推荐】进入Router SFT训练阶段:**

1. **准备训练数据** (预计需要500-1000样本):
   - 输入: `{user_query, node3_normalized_query, module_intent}`
   - 标签: `{Module, CollectionPath{anchor, target, via}}`
   - 数据来源:
     - 现有20个测试样例 + 标注
     - 扩充S1(unshipped)、S3(contract_change)场景
     - 补充S2(generic)、费用、系统等其他Module场景
2. **Router模型选型**:
   - 方案A: Qwen2.5-7B-Instruct LoRA微调 (推荐)
   - 方案B: Qwen2.5-14B-Instruct LoRA微调 (更高精度)
3. **训练目标**:
   - 目标pass@1 > 90% (在更大测试集)
   - 目标CollectionPath一致率 > 90%
4. **验证流程**:
   - 训练Router → 替换模拟器 → 重新运行A/B对比
   - 对比真实Router vs Oracle性能差距





---

---

现在我打算进行一个基于强化学习的“智能查询路由”实验方案
现在我需要你阅读并完全理解我的当前设计的方案文件《RL_MQL_Semantic_Router》，然后给出简要的总结，让我知道你已经完全理解了和对齐我的需求
现在请你确认你理解了我的需要，确认后，如果你还需要其他进一步的补充信息也请提出来，或者说如果你有其他更好的思考和需要了解的问题，也可以提出来我们来一起讨论，然后帮我完成一个高质量的任务成果

---



---

## 查询路径分析

### 问题：项目I-24094056的合同的变更金额是多少？

### 最直接的查询路径：
1. 直接查询`contract_change_log`表，因为变更金额(`amount`、`final_amount`等字段)直接存储在这个表中
2. 但需要先确定与该项目相关的合同ID，因为`contract_change_log`是通过`contract_id`与合同关联的

### 具体查询流程：
1. **第一步**：从`project`表中找到项目ID为"I-24094056"的项目
   - 获取该项目的`project_id`

2. **第二步**：使用`project_id`在`contract`表中查找相关合同
   - 获取与该项目关联的`contract_id`(可能有多个合同)

3. **第三步**：使用`contract_id`在`contract_change_log`表中查找变更记录
   - 获取相关的变更金额信息(`amount`、`final_amount`等)

### 为什么需要这个路径？
- `contract_change_log`表只直接关联到`contract_id`，而不是`project_id`
- 所以需要通过`project`→`contract`→`contract_change_log`的路径来获取信息

## 关于查询表的灵活性

您可以直接访问中间表或最终表，但有以下限制：

1. **当您有直接关联键时**：
   - 如果您知道`contract_id`，可以直接查询`contract_change_log`而不需要先查`project`表
   - 如果您知道`construction_contract_id`，可以直接查询`construction_daily`等表

2. **当您只知道起始信息时**：
   - 如果只知道项目信息，需要按照数据关系链路查询
   - 例如只知道项目ID，需要通过关系表找到对应的合同ID，再查询变更信息

3. **当您需要综合信息时**：
   - 可能需要连接多个表才能获取完整信息
   - 例如同时需要项目名称和合同变更金额，就需要连接`project`、`contract`和`contract_change_log`

## 总结

对于"项目I-24094056的合同的变更金额是多少？"这个问题，查询路径应该是：
```
project → contract → contract_change_log
```

这是因为：
1. 用户提供的是项目ID
2. 变更金额存储在`contract_change_log`表中
3. 该表通过`contract_id`与合同关联，而不是直接与项目关联

在实际应用中，查询路径的选择取决于：
- 您拥有的查询条件(如ID类型)
- 所需信息存储的位置
- 表之间的关联关系

您可以直接访问中间表或最终表，只要您有足够的关联条件。但在大多数情况下，从业务流程的角度理解表关系有助于构建更有效的查询。



---





---



你是一位精通自然语言理解（NLU）数据生成的专家，你的任务是为项目发货领域的智能问答系统创建高质量的训练数据。你需要基于给定的“种子问题”，生成大量风格多样、表达方式不同的问句变体。
以下是来自“项目发货核心问题决策树”的种子问题列表：
<种子问题列表>

1. xx项目的发货申请提交财务审批了么？

- 发货流程审批在哪个节点?
- 流程审批还剩哪些环节？
- 发货流程卡在什么地方？
- 发货流程审批的负责人是谁？是否有变更？

</种子问题列表>
针对每一个种子问题，你需要生成5 - 10个句式不同的变体，注重表达的丰富多样和自然性，以适合泛化未来可能的提问形式，使生成的问题尽可能像一般人类用户的提问，避免有AI味。这些变体需要覆盖以下几种提问风格：

- 直接式：像日常与人交流一样开门见山，直接提问，例如“P - 74456354发没发货？”
- 礼貌式：在提问中自然地包含“请问”、“你好”、“麻烦帮我查一下”等礼貌用语，比如“你好，请问CET保护什么时候能发货？”
- 口语化/省略式：模拟日常随意的对话场景，可能包含省略、倒装或非正式用语，例如“张三那批货发了不？”
- 专业术语式：使用项目发货领域内大家熟知的专业表述来提问，例如“H-20265108对应的项目是否已完成出库发运？”
- 同音错别字：使用同音错别字或者拼音来代替部分实体，比如“项目P-74456354写成同音的橡木P-74456354来提问”</selected_content>

在生成的扩充样本的时候，要记得将这些占位符替换为有差异变化的具体实体内容，以便后续进行程序化替换。例如项目号可以更换成不同的数字组合，设备名称、收件人、箱号、合同号等也使用不同的具体内容：

- `[项目ID]`: e.g., "P-74456354"
- `[设备名称]`: e.g., "研华工控机", "CET保护"
- `[收件人]`: e.g., "张三"
- `[箱号]`: e.g., "1#箱", "A03"
- `[合同号]`: e.g., "H-20231108"

为每一个生成的句子标注其对应的`意图(intent)`和`子域(subdomain)`。意图(intent)分为四类：

- `query`: 事实查询类，主要通过询问什么、谁、哪里、多少等疑问词，以获取具体的事实信息，如询问某个项目的具体状态、某个设备的名称等。
- `judge`: 是非判断类，通常使用是不是、有没有、能否等词汇来询问，目的是得到一个肯定或否定的回答，如判断某个项目是否已经发货等。
- `reason`: 原因追溯类，通过为什么、什么原因等疑问词来询问事件发生的原因。
- `unknown`: 当前无法识别的问题类型，通常是超出某个或多个领域之外的问题。

子域(subdomain)在本任务中统一为: `delivery`。
生成的变体必须保持种子问题的核心意图和关键实体类型不变。
请严格按照以下Markdown格式输出结果，使用`---`作为每个种子问题任务的分割线。

```markdown
### 种子问题: [这里是原始的种子问题]
- **核心意图**: [query/judge/reason]
- **子域**: delivery

#### 生成的变体:
1. [生成的句子1] `(意图: [intent], 子域: delivery)`
2. [生成的句子2] `(意图: [intent], 子域: delivery)`
...
15. [生成的句子15] `(意图: [intent], 子域: delivery)`
---
```

开始为种子问题生成训练数据

---



![image-20250701090731516](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250701090731516.png)

标注过的语料实体提取效果还行

![image-20250701090903368](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250701090903368.png)

暂时还没标注的实体完全无法泛化识别

![image-20250702113749353](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250702113749353.png)

实体识别的结果还行，但是意图这部分有问题（大多识别成了unknown，似乎欠拟合？）暂时还不清楚原因，另外relation这块似乎过拟合了（几乎所有实体两两之间都强制识别了关系）

![image-20250703113958835](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250703113958835.png)

调整了分类器权重weight_grammar = 1  # 从0.1增加到1，weight_relation = 0.5  # 从1.0减少到0.5。但是依旧是grammar识别不准，relation识别太多过拟合，目前只能从模型结构的训练和数据清洗去调整优化了

![image-20250703171433025](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250703171433025.png)

尝试除了正向关系的标签还给relation增加了none标签，明确告诉Roberta模型"什么是没有关系的实体对"，提供了正负样本的按比例分配的学习方法，缓解"所有实体对都有正向关系"的错误学习模式

但是初步实验的分阶段训练并没有对grammar的预测准确性起多大效果，依旧准确率不高

![image-20250704092143316](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250704092143316.png)

实体span还是过拟合了，在新构造的测试问题上span没有泛化性

![image-20250704173629901](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250704173629901.png)

尝试在不改变Roberta的多分类器输出层的情况下，调整分类器训练权重，只保留grammar的训练梯度，使用的concat的700+的标注和学习率等配置。但是效果不理想，其他三个类无法识别是在意料之中，但是grammar训练了依旧无法识别就说明**多分类器对这个难度的问题不适用**！！！



![image-20250711144802114](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250711144802114.png)![image-20250711145309892](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250711145309892.png)

解决了grammar多分类的编码异常的问题，虽然loss变化上有一点点过拟合，但是在我们构造的小领域测试数据上泛化目前是没有什么问题的。但是把修复的grammar分类器再加回原始的多分类器Roberta上又影响了其他分类的效果了，还需要进一步修改训练参数与结构进行调试



![image-20250711145605151](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250711145605151.png)

通过调整学习率和训练的分类器权重可以优化构建的测试集效果，目前小范围测试发现grammar、entity、relation的效果还行，其中relation与grammar有一点点过拟合。另外subdomains的训练会问题比较大，因为数据集比例极度失衡，目前标注的数据基本都是delivery的



![image-20250714104048355](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250714104048355.png)

多任务分类目前grammar和entity问题不大，主要是subdomains和relation预测不准，他们的数据都比较差、另外relation分类预测任务相较另外几个任务难一点

![image-20250715144253202](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250715144253202.png)

现在数据质量对预测的影响比重比较大，目前构造的测试数据中发现的可能与数据质量相关引起的问题有：问题冗余时会有重复的实体提取、标注数据样本量少/前后标注不一致导致实体边界提取重叠

![image-20250717134837872](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250717134837872.png)

通过随机搜索算法实验找到一组模型训练的局部最优的超参数组合（训练配置）



![image-20250718104020972](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250718104020972.png)

目前在多任务RoBERTa模型的训练过程中遇到的真正的系统性问题，训练损失极低，但某些任务的验证F1分数却很差。训练日志分析请求**： ``` 警告：损失低得可疑，可能存在数值问题 损失组成部分：语法=3.16e-08，子领域=0.0，位置=5.05e-07，实体=2.53e-07，关系=1.49e-07  验证指标：{'eval_loss': 1.004, 'eval_grammar_f1': 0.973, 'eval_subdomain_acc': 1.0, 'eval_start_end_f1': 0.952, 'eval_span_f1': 0.004, 'eval_relation_f1': 0.0} `**

**需要调查的具体问题**： 

- **损失计算问题**：各个任务的损失低得可疑（接近零），这表明可能存在**数值不稳定或损失计算不合理问题**
- **特定任务的性能不佳**：    - `eval_span_f1`：0.004（极其糟糕）    - `eval_relation_f1`：0.0（完全失败） 
- **训练与验证的差异**：训练损失极低，但验证性能不佳，这表明可能存在**过拟合或评估指标问题**。 
  - **需要进行的分析**： 检查每个任务（语法、子领域、位置、实体、关系）的损失计算方法，以找出数学错误或数值不稳定的情况。 
  - 回顾跨度检测和关系分类任务的F1分数计算逻辑。 检查**数据不平衡**、标签编码问题或指标计算错误。

**现状：**

- grammar、subdomains分类任务基本能够很好的进行测试预测了（因为解决了grammar、subdomains分类任务的训练错误问题）
- **entity与relation经过实验调试在训练流程上是没有明显问题的（已反复检验了训练metric和label方法正确性）；**
  **但是entity与relation任务的难度高，训练loss与验证F1计算方法可能需要实验进行因地制宜的调整；**
  **特别是避免数据不平衡进一步提高了训练难度**





![image-20250718111916113](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250718111916113.png)

新加了多任务分类器的权重超参数优化组合2（训练配置+任务权重）的搜索结果



![image-20250718112615757](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250718112615757.png)

![image-20250718112638605](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250718112638605.png)

更新优化后的超参数组合（训练配置+任务权重）又把测试预测的效果提高了一点（如上图）之前的优化组合1中badcase的短句样本容易识别掉部分实体，但新优化组合2现在可以识别的到

![image-20250725161346108](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250725161346108.png)

针对badcase的问题是可以通过不断补充垂域相关的泛化数据进行有效改善的





![image-20250728163449886](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250728163449886.png)

![image-20250728163617491](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250728163617491.png)

![image-20250724143215462](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250724143215462.png)![image-20250724143225581](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250724143225581.png)

冻结参数进行分离训练的效果目前实验看效果不好（暂时未检验代码）



![image-20250725084650129](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250725084650129.png)

<img src="C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250725091521792.png" alt="image-20250725091521792" style="zoom: 50%;" />

![image-20250725091934238](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250725091934238.png)![image-20250725091945346](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250725091945346.png)

新的基于evalLoss目标的随机搜索的超参数组合，eval的loss变化很好，但是测试很一般



![image-20250725093648325](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250725093648325.png)

---

### 关键发现和修改要点

#### Grammar和Subdomain任务的依赖性分析

- **低权重设置**：表明这两个任务对整体性能贡献较小
- **简单分类**：都基于CLS token，计算复杂度低
- **独立性强**：与Entity/Relation任务没有强耦合

#### Entity和Relation任务的核心地位

- **高权重设置**：表明这两个任务是模型的核心
- **复杂架构**：Entity需要边界检测+类型分类，Relation需要实体对交互
- **相互依赖**：Relation任务依赖于Entity任务的输出

#### 移除Grammar和Subdomain的影响评估

1. **简化模型架构**：减少分类器数量，降低参数量
2. **专注核心任务**：更多计算资源投入到Entity和Relation
3. **减少标签噪声**：避免简单任务对复杂任务的干扰
4. **提升训练效率**：减少损失计算和反向传播复杂度

---

![image-20250728153300696](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250728153300696.png)

![image-20250728153111181](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250728153111181.png)

实现跑通了单独针对entity/relation的训练网络结构，相较于之前冻结四任务部分输出神经元权重的效果模糊的实验，单独抽离出两任务的Roberta模型是确定的可以提升模型在抽取entity上的稳定性，减少entity_positions之间交叉的错误问题出现，改进效果见上述两图对比

![image-20250728164902806](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250728164902806.png)

'eval_start_end_f1': 0.93比较高，说明单独训练的神经网络结构对entity的位置抽取比较准

`start_end_f1`表现良好（0.92）是因为：

1. **简单的二分类任务**：只需要预测token是否为实体的开始/结束位置
2. **直接的标签格式**：使用简单的0/1标签，没有复杂的索引转换
3. **正确的评估逻辑**：直接比较二进制预测结果

![image-20250730170717213](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250730170717213.png)

![image-20250730161813325](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250730161813325.png)

![image-20250730173728853](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250730173728853.png)

导致`span_f1`和`relation_f1`分数极低的根本原因：

- **标签格式不匹配问题**。在`compute_metrics`函数中当`span_labels`是列表格式时，代码创建了全零的虚拟标签（`torch.zeros_like(pred_span_ids)`），这意味着所有真实标签都被设置为0，而模型的预测值通常不是0，导致F1分数极低。
-  **数据预处理与评估不一致**。在预处理阶段`span_entity_labels`被保存为列表格式，但在`compute_metrics`中，代码期望的是张量格式来提取第三列（标签ID）
- **标签ID映射问题**。在`load_labels_fromfile`函数中标签ID从1开始，但在前向传播中，又转换为0-based索引，这种来回转换容易造成混乱。

图1、图2、图3分别为修复后的评估指标训练验证的效果，均恢复正常范围，这也为后续随机搜索方法提供了良好的优化目标基础

![image-20250730191034412](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250730191034412.png)

修复后使用随机搜索的最佳超参数组合

![image-20250807101455331](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250807101455331.png)

排查解决了导致`span_f1`和`relation_f1`分数极低的根本原因，数据预处理与评估不一致。在预处理阶段`span_entity_labels`被保存为列表格式，但在`compute_metrics`中，代码期望的是张量格式来提取第三列（标签ID），导致，代码始终创建了全零的虚拟标签。目前已经将预处理的列表与metric的张量统一分开处理。

![image-20250807101940906](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250807101940906.png)

结合两个双任务模型的意图识别的API接口更加精准且稳健，并且使用预测的时间依旧在20ms内

![image-20250807103939268](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250807103939268.png)

构建prompt+chosen/rejected样本，来强化训练模型对样本的预测偏好

![image-20250807105616760](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250807105616760.png)

生成的偏好对数据集构成案例

![image-20250807104151672](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250807104151672.png)

目前已初步完成基于已有数据集来生成偏好对的SOP——可以作为后续DPO训练的基础 + GRPO奖励模型训练的部分材料

下一步需要进行基线模型（双任务Roberta）评估，同时也是我们目前SFT工作的**评价与迭代**的关键步骤

![image-20250807110920679](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250807110920679.png)

![image-20250807111046773](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250807111046773.png)

现状

已解决问题：

已有双任务意图识别模型（Roberta 和 API）+ DPO的偏好数据（JSON 和 生成的SOP）

后续构想：

​	**五个步骤**:

1. **生成偏好数据集**: 使用基线模型对原始数据集进行推理并生成偏好对——**已完成**
2. **评估基线模型**: 在测试集上评估原始基线模型性能——**待评估**
3. **DPO强化学习训练**: 使用LoRA技术进行DPO微调——待实验
4. **评估训练后模型**: 在相同测试集上评估DPO微调后模型性能
5. **对比分析**: 对比实体F1、关系F1等关键指标的提升情况

当前面临问题：

任务评估的策略待调研**（自己明确目标评估侧重点与边界 + 通过GPT探究具体实施方法）**；评估的范围与数据待整理**（暂时不需扩展数据和范围）**

![image-20250811143848912](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250811143848912.png)

![image-20250811143813060](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250811143813060.png)

### 模型表现分析——当前实体与关系的F1值计算有点苛刻了！（无法容忍小样本的标注存在波动情况）
1. **Subdomain 分类**：表现完美（100% 准确率）
2. **Grammar 分类**：表现良好（84.85% F1）
3. **实体识别**：表现中等（68.93% F1）——有必要区分span与type两个的评估指标，因为这两个在标注的时候很容易出现标注不统一的情况，特别是type的label区分不清晰很容易出现前后歧义的标注
4. **关系抽取**：表现较差（24.37% F1）——relation的标注就更多歧义了，而且还算上entity的累加错误

### 主要问题
1. **关系类型混淆**：模型经常将 `prj:has_role` 预测为 `prj:has_prop`，也有些是标注时候前后label有差异的问题
2. **复杂句子处理**：包含多个实体的句子关系抽取准确率低
3. **语法分类错误**：部分 `reason` 和 `judge` 类型被误分类

### 改进建议
1. **关系抽取**：需要更多 `prj:has_role` 类型的训练数据
2. **复杂句子**：增加多实体、多关系的训练样本
3. **语法分类**：加强 `reason` 和 `judge` 类型的特征学习

针对类似 "value": "柜子"这种可能存在标注时候（box/panel）出现前后歧义的label问题——是否需要近似语义匹配？或者简化成单一表示panel？



![image-20250815090457612](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250815090457612.png)

原始模型对于新标签框架的泛化能力也还行，这部分再增加相应的数据训练后应该会更准



- **评估的f1——评估优化——**数据用之前手动test的数据，后面也加入了一点，共33个标注样本；f1值计算使用的是一整句实体的字符级匹配才算TP，评估发现 entity F1 ≈0.7，grammar/subdomains F1≈1；这部分entity受标注数据质量的影响比较大。——待优化数据和标注
- **api推广应用——推进重点——**重构了的entity框架，现18个一级分类，之前6个，新框架就很好的融入的略哥那边的表结构实体数据，SFT新训练的模型也能回答部分表结构中实体构造的问题。——待添加新实体语料和标注
- **dpo训练的——技术实验——**这部分实现遇到难点，如果使用transformer的TRL库的DPO框架是因果模型的文本logit格式，我们这个多任务输出就需要改造输出的模型头





v3版本146个实体（包含Unknown）

v2版本186个实体标签（包含Unknown）

v1版本57个旧实体标签



**API推广应用**——A组——成功把新版Roberta的API嵌入到略哥那边的工作流中了，目前可同时返回Roberta和LLM的意图识别结果，方便后续数据和模型迭代

![image-20250820164053790](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250820164053790.png)

**API推广应用**——R组——调研后发现R组是先对用户问题进行三分类（**chat闲聊、download文档查看、query信息咨询**），再考虑回答，且会拼接约多条历史记录辅助识别意图。——**先仅尝试支持对query的训练**

- download：帮我找一下蓝云凭他v4.9发布记录、帮我找一下cetjava代码规范
- query：7320-G支持哪些通信协议
- chat：strawarry里面含有几个r

R组的槽位比较复杂不固定。它的领域范围非常大，且用户输入模糊，难以精准提取。R组通过决策树设定节点，引导用户补充问题信息（非问题重写），**大模型根据信息做决策**，虽槽位识别非精准提取信息，但决策准确度较高。

RAG 检索痛点问题：

- **检索误差**：RAG 本身是概率性检索，相近型号和版本号易导致检索太多，无法控制token，如 5100 和 5100G、不同版本号等情况。
- **权重识别需求**：检索时需识别关键词权重，避免因分词权重相同导致相似度误判，但实现难度较大。



![image-20250822141742358](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250822141742358.png)



评估结果可以清晰地指出各自的瓶颈，为下一步的优化（如优化RAG的Embedding模型、优化SQL生成的Prompt工程）提供明确方向。





![image-20250828134058761](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250828134058761.png)

添加功能实现了非极大值抑制(NMS)机制，用于解决基于位置索引预测时候出现的实体边界重叠的问题，通过IOU检测以及子串包含检测来优化实体重叠消除逻辑（现在对于例子"华为项目的设备是否都已经发货了？"：实体A："华为项目的设备"（位置0-6）；与实体B："设备"（位置5-6）不会再重复出现）



### **当前 L1 实现的潜在漏洞讨论**

基于以上分析，我们可以清晰地看到当前实现方式与“评估守门员能力”这一目标之间的差距。漏洞主要有以下几点：

1. **评估对象错误：评估的是“输入内容”，而非“守门员的反应”**

   - **现状**：当前的逻辑是，拿到一个用户输入后，脚本自己用一套规则 (`_classify_user_input`) 去判断这个输入“应该”属于哪一类。然后就结束了，直接拿这个分类结果去统计。
   - **漏洞**：整个过程**完全没有去看 Agent 对这个输入到底做了什么反应**。我们只是在对输入文本本身进行分类，这更像是在做“数据分析”，而不是在“评估 Agent 的行为”。一个真正的“守门员”评估，必须包含两步：A. 识别输入（例如，这是一个领域外问题）；B. **检查 Agent 的输出（例如，Agent 是否真的拒绝回答了）**。当前实现完全缺失了步骤 B。

2. **数据源混淆：将对话过程中的所有 `span` 输入都视为“初始输入”**

   - **现状**：[`extract_user_queries()`](vscode-webview://1l0bua3v5ikm7ovf6qgn5ml9595vu0n4989vrfmvvdn84ajmbcnu/MQL_Agent_Eval/analyze_trace.py:47) 会遍历一个 `trace` 里的**所有 `span`** 去找用户输入。
   - **漏洞**：一个 `trace` 包含了一整轮对话。除了用户的第一句问题，还可能包含 Agent 追问后用户的补充回答，或者系统内部模块之间传递的中间结果。将这些中间产物都当作是需要“守门员”来判断的“初始输入”，在逻辑上是不准确的。L1 “守门员”应该只作用于**对话的最开端**，即用户的第一句输入。

3. **指标定义模糊且可能产生误导**

   - **现状**：例如，`acceptance_rate` 的定义是 `in_domain` 数量 / 总问题数。

   - 漏洞

     ：这个指标并不能反映“守门员”的真实表现。举个例子：假设我们有100个领域内问题，Agent 正确处理了90个，但粗暴拒绝了10个。同时，我们有50个领域外问题，Agent 本该全部拒绝，但它错误地接受并处理了20个。

     - 按现有逻辑，它可能会统计出 `in_domain` 的问题有100个，然后计算出一个看似很高的“接受率”。
     - 但实际上，这个“守门员”的表现很差：它错误地拒绝了10%的有效用户，又错误地放过了40%的无效请求。当前的指标完全无法反映出这种细微但关键的失败。

4. **分类规则过于脆弱**

   - **现状**：[`_classify_user_input()`](vscode-webview://1l0bua3v5ikm7ovf6qgn5ml9595vu0n4989vrfmvvdn84ajmbcnu/MQL_Agent_Eval/analyze_trace.py:206) 完全依赖于硬编码的关键词和正则表达式。
   - **漏洞**：这种方法非常脆弱。比如，一个领域外问题“帮我查查附近有什么好吃的”，因为它不包含任何业务关键词，会被正确分类为 `out_domain`。但如果用户说“帮我查查这个项目的负责人和附近有什么好吃的”，因为它包含了“项目”和“负责人”，就可能会被错误地分类为 `in_domain`。

我的初步想法是，我们需要将评估的核心从**“分析输入”**转向**“分析输入与输出的匹配关系”**。为了做到这一点，我们需要：

1. **精准定位**：在每个 `trace` 中，准确地找到代表“用户初始输入”的 `span` 和代表“Agent 最终（或首次）回复”的 `span`。
2. **黄金标准**：我们需要一套带有“黄金分类”的测试集，用来判断用户的初始输入应该被接受还是拒绝。
3. **行为判断**：我们需要定义一套规则，用来判断 Agent 的回复是属于“接受处理”还是“拒绝服务”。
4. **指标重构**：基于以上三点，重新计算真正能反映“守门员”能力的指标，如“领域内问题正确接受率”和“领域外问题正确拒绝率”。


1、精准定位：其他业务开发同事在设计这个外部Agent的时候其实是内部添加了很多的subAgent（如# Role: 语义有效性校验专家 # Role: 问题分组专家 等等subAgent），你可以通过一个trace中所有输入与输出来整理这个Agent的工作流具体什么样的，以及区分哪些是用户的输入哪些是subAgent的中间过程输出，这样我们就挑选并找到输入与对应需要匹配的输出进行评估（比如针对L1层评估，语义有效性校验专家、代词识别指代消解专家subAgent的输出应该是我们需要评估的重点，其他层的评估也应该匹配对应的若干个subAgent的输出）
2、黄金标准：目前我已经构建了一套带有“黄金分类”标签的测试集问题语料，但是我们需要考虑一下如何把构建的黄金标签加入到当前的L1评估体系中（因为当前提取的数据只有输入的问题和Agent的输出，还没有携带语料类型的黄金标签）。
3、行为判断：当前Agent框架针对语义有效性校验专家、代词识别指代消解专家subAgent的提示词约定的输出中是包含了结果是否成功的字段，可以提取使用。



好的，信息非常清晰。我们现在拥有了重构 L1 评估所需的所有拼图：
1.  **Trace 数据样本**: [`MQL_Agent_Eval/eval_data/mql_agent_evaluation_dataset_20250901_175016.json`](MQL_Agent_Eval/eval_data/mql_agent_evaluation_dataset_20250901_175016.json)
2.  **黄金标准说明**: 您将提供一个包含“问题语料”、“gold_category”和“问题类别”的 Excel 文件。
3.  **行为判断依据**: sub-Agent 的输出中包含可直接解析的成功/失败字段。

基于这些，我现在为您更新设计框架，并规划出具体的代码修改步骤。

---

### **L1 评估重构设计方案**

#### **1. 总体架构**

我们将引入两个新的核心组件，并重构现有的评估流程。

```mermaid
graph TD
    subgraph "数据准备阶段"
        A[Trace 数据集.json<br>来自 connect_test.py] --> C{评估主流程};
        B[黄金标准测试集.xlsx] --> C;
    end

    subgraph "评估主流程 (在 analyze_trace.py 中)"
        C -- 1. 加载数据 --> D[<b>新: GoldenStandardLoader</b><br>读取Excel, 存入内存字典];
        C -- 2. 遍历Trace --> E[<b>新: TraceParser</b><br>解析Trace, 重建工作流];
        D -- 匹配 --> E;
        E -- 输出结构化Trace对象 --> F[<b>重构: L1_Evaluator</b><br>比对黄金标准和Agent行为];
        F --> G[计算 TP, TN, FP, FN];
        G --> H[计算新L1指标];
        H --> I[输出到评估报告];
    end
```

#### **2. 关键组件设计**

**组件一：`GoldenStandardLoader` (黄金标准加载器)**

*   **职责**: 读取您提供的 Excel 文件，并将其转换为一个高效查询的数据结构。
*   **输入**: Excel 文件路径。
*   **输出**: 一个字典，`key` 为“问题语料”，`value` 为一个包含 `gold_category` 和 `问题类别` 的对象。
    ```python
    # 伪代码
    {
        "查询项目P-123的负责人是谁？": {
            "gold_category": "typical_problems",
            "question_type": "项目基本信息"
        },
        "查一下那个超期的项目。": {
            "gold_category": "ambiguous_problems",
            "question_type": "综合查询"
        }
    }
    ```
*   **实现位置**: 在 [`MQL_Agent_Eval/analyze_trace.py`](MQL_Agent_Eval/analyze_trace.py) 中新增一个类或一组函数。需要安装 `pandas` 和 `openpyxl` 来读取 Excel。

**组件二：`TraceParser` (Trace 解析器)**

*   **职责**: 将原始的、扁平化的 `spans` 列表，转换为一个结构化的、能够反映 Agent 内部工作流的对象。
*   **输入**: 一个 `trace` 中的 `spans` 列表。
*   **输出**: 一个 `ParsedTrace` 对象（可以用 `dataclass` 或字典实现）。
    
    ```python
    # 伪代码
    class ParsedTrace:
        trace_id: str
        initial_user_input: str
        sub_agent_calls: List[Dict]
        final_response: str
        golden_standard: Dict  # 从GoldenStandardLoader匹配到的黄金标签
    ```
*   **核心逻辑 (识别 sub-Agent)**: 我将分析 [`mql_agent_evaluation_dataset_20250901_175016.json`](MQL_Agent_Eval/eval_data/mql_agent_evaluation_dataset_20250901_175016.json) 中的 `span` 结构。我推测可以通过 `span` 的 `name` 字段（例如 `name: "# Role: 语义有效性校验专家"`）来识别不同的 sub-Agent。`input` 和 `output` 字段则记录了它们的输入输出。
*   **实现位置**: 在 [`MQL_Agent_Eval/analyze_trace.py`](MQL_Agent_Eval/analyze_trace.py) 中新增一个核心类。

**组件三：`L1_Evaluator` (L1 评估器 - 重构)**

*   **职责**: 实现新的 L1 评估逻辑，**评估行为而非内容**。
*   **输入**: 一个 `ParsedTrace` 对象列表（已经过解析和黄金标准匹配）。
*   **核心逻辑**:
    1.  遍历 `ParsedTrace` 对象。
    2.  检查 `golden_standard` 是否存在。如果不存在，则跳过该 `trace`（因为它不在我们的黄金测试集中）。
    3.  根据 `golden_standard['gold_category']` 判断预期行为（`Expected: Accept` 或 `Expected: Reject`）。例如，`typical_problems` 应该被接受，而 `invalid_problems` 和 `ambiguous_problems` 在 L1 层面应该被“拒绝”或“要求澄清”。
    4.  在 `sub_agent_calls` 中查找 L1 相关的 sub-Agent（如“语义有效性校验专家”）。
    5.  解析该 sub-Agent 的 `output` JSON，根据 `valid` 字段判断实际行为 (`Actual: Accepted` 或 `Actual: Rejected`)。
    6.  根据预期和实际行为，将结果归类到 TP, TN, FP, FN。
*   **实现位置**: 重构 [`MQL_Agent_Eval/analyze_trace.py`](MQL_Agent_Eval/analyze_trace.py) 中的 `evaluate_L1_input_layer` 方法。

#### **3. 代码修改计划**

现在，我将这个设计方案转化为具体的编码步骤。

**第一步：分析 `trace` 数据结构并实现 `TraceParser`**

这是所有工作的基础。我现在就来读取并分析您提供的 `trace` 数据样本，以确定识别 sub-Agent 的可靠方法。



| 问题语料                                                     | gold_category      | 问题类别   |
| ------------------------------------------------------------ | ------------------ | ---------- |
| 给我所有项目经理是王明，项目类型是PQ项目，客户是XX银行，发货状态是已到货，并且4S部门意见是风险高的项目。 | typical_problems   | 综合查询   |
| 列出所有发货到北京市海淀区XX大厦，收货人是张三，物料包含服务器和交换机，并且发货日期在本周内的所有发货箱号和物流信息。 | typical_problems   | 发货与物料 |
| 查找所有合同状态是执行中，业务类型是T1，风险等级是中风险，项目经理是一级项目经理，并且最终用户是YY集团的合同清单。 | typical_problems   | 合同信息   |
| 统计一下所有由交付团队负责，项目状态是进行中，客户分类是VIP客户，销售区域在华东区，并且最近一次变更金额大于10万的项目数量。 | typical_problems   | 统计与状态 |
| 查一下那个项目的进度。                                       | ambiguous_problems | 综合查询   |
| 那个项目的负责人是谁？                                       | ambiguous_problems | 人员与组织 |
| 李经理名下有多少个项目？                                     | ambiguous_problems | 综合查询   |









---

**时间匹配完全可行**：发现trace数据与API请求时间完美对应（注意UTC时差）

### 💡 **在线评估管线最终实施方案**

基于验证结果，我推荐以下实施策略：

#### **阶段1：触发执行**

- 批量发送API请求
- 记录精确时间戳和问题内容
- 获取API响应结果

#### **阶段2：等待同步**

- 等待2-3分钟让trace数据写入数据库
- 监控数据库最新记录时间

#### **阶段3：数据关联**

- 使用时间窗口（±3分钟）+ UTC转换查找trace
- 通过问题内容匹配验证关联
- 确保API响应与trace输出一致

#### **阶段4：评估分析**

- 收集完整的trace span数据
- 应用现有L1评估逻辑
- 生成评估报告





**实际laminar表结构：**

- span_id, name, span_type, start_time, end_time
- input_cost, output_cost, total_cost, model, session_id
- project_id, trace_id, provider, input_tokens, output_tokens, total_tokens
- user_id, path, input, output, input_lower, output_lower
- size_bytes, status

```sql
SELECT 
    trace_id,           -- 🔑 核心追踪ID
    session_id,         -- 会话标识  
    project_id,         -- 项目ID (I-23112109)
    start_time,         -- 开始时间 (UTC)
    end_time,           -- 结束时间
    path,               -- 操作路径 ('chat', 'chat.openai.chat')
    input,              -- 🎯 用户输入 (L1-L3关键)
    output,             -- 🎯 Agent输出 (L3-L4关键)
    name,               -- 操作名称
    model,              -- 使用模型
    input_tokens,       -- Token统计
    output_tokens,
    total_tokens,
    status              -- 执行状态
FROM spans
```



----



![image-20250905153142501](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250905153142501.png)

![image-20250905153826680](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250905153826680.png)

### L1层评估的现状

**数据：**用GPT按比例15:4:3:3（典型/歧义/无关/恶意问题）批量生成，实现了用AI表格批量分类标注然后人工抽检

**在线预测匹配：**A组原始接口只有最终结果；目前实现clickhouse的成功解析，匹配对应trace_id的subAgent评估对象，也就是目前有了y_true，也能批量预测并获取到y_pred

**批量评估：**目前评估指标的计算还比较简单就是混淆矩阵和f1，整个计算管线跑通了，但是计算设计的代码还有点bug（subAgent的计算优先级存在混淆）

### 后续的难点

数据：L2的黄金标准 的构造 和 自动标注 比较麻烦，A组这部分与之前的Roberta应该是两套不同体系的

流程：A组是LLM问题分解后 - 先cypher查询 - 再Cypher意图与槽位提取，这里似乎有两次意图的提取，而且不确定这个流程是不是固定下来的

### 接下来计划

优化L1评估管线的代码，解决评估计算的设计bug

分析A组的意图与槽位的评估数据该有的设计方法，并进行AI表格自动标注

![image-20250908143600791](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250908143600791.png)



工具选择成功率 - 检查complete的结果——判断是否能根据已知的Schema为这个问题生成一个合法的Cypher查询

cypher查询组件完整性判断 - 实体关键词、数据库语法检查

cypher查询结果的黄金标准获取比较困难

cypher查询语句的操作符生成的规律不好约束，不太好作为固定评估指标

{"question":"统计一下当前项目的相关人员总数","project_id":"I-23112109"}

{
  "complete": true,
  "cypher": "MATCH (p:project { _id: 'I-23112109' }) RETURN size(p.members) AS person_count",
  "error": ""
}

![image-20250912175327498](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250912175327498.png)

之前版本的_extract_with_optimized传入只是裸的 Cypher（或普通文本），不含 `complete` 字段。即：不是解析逻辑失效，而是“裁剪过度”导致 `complete` 字段在早期筛选阶段丢失。

本次补丁做了两层修改：1、优先锁定 `input` 含 “图数据库Cypher查询生成专家” 的 span；2、在 [_extract_with_optimized](vscode-file://vscode-app/e:/vscode/Microsoft VS Code/Microsoft VS Code/resources/app/out/vs/code/electron-browser/workbench/workbench.html) 中，当命中候选时不再返回提炼后的查询文本，而是构造{"complete": <bool>, "cypher": "<query>"}



{"question":"查一下最近的合同。","project_id":"I-23112109"}

查询contract表中pms_project_code为'I-23112109'的合同记录，按last_modified_time降序排列，返回第一条记录





## 黄金标准与标注口径

- y_true_complete 的标注准则
  - 风险：你给出“典型/实体密集型=true，歧义/领域外/无效=false”的口径，但现实中“典型但缺关键实体”的样本是否应标 false？歧义问题若 Agent 触发澄清并补齐信息，之后 complete 是否 true？
  - 建议：将“场景标签”与“complete 标注”解耦，给出判定规则（是否在该 span 阶段已具备生成查询的必要条件）。



## 校验器与评估指标

- 组件验证的脆弱性
  - 风险：纯正则/包含式匹配对重排序、别名、参数化、子查询/管道语法不鲁棒。
  - 建议：优先做“轻量解析+规范化”（Cypher/MQL AST 或半结构化分段），最少也应：去注释→统一大小写→去多余空白→别名映射→顺序无关匹配。





#### **1. 维度二，指标2：Cypher语法检查是如何实现的？（详细解析）**



您对这一点的疑问很正常，这确实是一个技术细节。其实施起来比听上去要简单，主要有两种自动化方法：

**方法一 (最推荐)：利用数据库驱动的“预执行”功能**

这是最可靠的方法，因为它利用了数据库自身的语法解析器。几乎所有的数据库驱动（包括Neo4j的官方驱动）都支持发送一个查询去**解析和生成执行计划，但不真正执行它**。

- **原理**：我们尝试让数据库为我们的Cypher“制定一个计划”。如果Cypher有语法错误，数据库在制定计划的阶段就会直接抛出`SyntaxError`异常。如果它成功返回了一个计划（或者没有报错），就证明语法是合法的。
- **优点**：与真实的数据库环境100%匹配，最为精准。
- **实现细节 (以Python伪代码为例)**：

**方法二：使用本地语法检查库 (Linter)**

- **原理**：有一些第三方的开源库可以对Cypher代码进行本地的语法解析。
- **优点**：不依赖数据库连接，速度可能更快。
- **缺点**：可能与您线上数据库的版本不完全匹配，存在微小差异。

**结论**：**推荐使用方法一**。它既简单又可靠，能够完美地自动化“Cypher语法通过率”这个指标的计算。



---



### L3评估现状与难点

经过调研确认了以下**三个**较为合适建立的评估指标：

- **查询可生成性判断准确率**：这个还在进一步优化检索代码，这个指标比较明确，匹配对象就是图数据库Cypher查询生成专家是否能针对正常问题语料输出cypher
  - <u>这个subAgent输出有个complete字段可作为评估的钩子去匹配</u>
- **Cypher语法通过率**：这个初步打算使用数据库在线驱动的`EXPLAIN`功能进行自动化批量校验。或者使用离线的开源库进行类似的语法检验，实现也是比较明确的
  - <u>这个使用开源库进行语法校验</u>
- **Cypher关键组件验证通过率**：这个目前难点较大，一个是**“关键组件”的确定会有 不固定、不一致 问题**，另一个是代码实现复杂度会相对高，当前是让表格AI（表格的LLM可选）去模拟Agent来生成cypher的关键组件的标注（搭配相同prompt）
  - <u>这个目前观察下来，在subAgent（cypher智能体）与AI表格（模拟评估）都能生成cypher的情况下，两者的cypher结构一般都有出入，但是关键组件的重合能在**70%左右**，可作为交集部分判断的参考</u>

**数据标注与批量预测还是引用的L1那套管线：**用GPT按比例批量生成，实现用AI表格批量分类标注（查询cypher是否可生成、标注cypher关键组件）然后人工抽检

![image-20250912174331057](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250912174331057.png)

![image-20250912174736804](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250912174736804.png)



![image-20250915142423807](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250915142423807.png)

![image-20250915153030950](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250915153030950.png)



**当前查询大致流程：原始问题 —— L1层安全筛查 —— cypher生成判断（仅作为有效问题的一个筛选） —— cypher查询（没用上）—— 任务规划与推理Agent（规划问题）—— 智能工具选择器（对接mongoSDK生成查询配置）**

- **无效问题路径：**{"question":"中国的首都是哪里？","project_id":"I-23112109"}

  - 语义有效性校验专家（句子结构校验）

  - 代词识别、指代消解专家

  - 问题分组专家
    - 图数据库Cypher查询生成专家
      - （"error": "问题'中国的首都是哪里？'与图数据库中的实体、属性或关系无关联，无法生成合法Cypher查询"）

  - 问题生成器
    - 你输入的问题有误，请重新输入。你可以咨询以下问题：

       - 查询项目ID为I-23112109的项目信息
       - 获取项目ID为I-23112109的详细数据
       - 查找项目ID为I-23112109的相关记录



- **有效问题路径：**{"question":"统计一下所有由一级销售经理负责，在华北区，过去一个季度内签署的，客户分类是重要客户，并且合同金额超过300万的合同总数。","project_id":"I-23112109"}
  - ....
  - 图数据库Cypher查询生成专家
    - {
        "complete": true,
        "cypher": "MATCH (c:contract) WHERE c.sale_manager_1_level IS NOT NULL AND c.sale_team_1st = '华北区' AND c.signing_date >= date('now') - duration('P3M') AND c.customer_classification = '重要客户' AND toInteger(c.price_including_tax) > 3000000 RETURN count(c) AS total",
        "error": ""
      }
  - 任务规划与推理Agent
    - （{"thought":"需要统计满足多个条件的合同总数：一级销售经理负责、华北区、过去一个季度内签署、客户分类为重要客户、合同金额超过300万。首先需确认项目编号I-23112109对应的合同信息，再筛选符合上述条件的合同。","plan":"调用工具查询项目编号I-23112109下的所有合同信息，然后根据条件进行过滤统计。","action":{"type":"tool","content":"查询项目编号I-23112109对应的所有合同记录"},"thoughtNumber":1,"needMoreThoughts":true}）
  - 智能工具选择器
    - {"tool":"find","parameters":{"collection":"contract","filter":{"pms_project_code":"I-23112109"}}}
    - ...
    - {"thought":"已获取项目编号I-23112109下的所有合同记录，现需根据问题条件进行筛选：一级销售经理负责、销售区域为华北区、签署日期在过去一个季度内、客户分类为重要客户、合同含税金额超过300万元。当前仅有一条合同记录，需逐项验证其是否满足所有条件。","plan":"检查当前合同记录是否满足全部筛选条件，若满足则计数加1，否则不计入。","action":{"type":"tool","content":"检查合同编号23-IX-110002是否满足一级销售经理负责、销售区域为华北区、签署日期在2024-10-16至2025-01-16之间、客户分类为重要客户、合同含税金额大于3000000的条件"},"thoughtNumber":1,"needMoreThoughts":true}
    - `{"tool":"find","parameters":{"collection":"contract","filter":{"_id":"23-IX-110002","sale_manager_1_level":{"$exists":true},"sale_area":"华北区","signing_date":{"$gte":"2024-10-16","$lte":"2025-01-16"},"customer_classification":"重要客户","price_including_tax":{"$gt":"3000000"}}}}`
  - 任务规划与推理Agent
    - {"thought":"当前已获取该项目的唯一合同记录（合同编号23-IX-110002），但检查结果显示该合同不满足所有筛选条件，且无其他合同记录。因此，满足条件的合同总数为0。","plan":"整合所有查询结果，确认无其他合同记录，输出最终统计结果。","action":{"type":"output","content":"满足条件的合同总数为0。"},"thoughtNumber":3,"needMoreThoughts":false}
  - 数据问答助手(满足条件的合同总数为 **0**。)







## 新的端到端工作流（简化视图）

原始问题
→ L1 安全 / 语义结构 / 指代 / 分组 / 图数据库Cypher查询生成专家（判定是否具备项目数据层意义，输出 complete=true 或 error）
→ （如果无效：触发问题生成器建议有效问法并终止）
→ （如果有效）直接进入 智能工具选择器
→ 智能工具选择器 生成一个或多条结构化查询配置（collection + filter + 操作类型 find/aggregate 等）
→ 执行 / 验证 → 汇总结果 → 答案输出

（你计划移除 “任务规划与推理Agent” 的强制地位，把工具选择直接作为 L3 核心）

## 3. 层级评估指标重构建议

| 层级 | 之前重点                                     | 修正后建议指标                                               | 说明                                                  |
| ---- | -------------------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------- |
| L1   | complete 字段 / Cypher 语法 / generatability | 问题“项目语义有效性”判定准确率（Valid vs Invalid）           | Cypher 仅作为辅助证据，不再做细粒度语法评估（可降权） |
| L2   | （未详细讨论）                               | 可继续保留指代消解/结构合规（可选）                          | 不变                                                  |
| L3   | Cypher组件/语法/生成性/组件覆盖              | 智能工具选择器 过滤条件组件覆盖率 + 约束正确性 + 冗余/缺失分析 | 面向 Mongo/filter 表达能力                            |
| L4+  | 答案正确性                                   | 以后再扩展                                                   | 暂缓                                                  |

原有 “generatability (complete)” 在新体系里变为：
L1 指标：是否应进入数据检索阶段（Valid Question Classification）。
不再把 complete 直接等同 “能正确生成最终查询”。

## 4. 对现有实现的影响（代码与逻辑）

| 现状模块                           | 影响                                                         | 行动方向                                                     |
| ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 当前 L3 解析引擎（聚焦 Cypher）    | 目标偏移（对象错误）                                         | 将其下沉到 L1，保留做“辅助判定 + 解释字段”                   |
| 既有 complete/cypher 提取逻辑      | 仍可复用（作为 L1 子信号）                                   | 重命名：`l1_query_viability_analyzer` 或类似                 |
| 计划中的组件匹配（节点/关系/属性） | 不再是 L3 主战场                                             | 可冻结，除非将来仍需要图数据评估                             |
| 新 L3 需要的解析对象               | 智能工具选择器输出（JSON 序列，每条包含 tool / parameters.filter） | 新增解析器：`tool_selector_extractor`                        |
| 黄金标准结构                       | 之前或基于图语义组件                                         | 需重建：列出期望的 collection / 必要字段 / 操作 / 操作符 / 值条件 |
| 指标计算                           | confusion matrix 基于 complete                               | 改为：组件级匹配率 + 严格/宽松两档得分                       |

## 5. L3 新评分设计（初稿）

目标：衡量 智能工具选择器 输出的查询配置在多大程度上覆盖了黄金标准中的“关键约束组件”。

建议定义“组件”维度：

1. collection / target（例如 `contract`）
2. 操作类型（find / aggregate / count / update）—可选
3. 字段级约束（形如：field = value，field in [...], numeric comparison，日期范围）
4. 时间区间表达（起止字段 + 操作符组合）
5. 复合逻辑（AND 结构; 暂不对 OR/Nested 深入，MVP 可视为扁平 AND）

组件匹配判定策略：

- 精确匹配（Strict）：字段名 + 操作符 + 值（或值集合）完全一致（数值/日期需要归一化）。
- 宽松匹配（Loose）：字段名匹配 + 值语义同类（如 “过去一个季度” vs 明确的日期区间）。
- 缺失 (Missing)：标准有而输出缺。
- 冗余 (Redundant)：输出存在但黄金标准不要求（可记录数量，但不一定扣太多分，防止过拟合）。





好的，我们先来理解初始样例吧，我已经在原始的Excel上更换了原始的cypher组件黄金标准为新的mql组件黄金标准“MQL_Agent_Eval\L3_Fix_Eval\L3_Gold_data\l3新评估语料与标注.xlsx”

我已经新建了与MQL_Agent_Eval\L3_Eval同级的新目录MQL_Agent_Eval\L3_Fix_Eval，你可以在新目录下找到“MQL_Agent_Eval\L3_Fix_Eval\generated\l3_tool_selector_golden.json”文件来分析当前新样例的结构



我已经分析了原始的MQL_Agent_Eval\L3_Eval目录下各个脚本的作用，并找出了可以直接复用的脚本与函数。

## 原始L3_Eval目录下各个脚本的作用：

1. **README.md** - L3层开发指南，描述了项目目标、核心评估指标（查询可生成性判断准确率、Cypher语法通过率、Cypher关键组件验证通过率）、项目结构和开发优先级

2. l3_cypher_analyzer.py

    

   \- Cypher分析器，实现语法验证和组件验证功能，包含：

   - CypherNormalizer（标准化器）
   - CypherSyntaxValidator（语法验证器）
   - CypherComponentValidator（组件验证器）

3. **l3_golden_standard_loader.py** - 黄金标准加载器，负责加载Excel或JSON格式的测试用例和Cypher组件定义，包含load_test_cases、load_cypher_components等方法

4. l3_evaluation_engine.py

    

   \- 评估引擎，实现三大核心评估指标的计算，包含：

   - L3EvaluationMetrics（评估指标数据类）
   - L3TestCaseResult（测试用例结果数据类）

5. **l3_test_runner.py** - 测试运行器，集成到现有的MQL Agent评估管线，复用L1层的API请求、trace匹配等基础设施

6. **optimized_cypher_analyzer.py** - 优化的Cypher分析器，专门处理实际的ClickHouse spans数据格式，解决两层JSON包装等问题

## 可以直接复用的脚本与函数：

1. **l3_test_runner.py中的L1层集成基础设施** - API请求、trace匹配、数据收集等流程可以复用
2. **l3_golden_standard_loader.py的数据加载框架** - 文件读取、JSON解析等基础功能可以复用，只需适配新的数据结构
3. **l3_evaluation_engine.py的报告生成模块** - 评估报告的生成框架和格式化逻辑可以复用
4. **l3_cypher_analyzer.py中的CypherNormalizer** - 文本标准化功能可以复用，用于处理工具选择器的输出
5. **日志记录、配置管理等辅助功能** - 通用的工具函数可以复用

## 需要重写的部分：

1. **核心评估逻辑** - 从Cypher查询评估改为工具选择器评估
2. **数据解析逻辑** - 从解析Cypher专家输出改为解析工具选择器输出
3. **验证算法** - 从Cypher语法验证改为MongoDB查询对象验证
4. **评估指标定义** - 重新定义适合工具选择器的评估指标





MVP 简化策略：

- 只取字段名主干：`signing_date.$gte` 截断为 `signing_date`
- `$exists` / `$gt` / `$in` 等全部忽略，仅保留字段的“主键路径”（例：`delivery_docs.delivery_status` 保留原层级 path）。
- 去重后视为集合：GoldenSet, AgentSet
- strict_coverage = |GoldenSet ∩ AgentSet| / |GoldenSet|
- redundant_count = |AgentSet - GoldenSet|
- missing = GoldenSet - AgentSet

全局指标：

- avg_coverage
- micro_coverage (全命中计数累加 / 全黄金组件计数累加)
- avg_missing_per_case
- total_redundant
- case_count

## 3. 组件截断规范 (字段主干抽取)

规则：

1. 如果包含`.`与`$`：

   - 先按 `.` 分段：`signing_date.$gte` → [`signing_date`, `$gte`]
- 丢弃以 `$` 开头的段
   - 再合并：只剩 `signing_date`

2. 若末尾包含 `$exists` / `$gte` / `$lte` / `$gt` / `$lt` / `$in` → 去掉该后缀。

3. 其他保持原样；多层路径保留（如 `contract_docs.signing_unit`）。





将种子问题的范围**严格限定在项目ID `I-23112109` 的相关信息上**，这是一个非常明智和务实的决定。

这样做的好处是：

1. **易于验证**：我们可以将所有注意力集中在一个实体上，人工核对事实和“黄金标准”会非常快速和准确。
2. **快速迭代**：在初步搭建和调试评估管线时，使用一个小的、确定的数据范围可以更快地定位问题。
3. **最小化成本**：在验证整个评估流程可行之前，我们只需投入最少的精力来构造和标注数据。

现在，我们来执行这个聚焦的计划。

------



### **第一步：数据事实分析 (聚焦于项目 `I-23112109`)**



我将基于您提供的`pms_corpus_data_sample.json`文件，重新进行分析，但这次只提取与项目`I-23112109`直接相关的所有事实。

- **核心实体**:
  - **项目ID (`_id`)**: `I-23112109`
  - **项目名称 (`name`)**: `马来西亚MY06-PH2-BMS&CCTV改造项目`
  - **业务类别 (`business_categories`)**: `T1：电力监控、电能管理和保护综合自动化`
  - **交付团队 (`delivery_team`)**: `数据中心`
  - **最终用户 (`end_user`)**: `BRIDGE DATA CENTRES MALAYSIA III SDN. BHD.`
- **关键人员**:
  - **一级项目经理**: `熊瀛`
  - **二级项目经理**: `谢斐`
  - **销售工程师**: `李杰`
  - **商务支持部成员**: `李璇`
- **关键组织**:
  - **一级销售团队**: `数据中心与电信运营商`
  - **三级销售团队**: `王牌团队`
- **关联信息与边界情况**:
  - **关联合同**: 项目数据明确指出，关联了合同 `23-IX-110002`。**（重要发现：在我们收到的`contract`表示例中，并没有`23-IX-110002`这条记录。因此，我们无法构造关于此合同金额、状态等详细信息的问题，但可以构造“项目关联了哪个合同”这类问题。）**
  - **项目问题**: 在`project_problem`集合中，项目`I-23112109`的`problems`数组是空的`[]`。这是一个绝佳的**边界返回**测试点。
  - **发货/物料**: 在`delivery_order`、`delivery_request`和`contract_bom`的样本中，**均未发现**与项目`I-23112109`相关的记录。

**结论**：基于当前数据样本，我们的问题将主要围绕`project`集合内的信息以及`project_problem`集合中的“无问题记录”这一事实来构造。

------



### **第二步：构造聚焦于项目 `I-23112109` 的种子问题集**



以下是根据上述分析，为您量身打造的、严格限定范围的种子问题集。这些问题可直接用于**L4层“关键词精准匹配率”**的自动化评估。

| ID   | 问题类别       | 基于JSON的种子问题 (项目 I-23112109)                         | 黄金答案关键词 (`y_true_keywords`)               |
| ---- | -------------- | ------------------------------------------------------------ | ------------------------------------------------ |
| 1    | 典型问题       | 查一下项目I-23112109的名称。                                 | `["马来西亚MY06-PH2-BMS&CCTV改造项目"]`          |
| 2    | 典型问题       | 项目I-23112109的一级项目经理是谁？                           | `["熊瀛"]`                                       |
| 3    | 典型问题       | 哪个交付团队负责项目I-23112109？                             | `["数据中心"]`                                   |
| 4    | 典型问题       | 马来西亚MY06-PH2-BMS&CCTV改造项目的销售工程师是谁？          | `["李杰"]`                                       |
| 5    | 典型问题       | 项目I-23112109的最终用户是谁？                               | `["BRIDGE DATA CENTRES MALAYSIA III SDN. BHD."]` |
| 6    | 典型问题       | 查一下项目I-23112109的业务类别。                             | `["T1"]` 或 `["电力监控"]`                       |
| 7    | 典型问题       | 谁是项目I-23112109的二级项目经理？                           | `["谢斐", "张文冲"]`                             |
| 8    | 典型问题       | 项目I-23112109有没有记录任何问题？                           | `["没有"]` 或 `["无"]` (边界返回)                |
| 9    | 典型问题       | I-23112109这个项目关联的合同编号是什么？                     | `["23-IX-110002"]`                               |
| 10   | 典型问题       | 查一下项目I-23112109的发货记录。                             | `["没有"]` 或 `["未查询到"]` (边界返回)          |
| 11   | 实体密集型问题 | 熊瀛担任一级项目经理的I-23112109项目，它的交付团队是哪个？   | `["数据中心"]`                                   |
| 12   | 实体密集型问题 | 项目I-23112109中，商务支持部的联系人是谁？                   | `["李璇"]`                                       |
| 13   | 实体密集型问题 | 由销售工程师李杰负责的马来西亚MY06-PH2-BMS&CCTV改造项目，属于哪个一级销售团队？ | `["数据中心与电信运营商"]`                       |
| 14   | 实体密集型问题 | 最终用户是BRIDGE DATA CENTRES MALAYSIA III SDN. BHD.的项目I-23112109，它的三级销售团队叫什么？ | `["王牌团队"]`                                   |
| 15   | 实体密集型问题 | 查一下由熊瀛和谢斐参与的I-23112109项目的项目名称。           | `["马来西亚MY06-PH2-BMS&CCTV改造项目"]`          |

------

**总结与下一步**

这份高度聚焦的种子问题集，是您启动和调试评估流程的完美“第一滴油”。

1. **立即使用**：您可以立即将这批问题和黄金关键词用于搭建和测试您的**L4自动化评估脚本**。
2. **扩展建议**：当您需要进行更全面的评估时，可以采取以下两种方式扩展：
   - **横向扩展**：挑选另一个项目ID，按照同样的方法构造一套新的、围绕该项目的问题集。
   - **纵向扩展**：提供一个包含了项目`I-23112109`及其所有关联合同、发货单、物料清单等**完整数据链条**的JSON样本，届时我们将能构造出更丰富、更复杂的跨表查询问题。





### L3评估管线现状

- **数据：**结合背景上下文利用AI表格自动生成初版MQL查询，然后我在人工抽查调整的方式实现MQL关键组件的批量自动化标注——done

- **评估：**当前L3层的评估修改后聚焦在**查询可生成性**判断准确率、**MCP工具**选择正确性、**集合（pms表范围）**选择正确性、**过滤条件组件**匹配度——已跑通自动化评分
  - 解决了避免无效问题参与MQL评估的计算
  - 解决了多轮MQL情况下的聚合评估问题

当前MQL中间环节的评估可**快速筛选出badcase**，并**了解哪些prompt在查询组件生成时候Agent总是出幻觉**或出错

![img](https://img2024.cnblogs.com/blog/2045416/202509/2045416-20250919163212699-740790669.png)

![image-20250919164408521](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250919164408521.png)

![img](https://img2024.cnblogs.com/blog/2045416/202509/2045416-20250919164940686-449682091.png)

### L4评估方案的更新

这里我改写了一下L4的评估逻辑为**端到端**的评分，L4答案层的效果评估与L3的查询解耦，**不在关注或断言这些信息是否存在于L3返回的查询结果(JSON)中**

将**基于L3的答案忠实度**——>**answer关键词精准匹配**，将从数据实际**值或者关键字**出发，构造精准的问答查询对，**判断答案是否覆盖了用户问题中的所有查询点**。

- **核心指标：关键词精准匹配率 (Keyword Precision Matching)**
  $$
  \text{匹配率} = \frac{\text{关键词全部匹配成功的问题数量}}{\text{问题总数}} \times 100\%
  $$
  ![image-20250919170322744](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250919170322744.png)









**表1：候选模型关键规格概览**

| 特性                    | Qwen2-72B-Instruct             | GLM-4-9B-Chat              | Yi-1.5-34B-Chat         | Llama3-70B-Instruct (中文微调版)    |
| ----------------------- | ------------------------------ | -------------------------- | ----------------------- | ----------------------------------- |
| **参数规模 (总/激活)**  | 72B                            | 9B                         | 34B                     | 70B                                 |
| **上下文长度**          | 128K                           | 128K                       | 4K (可扩展)             | 8K (可扩展至128K)                   |
| **许可证**              | 定制许可证 (商用有MAU<1亿限制) | GLM-4 License (商用需署名) | Apache 2.0              | Llama 3 License (商用有MAU<7亿限制) |
| **MMLU (5-shot)**       | **~83.0+** (推断值)            | 72.4                       | ~76.0+ (推断值)         | 82.0                                |
| **C-Eval (5-shot)**     | **~75.0+** (推断值)            | **75.6**                   | **~80.0+** (基于Yi-34B) | 66.1                                |
| **CMMLU**               | N/A                            | N/A                        | **~83.0+** (基于Yi-34B) | 70.28                               |
| **GSM8K (8-shot)**      | **~92.0+** (推断值)            | 79.6                       | N/A                     | 93.0                                |
| **HumanEval (0-shot)**  | **80.2**                       | 71.8                       | ~70.0+ (推断值)         | 81.7                                |
| **预估VRAM (INT4 AWQ)** | ~40-45 GB                      | **~6-8 GB**                | ~20-24 GB               | ~38-42 GB                           |



**表2：Agent工作流性能与部署综合记分卡**

| 评估维度                  | Qwen2-72B-Instruct | GLM-4-9B-Chat  | Yi-1.5-34B-Chat | Llama3-70B-Instruct (中文微调) |
| ------------------------- | ------------------ | -------------- | --------------- | ------------------------------ |
| **L1 (输入): 上下文**     | **5/5** (128K)     | **5/5** (128K) | 4/5 (~32K+)     | 4/5 (~32K+)                    |
| **L2 (理解): 中文与推理** | **5/5**            | 4/5            | **4.5/5**       | 3.5/5                          |
| **L3 (查询): 智能体能力** | 4/5                | **5/5**        | 3.5/5           | 3/5                            |
| **L4 (答案): 综合质量**   | **5/5**            | 4.5/5          | 4.5/5           | 4/5                            |
| **部署可行性 (成本)**     | 2/5 (高)           | **5/5** (低)   | 3/5 (中)        | 2.5/5 (高)                     |
| **综合推荐指数**          | 3.8 / 5            | **4.7 / 5**    | 4.2 / 5         | 3.3 / 5                        |

现状是A组那边可以更换模型接口的URL，但是



Qwen/Qwen3-Next-80B-A3B-Instruct

ZhipuAI/GLM-4.5

deepseek-ai/DeepSeek-R1-Distill-Llama-70B

openai-mirror/gpt-oss-120b

PaddlePaddle/ERNIE-4.5-21B-A3B-PT

Qwen/Qwen2.5-72B-Instruct

Qwen/Qwen3-32B-AWQ

XGenerationLab/XiYanSQL-QwenCoder-32B-2504



千问魔搭的api：ms-4a393b32-f01e-4b73-9524-d4cfff075a3f

base_url="https://api-inference.modelscope.cn/v1"

然后需要测试的模型名称为

Qwen/Qwen3-32B-AWQ（baseline已部署）

Qwen/Qwen3-Next-80B-A3B-Instruct（同系列升级）

ZhipuAI/GLM-4.5（旗舰级对标）

XGenerationLab/XiYanSQL-QwenCoder-32B-2504（差异化）



硅基流动的API：sk-jkbchzeqfoonrqnlesqhftyclxwxmvsyqpbhkxwdchitfdjt

url = "https://api.siliconflow.cn/v1/chat/completions"

Qwen/Qwen3-32B-AWQ（本地部署baseline）（http://172.17.6.27:8100/）

Qwen/Qwen3-Next-80B-A3B-Instruct（同系列尺寸升级）（http://172.17.6.27:8101/）

zai-org/GLM-4.5-Air（跨厂商效率对标）（http://172.17.6.27:8101/）

Qwen/Qwen3-Coder-30B-A3B-Instruct（同系列专业化）（http://172.17.6.27:8101/）



云雾API聚合平台APIkey：sk-H8ZjSo7v0N1eKbNFIZrz82HFA97zAC7oCzRpzb3tdEPyP4YY

url = "https://yunwu.ai/v1/chat/completions"

qwen3-coder-plus（qwen旗舰顶级模型横向对比）480B-A35B（172.17.6.27:8104）

gpt-5-chat-latest（gpt旗舰顶级模型横向对比）（172.17.6.27:8105）

gemini-2.5-flash（gemini旗舰顶级模型横向对比）（172.17.6.27:8106）





这个组合可以用最少的工作量，获得关于“升级路径”、“外部竞争”和“性价比”三个维度的核心洞察。

| 模型名称                                | 所属分组      | **入选理由 (我们想通过它回答什么问题？)**                    | 接口地址 |
| --------------------------------------- | ------------- | ------------------------------------------------------------ | -------- |
| **`Qwen/Qwen3-32B-AWQ`**                | 本地baseline  | **（不变的基准）** 我们当前的性能水平到底如何？所有对比的参照系。 | 8100     |
| **`Qwen/Qwen3-Next-80B-A3B-Instruct`**  | 同系列升级    | **（内部升级收益）** 花更多的资源换到最新的Qwen3代，性能提升是否显著？尤其是在L3复杂查询和L4忠实度上。 | 8101     |
| **`zai-org/GLM-4.5-Air`**               | 旗舰级对标    | **（外部顶级对手表现）** 如果我们切换到另一个主流顶级模型系列，表现会更好还是更差？GLM是Qwen的直接竞争者，非常有对比价值。 | 8102     |
| **`Qwen/Qwen3-Coder-30B-A3B-Instruct`** | 性价比/差异化 | **（降本增效可能性）** 参数量最小的模型，它的性能能否满足我们的底线要求？如果表现尚可，将是巨大的成本优势。 | 8103     |

**执行这个方案后，您将能清晰地回答：**

- 最新的Qwen3值不值得升级？
- 和隔壁的GLM相比我们有优势吗？
- 有没有可能用一个更便宜的ERNIE模型来达到可接受的效果？



![image-20250929163608740](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250929163608740.png)

![image-20250929163250552](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20250929163250552.png)

![image-20251013091939538](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20251013091939538.png)



> 当前QWQ32B的Agent提示词框架下发现的一些问题

![image-20251010092453187](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20251010092453187.png)

"查一下由熊瀛和谢斐参与的I-23112109项目的项目名称。"这个问题QWQ32B的回答正确率不高，四次中只有1次sql查询结果正确的



gpt-5-codex
gpt-5-codex-high
gpt-5-codex-low
gpt-5-codex-medium
gpt-5-2025-08-07
gpt-5-chat-latest
gpt-5-mini-2025-08-07
gpt-5-nano-2025-08-07
chatgpt-4o-latest
gpt-4.1-2025-04-14
gpt-4o
gpt-4o-mini
gpt-4o-mini-2024-07-18
gpt-oss-120b
gpt-oss-20b
claude-sonnet-4-5-20250929
gemini-2.5-flash-lite-preview-09-2025
gemini-2.5-flash-preview-09-2025
gemini-2.5-flash
gemini-2.5-flash-lite
gemini-2.5-flash-lite-preview-06-17
gemini-2.5-pro
deepseek-r1-distill-llama-70b
deepseek-r1-distill-qwen-32b
deepseek-r1-distill-qwen-7b
qwen3-max
qwen3-next-80b-a3b-instruct
qwen3-next-80b-a3b-thinking
qwen3-max-preview-n
qwen3-coder-plus
qwen3-14b
qwen3-235b-a22b
qwen3-235b-a22b-instruct-2507
qwen3-235b-a22b-think
qwen3-235b-a22b-thinking-2507
qwen3-30b-a3b
qwen3-30b-a3b-instruct-2507
qwen3-30b-a3b-think
qwen3-30b-a3b-thinking-2507
qwen3-32b
qwen2.5-32b-instruct
qwen2.5-72b-instruct
qwen2.5-7b-instruct
qwen-max
qwen-max-2025-01-25
qwen-max-latest
qwen-plus
qwen-plus-2025-09-11
qwen-plus-latest
qwen2.5-vl-7b-instruct
qwq-32b
qwq-72b-preview
qwq-plus
qwq-plus-2025-03-05
qvq-72b-preview
doubao-seed-1-6-250615
doubao-seed-1-6-flash-250615
doubao-seed-1-6-flash-250828
doubao-1-5-pro-256k-250115
doubao-1-5-pro-32k-character-250228
glm-4.6
glm-4.6-thinking

在进行横向评估时，**控制变量**是得出可信结论的唯一途径。选择的云雾API平台提供了丰富的模型，这既是优势也是挑战。

我们完全按照您提议的三个步骤来讨论，最终确定一个“少而精”的评测模型列表。

------



### **第一步：确定横向对比的方向 (The "Why")**



在选择模型之前，我们必须先明确我们想通过对比**回答哪些核心问题**。这些问题构成了我们横向评测的“对比方向”或“评估维度”。

1. **性能与成本的权衡 (Performance vs. Cost)**
   - **核心问题**: “最贵/最强的模型”带来的性能提升，是否值得其高昂的成本和更长的响应时间？是否存在一个“性价比”最高的甜点模型？
   - **需要对比**: 高性能模型 vs. 轻量级/速度优化模型。
2. **模型迭代的收益 (Generational Improvement)**
   - **核心问题**: 同一个供应商的最新一代模型（如Qwen3）比上一代（如Qwen2.5）到底强多少？升级的价值有多大？
   - **需要对比**: 来自同一家族、但不同版本的模型。
3. **参数规模的效应 (Scaling Effect)**
   - **核心问题**: 在同一代模型中，7B、32B、72B、235B等不同参数规模的模型，其性能是否呈线性增长？增长的“拐点”在哪里？
   - **需要对比**: 来自同一家族、同一版本、但参数量不同的模型。
4. **通用与专业的差异 (Generalist vs. Specialist)**
   - **核心问题**: 针对我们的任务（涉及将自然语言转为结构化查询），一个经过代码或逻辑推理优化的“专才”模型，是否会比一个通用的“全才”对话模型表现更好？
   - **需要对比**: `Instruct` / `Chat` 模型 vs. `Codex` / `Coder` / `Thinking` 模型。
5. **不同技术路线的比较 (Cross-Provider Landscape)**
   - **核心问题**: 顶级的国产模型（Qwen, GLM, Doubao）与国际顶尖模型（GPT, Gemini, Claude）在我们的特定任务上，各自的优势和劣势是什么？
   - **需要对比**: 不同供应商的旗舰模型。

------



### **第二步：划分模型类别 (The "What")**



现在，我们运用“控制变量”的思想，将您提供的庞大列表，按照上述对比方向进行归类。



#### **1. 按供应商/家族划分**



- **OpenAI系**: `gpt-5-*`, `gpt-4*`, `chatgpt-*` 等
- **Google系**: `gemini-2.5-*`
- **Anthropic系**: `claude-sonnet-4-5-*`
- **阿里通义系 (Qwen)**: `qwen*`, `qwq*`, `qvq*` (这是最大、最复杂的家族)
- **智谱清言系 (GLM)**: `glm-4.6*`
- **DeepSeek系**: `deepseek-r1-*`
- **字节跳动系 (Doubao)**: `doubao-*`



#### **2. 按参数规模划分 (以Qwen家族为例)**



- **小型 (Small)**: `qwen2.5-7b-instruct`, `qwen3-14b`
- **中型 (Medium)**: `qwen3-32b`, `qwen2.5-32b-instruct`, `qwen3-30b-a3b`
- **大型 (Large)**: `qwen2.5-72b-instruct`, `qwen3-next-80b-a3b-instruct`
- **超大型 (X-Large)**: `qwen3-235b-a22b`



#### **3. 按版本/迭代划分 (以Qwen家族为例)**



- **Qwen-Max (早期旗舰)**: `qwen-max`, `qwen-plus`
- **Qwen 2.5 代**: `qwen2.5-32b-instruct`, `qwen2.5-72b-instruct`
- **Qwen 3.0 代**: `qwen3-32b`, `qwen3-max`



#### **4. 按专业化能力划分**



- **通用对话/指令**: 名称中带 `chat`, `instruct` 的模型 (例如 `qwen2.5-32b-instruct`)。
- **代码/逻辑优化**: 名称中带 `codex`, `coder`, `thinking` 的模型 (例如 `gpt-5-codex`, `qwen3-coder-plus`, `glm-4.6-thinking`)。
- **速度/轻量优化**: 名称中带 `mini`, `nano`, `flash`, `lite` 的模型 (例如 `gpt-4o-mini`, `gemini-2.5-flash-lite`)。

------



### **第三步：确定代表模型 (The "Which")**



好的，我们来设计最终的对比报告表格。

基于您最终确定的这个极具洞察力的7模型组合，以及我们之前讨论过的所有优化点（分层耗时、优胜者、性价比指数等），我为您制作了一份**终版横向评测报告的核心表格模板**。这份表格不仅结构清晰，而且我填充的**示例数据**也经过精心设计，旨在模拟一个真实、复杂且能引发深入讨论的评测结果。

------



### **MQL Agent 多模型横向评测报告 (终版模板)**



- **评估日期**: `2025-10-10`
- **测试集版本**: `黄金标准测试集 v2.0 (已优化)`
- **测试用例总数**: `300`



#### **核心指标量化对比表**



| 评估维度     | 指标名称                        | **基线** (Qwen3-32B-AWQ) | **升级** (Qwen3-Next-80B) | **甜品级** (GLM-4.5-Air) | **专业化** (Qwen3-Coder-30B) | **专业化+** (Qwen3-Coder-Plus) | **GPT旗舰** (GPT-5-Codex) | **Gemini旗舰** (Gemini-2.5-Pro) | **优胜者 🏆**      |
| ------------ | ------------------------------- | ------------------------ | ------------------------- | ------------------------ | ---------------------------- | ------------------------------ | ------------------------- | ------------------------------- | ----------------- |
| **L1: 输入** | **无效/恶意输入拦截率**         | 99.0%                    | 98.5%                     | **99.5%**                | 99.0%                        | 99.0%                          | 99.0%                     | **99.5%**                       | GLM / Gemini      |
| **L2: 理解** | **实体抽取 F1 分数**            | 86.5%                    | 91.0%                     | 89.0%                    | 85.0%                        | 88.5%                          | 94.5%                     | **96.0%**                       | Gemini旗舰        |
| **L3: 查询** | **聚合查询准确率**              | 35.5%                    | 45.0%                     | 42.5%                    | 75.0%                        | 88.0%                          | **92.5%**                 | 81.0%                           | **GPT旗舰**       |
| **L4: 答案** | **关键词精准匹配率 (忠实度)**   | 82.0%                    | 90.5%                     | 88.0%                    | 89.5%                        | 92.5%                          | 94.0%                     | **97.5%**                       | **Gemini旗舰**    |
| **性能效率** | **L3_查询层平均耗时 (秒)**      | 1.5s                     | 2.8s                      | 2.5s                     | **1.1s**                     | 1.4s                           | 1.8s                      | 2.1s                            | **专业化(Coder)** |
|              | **总平均响应时延 (秒)**         | 2.8s                     | 4.8s                      | 4.2s                     | **2.5s**                     | 3.0s                           | 3.5s                      | 3.9s                            | **专业化(Coder)** |
| **商业价值** | **API成本 (估算, 元/千次查询)** | **¥ 10.0**               | ¥ 25.0                    | ¥ 18.0                   | ¥ 12.0                       | ¥ 15.0                         | ¥ 40.0                    | ¥ 35.0                          | **基线(Qwen3)**   |
|              | **性价比指数 (综合得分/成本)**  | **8.1**                  | 3.4                       | 4.6                      | 6.8                          | 6.1                            | 2.4                       | 2.7                             | **基线(Qwen3)**   |
| **综合得分** | **(加权平均分)**                | 81.0                     | 86.0                      | 83.5                     | 82.0                         | 91.0                           | 95.5                      | **96.5**                        | **Gemini旗舰**    |

------



### **如何解读此报告 (基于示例数据的分析)**



这份报告不仅仅是数字的罗列，它讲述了一个关于模型选型的完整故事：

1. **基线模型 (`Qwen3-32B-AWQ`) 的价值**:
   - **故事**: 它是“**性价比之王**”。虽然在各项性能指标上都不拔尖，但它的成本是最低的，从而获得了最高的**性价比指数（8.1）**。
   - **决策点**: 如果预算是首要考虑因素，且当前性能（尽管L3较弱）可以勉强接受，那么维持基线并持续优化是合理的选择。
2. **“大力不一定出奇迹”**:
   - **故事**: `升级 (Qwen3-Next-80B)` 和 `甜品级 (GLM-4.5-Air)` 作为通用大模型，虽然在L2理解和L4答案生成上优于基线，但在最核心的**L3查询准确率**上提升有限（仅从35.5%提升至45%左右），未能解决根本瓶颈。
   - **决策点**: 这证明了单纯扩大通用模型的规模，并不能有效解决我们面临的“结构化查询生成”这一专业化问题。
3. **专业化模型的“奇袭” (最重要的发现)**:
   - **故事**: `专业化+ (Qwen3-Coder-Plus)` 和 `GPT旗舰 (GPT-5-Codex)` 两个为代码/逻辑优化的模型，在**L3查询准确率**上实现了**断层式领先**（88.0% 和 92.5%），直接命中了我们的核心痛点。
   - **决策点**: 这强烈地表明，要从根本上提升Agent性能，必须引入**专业化模型**来处理L3层的任务。
4. **旗舰模型的“天花板”**:
   - **故事**: `Gemini旗舰 (Gemini-2.5-Pro)` 在**L2理解（96.0%）**和**L4忠实度（97.5%）**上展现了绝对的统治力，定义了“通用能力”的天花板。但即便如此，它在L3层的表现依然输给了顶级的专业化模型。
   - **决策点**: 这为我们指明了理想的系统架构——能否利用Gemini进行**前置的意图理解（L2）\**和\**后置的答案美化（L4）**，而将最核心的**L3查询生成**交给`GPT-5-Codex`或`Qwen3-Coder-Plus`来处理？



### **最终结论 (示例)**



根据这份报告，可以向上汇报的结论是：

> “本次横向评测表明，通过引入**专业化的Codex/Coder模型**，我们可以将Agent最核心的**查询准确率（L3）从35%提升至90%以上**，这是解决当前系统瓶颈的关键。与此同时，**Gemini-2.5-Pro**在自然语言理解和生成方面树立了性能天花板。
>
> **建议**：下一步，我们将主力研究**‘混合模型（Mixture-of-Experts）’**方案，探索将Gemini（负责L2/L4）与GPT-Codex（负责L3）相结合的系统架构，以期在性能和成本之间取得最佳平衡。”



---



如何解决Server disconnected的问题。
我发现本地显示Server disconnected的请求都能够在web网页上找到对应的laminar的trace记录，但是这个记录似乎都是都是未完成的状态

但为什么我直接通过web链接来访问网页的页面去问问题却不会出现Server disconnected和laminar中trace中断的问题。
我问过开发当前Agent的业务同事他们说没有给Agent的响应加超时限制

我自己有什么解决办法吗？比如修改那些配置可以解决Server disconnected问题



-----

###  L1 与 L3 协同

| 评估层    | 评估范围   | 核心指标                                   | 数据来源               |
| --------- | ---------- | ------------------------------------------ | ---------------------- |
| **L1 层** | 输入验证   | 语义安全 + 歧义处理 + **业务相关性**       | Cypher expert complete |
| **L3 层** | 工具选择器 | **工具类型** + **集合选择** + **查询条件** | Tool selector output   |



---



## 强化学习战略分析报告（面向项目助理Agent）

## 概览结论

- **当前架构**：单轮直达模式，缺失对话策略层；最大瓶颈在 L3 NL→MQL 生成
- RL黄金突破口：
  1. **L3**：结构化生成与候选重排（直接提升MQL正确率）
  2. **节点5代行L1/L2**：澄清/引导策略（减少slot-error，缩短轮次）
- 围绕现有5节点工作流，**所有可引入RL**进行优化的潜在环节如下：
  1. 节点4·模块/路径选择子策略（用RL探索MQL业务口径选择、集合/关系路径判定）
  2. 节点4·NL→MQL 生成器（主干生成策略）
  3. 节点4·候选查询生成与排序（Top-K采样/重排）
  4. 节点5·澄清策略（基于slot-error/intent-error的最小信息澄清问法选择）
  5. 节点3·归一化与复合问题拆分策略（规则合规+可执行性导向）
  6. 跨节点·意图/槽位充分性判定策略（执行vs澄清的门控）
  7. 节点2·指代消解的风险控制策略（何时保留/何时追问）
  8. 节点1·输入有效性阈值/容错策略（宽严动态调节）

### 第一部分：RL可应用点全景识别

```markdown
┌─────────────────────────────────────────────────────┐
│          Agent工作流 (5节点) + RL应用点               │
└─────────────────────────────────────────────────────┘

用户输入
   ↓
┌──────────────┐
│ 节点1: 校验   │  ← 不适用RL（简单二分类，监督学习即可）
└──────────────┘
   ↓
┌──────────────┐
│ 节点2: 消解   │  ← 不适用RL（规则+监督学习足够）
└──────────────┘
   ↓
┌────────────────────────────────────┐
│ 节点3: 问题归一化                   │ ← 🎯 P2: RL对象选择 + 拆分策略
│  · 对象识别 (project/contract/...) │    (作为L3减难器)
│  · 属性提取                         │
│  · 复合问题拆分                     │
└────────────────────────────────────┘
   ↓
┌────────────────────────────────────┐
│ L3 (节点4): NL→MQL生成             │ ← 🔥 P0: 分步生成 + 候选重排
│  · 集合选择                         │    (核心瓶颈，RL主战场)
│  · $match生成                       │
│  · $lookup决策 (是否关联+选表)      │ ← 🎯 P3: 不确定性策略
│  · 聚合/排序                        │    (何时返回error vs 冒险生成)
│  → {correct/slot-error/intent-error}│
└────────────────────────────────────┘
   ↓
┌────────────────────────────────────┐
│ 节点5: 数据问答助手                 │ ← 🎯 P1: 代行对话策略
│  当前: 仅基于数据生成答案           │    (澄清/引导/结束，Bandit)
│  增强: if error → 追问/引导         │
└────────────────────────────────────┘
   ↓
返回用户
```

------

### 第二部分：逐点深度论证（目标/策略/奖励/可行性）

A. 节点4 - NL→MQL 主干生成策略（首要）

- 优化目标
  - 最大化生成的MQL正确率与可执行率；在信息不足时稳定产出可解释的slot-error/intent-error（避免错误执行）。
  - 严格遵循范式A、别名白名单、数组最小原则、聚合-only等硬约束。
- 策略模型
  - 基于指令微调的结构化解码模型（LLM或专用结构化解码器），输出严格JSON。
  - 行为空间分解：工具类型/集合选择 → 入口$match → 是否业务口径模块 → $lookup链路 → project/*p**ro**j**ec**t*/unwind/$group 等序列化动作。
  - 采用约束语法（grammar-constrained decoding）+ 模板化片段库，减少无效搜索。
- 奖励函数设计
  - 程序化奖励（可离线计算，易规模化）分层加权：
    - 语法/结构有效性 Rsyn：JSON可解析、字段齐全、operation=aggregate。
    - 架构合规 Rrule：范式A检查、禁止行为检查、数组/lookup规范、别名白名单命中。
    - 模式正确 Rpath：集合/主外键链路与题意一致；入口$match的高选择性字段前置。
    - 执行成功 Rexec：Mongo沙箱成功运行，无异常；可选加入执行耗时/阶段数惩罚以控成本。
    - 语义对齐 Rsem：与GT或校准评测器的语义一致性（字段/过滤/聚合/排序/TopN）。
    - 错误优雅 Rerr：信息确实不足时产出slot-error/intent-error且reason准确、可用。
  - 总奖励示例：R = α1*Rsyn + α2*Rrule + α3*Rpath + α4*Rexec + α5*Rsem + α6*Rerr − β1*违禁项 − β2*过度复杂度
  - 离线偏好：从日志中抽取“成功回答/用户确认有用”的样本作为正例，与失败样本配对进行偏好优化（DPO/RRHF）。
- 可行性与挑战
  - Pros：奖励可程序化、可离线大规模训练；与北极星强耦合；改进幅度潜力最大。
  - Cons：执行型奖励成本高（需要Mongo沙箱）；存在“虚假成功”（spurious match）风险；需强约束避免“把slot-error当捷径”。

B. 节点4 - 模块/路径选择子策略（业务口径与集合链路判定）

- 优化目标
  - 正确触发业务口径模块（如“未发货”）或选择通用路径；在歧义时选择信息最丰富且唯一可达路径。
- 策略模型
  - 上游判别/路由器（contextual bandit或小型分类器+RL微调），输入为归一化问题与上下文，输出为：{模块ID/集合起点/lookup跳数/是否展开数组}。
- 奖励函数设计
  - 即时奖励：模块选择后，若后续查询正确则+1，否则-1；附加规则一致性奖励（是否满足模块入口与字段强约束）。
  - 反事实重放：利用日志中的成功/失败轨迹离线训练CB（IPS/DR/CRM）。
- 可行性与挑战
  - Pros：动作空间小、学习稳定、样本复用率高。
  - Cons：依赖业务口径覆盖度；错判将连锁影响后续所有生成阶段。

C. 节点4 - 候选查询生成与排序（Top-K采样/重排）

- 优化目标
  - 单轮内生成K个候选MQL并进行学习排序，提升pass@k与最终一选正确率。
- 策略模型
  - 生成器（同A）+ 强化排序器（listwise/pointwise）或RL重排器（bandit）。
- 奖励函数设计
  - 对候选集执行奖励：top-1正确→高奖；top-k内有正确→中等奖；全部错误→惩罚；附加合规与成本正则。
- 可行性与挑战
  - Pros：显著提升稳健性；与在线系统易集成（保留Top-K重试）。
  - Cons：推理成本上升；需良好的候选多样性与去重策略。

D. 节点5 - 澄清策略（基于slot-error/intent-error的最小追问）

- 优化目标
  - 在信息不足时，用最少轮次补齐关键槽位（如project_id/contract_id/时间范围），最大化最终任务成功率、最小化对话轮数。
- 策略模型
  - 动作空间为“询问哪个槽位+以哪个模板问法”；上下文状态包含已知槽位、错误reason、历史问答。
  - 初始可用模板库（由问题决策树/Node4 reason对齐），逐步引入生成式改写。
- 奖励函数设计
  - 任务回合奖励：最终成功+高奖，失败-惩罚；每多一轮-小惩罚；一次问多槽位视疲劳度增加适当惩罚。
  - 中间奖励：成功消除某错误类别（由Node4从slot-error转为correct）+正奖励。
- 可行性与挑战
  - Pros：多轮闭环能力迅速提升端到端成功率；奖励可由系统自动采集（无需人工标注）。
  - Cons：长程信用分配难；需模拟用户或基于历史日志做离线策略评估（OPC/IS）。

E. 节点3 - 归一化与复合问题拆分策略（合规+可执行性导向）

- 优化目标
  - 在遵循14条规则的同时，产生更“易于Node4正确生成”的规范问句；对复合问题的拆分更有利于模块/路径选择。
- 策略模型
  - 生成式/判别式策略，输出“归一化问句序列”；加入“项目编号注入”等硬约束（可作为语法约束或规则奖励）。
- 奖励函数设计
  - 合规模块：逐条规则打分（对象/属性/关系/判断语义/编号/范围等）。
  - 可执行模块：将归一化产出喂给固定Node4评测，取其生成正确率作为奖励信号（离线可行）。
- 可行性与挑战
  - Pros：奖励自动化、可与Node4闭环优化；对复杂问题有正效应。
  - Cons：需防止为迎合Node4而牺牲语义保真（需强约束）。

F. 跨节点 - 执行vs澄清门控策略

- 优化目标
  - 决策当前轮是否直接执行Node4，或先触发澄清（Node5）。
- 策略模型
  - 小模型门控（bandit），输入为节点1~3产物+Node4置信特征（如路径冲突、别名歧义计数）。
- 奖励函数设计
  - 端到端成功率+轮次惩罚；将“误执行”设为大惩罚，鼓励在风险大时先澄清。
- 可行性与挑战
  - Pros：简单有效，降低早期错误。
  - Cons：特征工程依赖质量；需避免过度保守导致轮次过多。

G. 节点2 - 指代消解风险控制

- 优化目标
  - 当指代不确定时选择“保留/澄清”，避免误解导致错误查询。
- 策略模型
  - 二分类/多分类策略，决定指代置信阈值与澄清动作。
- 奖励函数设计
  - 后验奖励：错误指代→后续查询失败为惩罚；成功为奖励。
- 可行性与挑战
  - Pros：减少误解引发的系统性错误。
  - Cons：收益相对次级，且可部分由D/F覆盖。

H. 节点1 - 有效性阈值/容错策略

- 优化目标
  - 避免过严拦截（损失召回）或过松放行（增加后续失败成本）。
- 策略模型
  - 阈值/规则强度的bandit选择（场景化调节）。
- 奖励函数设计
  - 全链路成功率与平均轮次；阈值越合理，总体回报越高。
- 可行性与挑战
  - Pros：实现简单，可作为后续精调项。
  - Cons：收益有限，不建议作为首批重点。

------

### 第三部分：最终战略建议（有序优先级）

优先级排序（从先到后）：

1. 节点4·NL→MQL 主干生成策略（A）
2. 节点5·澄清策略（D）与跨节点门控（F）的小闭环
3. 节点4·模块/路径选择子策略（B）
4. 节点4·候选生成与排序（C）
5. 节点3·归一化可执行性导向优化（E）
6. 节点2/节点1风控微调（G/H）

为什么先做A（最佳起步点）：

- 影响最大：L3查询生成是已知主瓶颈，直接决定“是否能答对”；对北极星提升潜力最大。
- 可落地性强：奖励可程序化（语法、范式、执行、合规），无需大量人工标注；支持离线大规模训练与快速迭代。
- 风险可控：通过语法/规则/范式A的强约束与结构化解码，降低RL探索风险与“投机取巧”。
- 数据充足：ClickHouse日志+Mongo沙箱即可搭建完整离线训练—评测闭环。

紧随其后做D+F（最小对话闭环）：

- 系统目前“单轮直达”，错误多来自信息不足或早期误解；澄清策略能显著减少“无效执行”和“反复失败”，有效降低轮次并提高成功率。
- D（澄清策略）可先用模板动作空间+bandit快速启动；F（门控）作为保底，避免高风险直接执行。

B/C/E作为二期增强：

- B保证模块/路径的稳定可达性（特别是“未发货”口径等场景）。
- C提高稳健性（Top-K+重排）以减少长尾错误。
- E将归一化与Node4捆绑优化，使输入更“可执行”。











---

王工好，我调研分析了实体关系抽取任务相比表查询路径任务的这种树action空间复杂度更高，是否后续优先以表查询路径生成为实验主线

另外强化学习的主流算法流程原理我都大概过了一遍，目前我感觉难点是在环境的建模上。前面我实操的RL游戏案例都是实时的交互奖励进行训练，比较标准，但是我们现在业务问题基本都是一次性或者说回合制下的奖励机制，这里在实现上会有一个gap，不好处理，可能需要补充学习和实验，或者看看王工您是否有思路的指导和建议



比如当前表查询路径的任务可能的几种训练方式如下：

**标准的表查询路径任务：**状态1输入问题语料——动作1查表A（执行查表，计算r）——状态2输入语料+表A记录——动作2查表B（执行查表，计算r）...——判断done

**简化的表查询路径任务：**状态1输入问题语料——动作1查表A（执行查表，计算r）——状态2输入语料+动作1——动作2查表B（执行查表，计算r）...——判断done

**更简化的表查询路径任务：**输入问题语料——生成完整查询路径（执行查表，计算R，重复5次即为done），类似SFT回合制，效果未知

![image-20251110142026218](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20251110142026218.png)

尝试了采用第三种最简单的方式做了一个模拟实验，设计如下：(**更简化的表查询路径任务：**嵌入正确路径加噪声作为输入向量——生成完整查询路径（匹配路径组件，计算R，重复5次即为done）)

![image-20251110153406037](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20251110153406037.png)

跑了100000个episode 的训练效果  如下

![image-20251110153914567](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20251110153914567.png)

在PPO训练的MLP进行单个样本测试如下：



![image-20251110154152114](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20251110154152114.png)





发现PPO当前不收敛的原因是因为模型找到了激励机制的漏洞，（对于 PPO 的优化目标（最大化回报），持续拿 +12 并延长回合到5，累计回报+60明显高于一次命中拿 +16。因此，策略学会了一个“更优但不是想要的”策略：稳定选择某个合法但错误的 Target，避免结束回合，赚更多的回合总奖励。）——这说明了奖励机制或者奖励模型的逻辑闭合很关键
